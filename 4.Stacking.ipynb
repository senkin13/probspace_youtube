{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ProbSpace: YouTube動画視聴回数予測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = \"out_tmp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "import itertools\n",
    "import os, datetime, gc, glob, re, random\n",
    "import time, datetime\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import optuna\n",
    "import bhtsne, umap\n",
    "\n",
    "from janome.tokenizer import Tokenizer\n",
    "from janome.analyzer import Analyzer\n",
    "from janome.tokenfilter import *\n",
    "from janome.charfilter import UnicodeNormalizeCharFilter, RegexReplaceCharFilter\n",
    "import unicodedata\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import Pool, CatBoostRegressor, CatBoostClassifier\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.linear_model import LinearRegression, BayesianRidge, ElasticNet, Lasso, LogisticRegression, Ridge, SGDRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor, BaggingRegressor\n",
    "from sklearn.ensemble import StackingRegressor, VotingRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor, GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.svm import LinearSVR\n",
    "\n",
    "from ngboost import NGBRegressor\n",
    "from ngboost.ngboost import NGBoost\n",
    "from ngboost.learners import default_tree_learner\n",
    "from ngboost.scores import MLE, CRPS, LogScore\n",
    "from ngboost.distns import Normal, LogNormal\n",
    "\n",
    "from sklearn.linear_model import BayesianRidge, ElasticNet, Lasso, LogisticRegression, Ridge, SGDRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans, DBSCAN\n",
    "from sklearn.model_selection import KFold, RepeatedKFold, StratifiedKFold, cross_validate, cross_val_predict, train_test_split\n",
    "from sklearn.metrics import mean_squared_error, roc_auc_score, log_loss\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler, Normalizer, RobustScaler, QuantileTransformer, PowerTransformer\n",
    "from sklearn.feature_selection import SelectFromModel, RFE, SelectPercentile, SelectKBest\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.initializers import he_normal, he_uniform, GlorotNormal, GlorotUniform\n",
    "from tensorflow.keras.optimizers import Adadelta, Adagrad, Adam, Adamax, Ftrl, Nadam, RMSprop, SGD\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, EarlyStopping, TensorBoard, LambdaCallback, ReduceLROnPlateau\n",
    "from tensorflow.keras.metrics import MeanSquaredError, RootMeanSquaredError\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Concatenate, Lambda\n",
    "from tensorflow.keras.layers import Activation, Average, Dense, Dropout, Flatten, BatchNormalization, LeakyReLU, Input\n",
    "from tensorflow.keras.layers import GaussianDropout, GaussianNoise\n",
    "from tensorflow.keras.layers import Conv2D, SeparableConv2D, MaxPooling2D, AveragePooling2D, GlobalAveragePooling2D\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import missingno as msno \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for variable description\n",
    "def description(df):\n",
    "    summary = pd.DataFrame(df.dtypes, columns=['dtypes'])\n",
    "    summary = summary.reset_index()\n",
    "    summary[\"Name\"] = summary['index']\n",
    "    summary = summary[[\"Name\",'dtypes']]\n",
    "    summary[\"Missing\"] = df.isnull().sum().values    \n",
    "    summary[\"Uniques\"] = df.nunique().values\n",
    "    summary[\"Mean\"] = np.nanmean(df, axis=0).astype(df.dtypes)\n",
    "    summary[\"Std\"] = np.nanstd(df, axis=0).astype(df.dtypes)\n",
    "    summary[\"Minimum\"] = np.nanmin(df, axis=0).astype(df.dtypes)\n",
    "    summary[\"Maximum\"] = np.nanmax(df, axis=0).astype(df.dtypes)\n",
    "    summary[\"First Value\"] = df.iloc[0].values\n",
    "    summary[\"Second Value\"] = df.iloc[1].values\n",
    "    summary[\"Third Value\"] = df.iloc[2].values\n",
    "    summary[\"dimension\"] = str(df.shape)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hist(target):\n",
    "    plt.hist(target, bins=100)\n",
    "\n",
    "    print(\"max:  {:>10,.6f}\".format(target.max()))\n",
    "    print(\"min:  {:>10,.6f}\".format(target.min()))\n",
    "    print(\"mean: {:>10,.6f}\".format(target.mean()))\n",
    "    print(\"std:  {:>10,.6f}\".format(target.std()))\n",
    "    \n",
    "    return\n",
    "\n",
    "def get_hist4(target1, title1, target2, title2, target3, title3, target4, title4):\n",
    "    fig = plt.figure(figsize=(18, 18))\n",
    "\n",
    "    ax1 = fig.add_subplot(5,1,1)\n",
    "    ax2 = fig.add_subplot(5,1,2)\n",
    "    ax3 = fig.add_subplot(5,1,3)\n",
    "    ax4 = fig.add_subplot(5,1,4)\n",
    "    ax5 = fig.add_subplot(5,1,5)\n",
    "\n",
    "    ax1.set_title(title1)\n",
    "    ax2.set_title(title2)\n",
    "    ax3.set_title(title3)\n",
    "    ax4.set_title(title4)\n",
    "    ax5.set_title(\"OVERALL\")\n",
    "    \n",
    "    ax1.hist(target1, bins=100)\n",
    "    ax2.hist(target2, bins=100)\n",
    "    ax3.hist(target3, bins=100)\n",
    "    ax4.hist(target4, bins=100)\n",
    "\n",
    "    ax5.hist(target1, bins=100, alpha=0.2, color='red')\n",
    "    ax5.hist(target2, bins=100, alpha=0.2, color='green')\n",
    "    ax5.hist(target3, bins=100, alpha=0.2, color='blue')\n",
    "    #ax5.hist(target4, bins=100, alpha=0.2, color='grey')\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# for train/test data\n",
    "train_data = pd.read_csv(\"./input/train_data.csv\")\n",
    "test_data = pd.read_csv(\"./input/test_data.csv\")\n",
    "\n",
    "y = np.log1p(train_data['y']).copy()\n",
    "y_bin = pd.cut(train_data['y'], [0, 10, 100,1000,10000,100000,1000000,10000000000], labels=[1,2,3,4,5,6,7])\n",
    "y_bin = y_bin.astype(int)\n",
    "test_id = test_data.id\n",
    "\n",
    "train = train_data.drop(['id', 'y'], axis=1).copy()\n",
    "test  = test_data.drop(['id'], axis=1).copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 目的変数の分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_hist(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# seedの固定化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=1234):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIFF_THRESHOLD = 5\n",
    "\n",
    "################################################################################\n",
    "# RESULTS\n",
    "################################################################################\n",
    "def output_results(target, results, test_id, MODEL):\n",
    "    RMSLE = mean_squared_error(target.values, results['train'], squared=False)\n",
    "    print(f\"Overall RMSLE={RMSLE}\")\n",
    "\n",
    "    # Make submission\n",
    "    print(\"Saving submission file\")\n",
    "    submission = pd.DataFrame({'id': test_id, 'y': np.expm1(results['test'])})\n",
    "    submission.to_csv(f\"./{out_dir}/submission_{MODEL}_CV{RMSLE:.6f}.csv\", index=False)\n",
    "        \n",
    "    return submission\n",
    "\n",
    "def check_results(y, results):\n",
    "    y_diff = np.abs(np.expm1(y) - np.expm1(results[\"train\"]))\n",
    "    y_log1p_diff = np.abs(y - results[\"train\"])\n",
    "    display(y_diff[y_log1p_diff>DIFF_THRESHOLD].index.values)\n",
    "    display(train_data[y_log1p_diff>DIFF_THRESHOLD])\n",
    "    display(pd.concat([pd.DataFrame(y[y_log1p_diff>DIFF_THRESHOLD], columns=['y']), \\\n",
    "                       pd.DataFrame(results[\"train\"][y_log1p_diff>DIFF_THRESHOLD], \\\n",
    "                                    index=y_diff[y_log1p_diff>DIFF_THRESHOLD].index.values, columns=[\"pred_train\"])], axis=1))    \n",
    "\n",
    "    get_hist4(results[\"train\"], \"pred_train\", \\\n",
    "              y, \"y\", \\\n",
    "              results[\"test\"], \"pred_test\", \\\n",
    "              y_log1p_diff, \"diff\")\n",
    "    \n",
    "    display(pd.concat([pd.DataFrame(results[\"train\"], columns=[\"pred_train\"]), \\\n",
    "                       pd.DataFrame(y, columns=[\"y\"]), \\\n",
    "                       y_log1p_diff.rename(\"y_log1p_diff\")], \\\n",
    "                       axis=1).describe())\n",
    "    \n",
    "    display(pd.DataFrame(results[\"test\"], columns=[\"pred_test\"]).describe())\n",
    "    \n",
    "    RMSLE = mean_squared_error(y, results[\"train\"], squared=False)\n",
    "    display(f\"Overall RMSLE={RMSLE:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIFF_THRESHOLD = 5\n",
    "\n",
    "################################################################################\n",
    "# METRICS\n",
    "################################################################################\n",
    "def rmsle(y, pred_y):\n",
    "    return mean_squared_error(y, pred_y, squared=False)\n",
    "\n",
    "################################################################################\n",
    "# CROSS-VALIDATION\n",
    "################################################################################\n",
    "def print_cv_scores(label, cv_scores):\n",
    "    print(\"*\"*40)\n",
    "    print(f\"type(cv_scores): {type(cv_scores)}\")\n",
    "    print(f\"{label} cv scores : {cv_scores}\")\n",
    "    print(f\"{label} cv mean score : {np.mean(cv_scores)}\")\n",
    "    print(f\"{label} cv std score : {np.std(cv_scores)}\")\n",
    "    \n",
    "def run_cv_model(train, test, target, target_skf, encoding, model_fn, params={}, \n",
    "                 eval_fn=None, label='model', cv=5,  repeats=5, seed=43):\n",
    "\n",
    "    if repeats==1:\n",
    "        if target_skf is None:\n",
    "            kf = KFold(n_splits=cv, shuffle=True, random_state=seed)\n",
    "            target_y = target\n",
    "        else:\n",
    "            kf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=seed)\n",
    "            target_y = target_skf\n",
    "        divide_counts = cv\n",
    "    else:\n",
    "        if target_skf is None:\n",
    "            kf = RepeatedKFold(n_splits=cv,n_repeats=repeats, random_state=seed)\n",
    "            target_y = target\n",
    "        else:\n",
    "            kf = RepeatedStratifiedKFold(n_splits=cv, n_repeats=repeats, random_state=seed)\n",
    "            target_y = target_skf\n",
    "        divide_counts = kf.get_n_splits()\n",
    "        \n",
    "    cv_scores = []\n",
    "    pred_full_test = 0\n",
    "    pred_train = np.zeros((train.shape[0]))\n",
    "\n",
    "    for fold_id, (train_idx, val_idx) in enumerate(kf.split(train, target_y)):\n",
    "        print(\"*\"*40)\n",
    "        print(f\"Started {label} fold:{fold_id+1} / {divide_counts}\")\n",
    "        tr_X, val_X = train.iloc[train_idx].copy(), train.iloc[val_idx].copy()\n",
    "        tr_y, val_y = target.iloc[train_idx], target.iloc[val_idx]\n",
    "        \n",
    "        # TARGET ENCODING\n",
    "        if encoding:\n",
    "            for c in encoding:\n",
    "                # 学習データ全体で各カテゴリにおけるtargetの平均を計算\n",
    "                data_tmp = pd.DataFrame({c: tr_X[c], 'target': tr_y})\n",
    "                target_mean = data_tmp.groupby(c)['target'].mean()\n",
    "                # バリデーションデータのカテゴリを置換\n",
    "                val_X.loc[:, c] = val_X[c].map(target_mean)\n",
    "\n",
    "                # 学習データの変換後の値を格納する配列を準備\n",
    "                tmp = np.repeat(np.nan, tr_X.shape[0])\n",
    "                kf_encoding = KFold(n_splits=4, shuffle=True, random_state=seed)\n",
    "                for idx_1, idx_2 in kf_encoding.split(tr_X):\n",
    "                    # out-of-foldで各カテゴリにおける目的変数の平均を計算\n",
    "                    target_mean = data_tmp.iloc[idx_1].groupby(c)['target'].mean()\n",
    "                    # 変換後の値を一次配列に格納\n",
    "                    tmp[idx_2] = tr_X[c].iloc[idx_2].map(target_mean)\n",
    "\n",
    "\n",
    "                tr_X.loc[:, c] = tmp\n",
    "        # TARGET ENCODING\n",
    "        \n",
    "        params2 = params.copy() \n",
    "        model, pred_val_y, pred_test_y = model_fn(\n",
    "            tr_X, tr_y, val_X, val_y, test, params2)\n",
    "        \n",
    "        pred_full_test = pred_full_test + pred_test_y\n",
    "        pred_train[val_idx] = pred_val_y\n",
    "        if eval_fn is not None:\n",
    "            cv_score = eval_fn(val_y, pred_val_y)\n",
    "            cv_scores.append(cv_score)\n",
    "            print(f\"{label} cv score {fold_id+1}: {cv_score}\")\n",
    "            \n",
    "    \n",
    "    print_cv_scores(label, cv_scores)    \n",
    "    pred_full_test = pred_full_test / divide_counts\n",
    "    results = {\"label\": label,\n",
    "               \"train\": pred_train, \n",
    "               \"test\": pred_full_test, \n",
    "               \"cv\": cv_scores}\n",
    "    \n",
    "    RMSLE = mean_squared_error(target.values, results[\"train\"], squared=False)\n",
    "    print(f\"Overall RMSLE={RMSLE}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "################################################################################\n",
    "# RESULTS\n",
    "################################################################################\n",
    "def output_results(target, results, test_id, MODEL):\n",
    "    RMSLE = mean_squared_error(target.values, results[\"train\"], squared=False)\n",
    "    print(f\"Overall RMSLE={RMSLE}\")\n",
    "\n",
    "    # Make submission\n",
    "    print(\"Saving submission file\")\n",
    "    submission = pd.DataFrame({'id': test_id, 'y': np.expm1(results[\"test\"])})\n",
    "    submission.to_csv(f\"./{out_dir}/submission_{MODEL}_CV{RMSLE:.6f}.csv\", index=False)\n",
    "        \n",
    "    return submission\n",
    "\n",
    "def check_results(y, results):\n",
    "    y_diff = np.abs(np.expm1(y) - np.expm1(results[\"train\"]))\n",
    "    y_log1p_diff = np.abs(y - results[\"train\"])\n",
    "    display(y_diff[y_log1p_diff>DIFF_THRESHOLD].index.values)\n",
    "    display(train_data[y_log1p_diff>DIFF_THRESHOLD])\n",
    "    display(pd.concat([pd.DataFrame(y[y_log1p_diff>DIFF_THRESHOLD], columns=['y']), \\\n",
    "                       pd.DataFrame(results[\"train\"][y_log1p_diff>DIFF_THRESHOLD], \\\n",
    "                                    index=y_diff[y_log1p_diff>DIFF_THRESHOLD].index.values, columns=[\"pred_train\"])], axis=1))    \n",
    "\n",
    "    get_hist4(results[\"train\"], \"pred_train\", \\\n",
    "              y, \"y\", \\\n",
    "              results[\"test\"], \"pred_test\", \\\n",
    "              y_log1p_diff, \"diff\")\n",
    "    \n",
    "    display(pd.concat([pd.DataFrame(results[\"train\"], columns=[\"pred_train\"]), \\\n",
    "                       pd.DataFrame(y, columns=[\"y\"]), \\\n",
    "                       y_log1p_diff.rename(\"y_log1p_diff\")], \\\n",
    "                       axis=1).describe())\n",
    "    \n",
    "    display(pd.DataFrame(results[\"test\"], columns=[\"pred_test\"]).describe())\n",
    "    \n",
    "    RMSLE = mean_squared_error(y, results[\"train\"], squared=False)\n",
    "    display(f\"Overall RMSLE={RMSLE:.6f}\")\n",
    "    \n",
    "################################################################################\n",
    "# MODEL\n",
    "################################################################################\n",
    "def runLGB(train_X, train_y, val_X, val_y, test_X, params):\n",
    "    model = lgb.LGBMRegressor(**params)\n",
    "    model.fit(train_X, train_y, eval_set=(val_X, val_y), early_stopping_rounds=100, eval_metric='rmse', verbose=100)\n",
    "    pred_val_y = model.predict(val_X)\n",
    "    pred_test_y = model.predict(test_X)\n",
    "    return model, pred_val_y, pred_test_y\n",
    "\n",
    "def runXGB(train_X, train_y, val_X, val_y, test_X, params):\n",
    "    model = xgb.XGBRegressor(**params)\n",
    "    model.fit(train_X, train_y, eval_set=[[val_X, val_y]], early_stopping_rounds=100, eval_metric='rmse', verbose=100)\n",
    "    pred_val_y = model.predict(val_X)\n",
    "    pred_test_y = model.predict(test_X)\n",
    "    return model, pred_val_y, pred_test_y\n",
    "\n",
    "def runCAT(train_X, train_y, val_X, val_y, test_X, params):\n",
    "    model = CatBoostRegressor(**params)\n",
    "    model.fit(train_X, train_y, eval_set=(val_X, val_y),\n",
    "#              cat_features=cat_features,\n",
    "              early_stopping_rounds=100, use_best_model=True, verbose=100)\n",
    "    pred_val_y = model.predict(val_X)\n",
    "    pred_test_y = model.predict(test_X)\n",
    "    return model, pred_val_y, pred_test_y\n",
    "\n",
    "def runNGB(train_X, train_y, val_X, val_y, test_X, params):\n",
    "    model = NGBRegressor(**ngb_params)\n",
    "    model.fit(train_X, train_y, X_val=val_X, Y_val=val_y)\n",
    "    pred_val_y = model.predict(val_X)\n",
    "    pred_test_y = model.predict(test_X)\n",
    "    return model, pred_val_y, pred_test_y\n",
    "\n",
    "def runLR(train_X, train_y, val_X, val_y, test_X, params):\n",
    "    model = LogisticRegression(**params)\n",
    "    model.fit(train_X, train_y, sample_weight=None)\n",
    "    pred_val_y = model.predict_proba(val_X)[:, 1]\n",
    "    pred_test_y = model.predict_proba(test_X)[:, 1]\n",
    "    return model, pred_val_y, pred_test_y\n",
    "\n",
    "def runLINR(train_X, train_y, val_X, val_y, test_X, params):\n",
    "    model = LinearRegression(**params)\n",
    "    model.fit(train_X, train_y, sample_weight=None)\n",
    "    pred_val_y = model.predict(val_X)\n",
    "    pred_test_y = model.predict(test_X)\n",
    "    return model, pred_val_y, pred_test_y\n",
    "\n",
    "def runBAYRIDGE(train_X, train_y, val_X, val_y, test_X, params):\n",
    "    model = BayesianRidge(**params)\n",
    "    model.fit(train_X, train_y, sample_weight=None)\n",
    "    pred_val_y = model.predict(val_X)\n",
    "    pred_test_y = model.predict(test_X)\n",
    "    return model, pred_val_y, pred_test_y\n",
    "\n",
    "def runRDG(train_X, train_y, val_X, val_y, test_X, params):\n",
    "    model = Ridge(**params)\n",
    "    model.fit(train_X, train_y, sample_weight=None)\n",
    "    pred_val_y = model.predict(val_X)\n",
    "    pred_test_y = model.predict(test_X)\n",
    "    return model, pred_val_y, pred_test_y\n",
    "\n",
    "def runELASTIC(train_X, train_y, val_X, val_y, test_X, params):\n",
    "    model = ElasticNet(**params)\n",
    "    model.fit(train_X, train_y, check_input=True)\n",
    "    pred_val_y = model.predict(val_X)\n",
    "    pred_test_y = model.predict(test_X)\n",
    "    return model, pred_val_y, pred_test_y\n",
    "\n",
    "def runLASSO(train_X, train_y, val_X, val_y, test_X, params):\n",
    "    model = Lasso(**params)\n",
    "    model.fit(train_X, train_y, check_input=True)\n",
    "    pred_val_y = model.predict(val_X)\n",
    "    pred_test_y = model.predict(test_X)\n",
    "    return model, pred_val_y, pred_test_y\n",
    "\n",
    "def runKN(train_X, train_y, val_X, val_y, test_X, params):\n",
    "    model = KNeighborsRegressor(**params)\n",
    "    model.fit(train_X, train_y)\n",
    "    pred_val_y = model.predict(val_X)\n",
    "    pred_test_y = model.predict(test_X)\n",
    "    return model, pred_val_y, pred_test_y\n",
    "\n",
    "def runRFR(train_X, train_y, val_X, val_y, test_X, params):\n",
    "    model = RandomForestRegressor(**params)\n",
    "    model.fit(train_X, train_y)\n",
    "    pred_val_y = model.predict(val_X)\n",
    "    pred_test_y = model.predict(test_X)\n",
    "    return model, pred_val_y, pred_test_y\n",
    "\n",
    "def runSGD(train_X, train_y, val_X, val_y, test_X, params):\n",
    "    model = SGDRegressor(**params)\n",
    "    model.fit(train_X, train_y, coef_init=None, intercept_init=None, sample_weight=None)\n",
    "    pred_val_y = model.predict(val_X)\n",
    "    pred_test_y = model.predict(test_X)\n",
    "    return model, pred_val_y, pred_test_y\n",
    "\n",
    "def runETR(train_X, train_y, val_X, val_y, test_X, params):\n",
    "    model = ExtraTreesRegressor(**params)\n",
    "    model.fit(train_X, train_y)\n",
    "    pred_val_y = model.predict(val_X)\n",
    "    pred_test_y = model.predict(test_X)\n",
    "    return model, pred_val_y, pred_test_y\n",
    "\n",
    "\n",
    "def runGBR(train_X, train_y, val_X, val_y, test_X, params):\n",
    "    model = GradientBoostingRegressor(**params)\n",
    "    model.fit(train_X, train_y)\n",
    "    pred_val_y = model.predict(val_X)\n",
    "    pred_test_y = model.predict(test_X)\n",
    "    return model, pred_val_y, pred_test_y\n",
    "\n",
    "def runBAG(train_X, train_y, val_X, val_y, test_X, params):\n",
    "    model = BaggingRegressor(**params)\n",
    "    model.fit(train_X, train_y, sample_weight=None)\n",
    "    pred_val_y = model.predict(val_X)\n",
    "    pred_test_y = model.predict(test_X)\n",
    "    return model, pred_val_y, pred_test_y\n",
    "\n",
    "def runABR(train_X, train_y, val_X, val_y, test_X, params):\n",
    "    model = AdaBoostRegressor(**params)\n",
    "    model.fit(train_X, train_y, sample_weight=None)\n",
    "    pred_val_y = model.predict(val_X)\n",
    "    pred_test_y = model.predict(test_X)\n",
    "    return model, pred_val_y, pred_test_y\n",
    "\n",
    "def runLINSVR(train_X, train_y, val_X, val_y, test_X, params):\n",
    "    model = LinearSVR(**params)\n",
    "    model.fit(train_X, train_y, sample_weight=None)\n",
    "    pred_val_y = model.predict(val_X)\n",
    "    pred_test_y = model.predict(test_X)\n",
    "    return model, pred_val_y, pred_test_y\n",
    "\n",
    "################################################################################\n",
    "# MODEL PARAMETERS\n",
    "################################################################################\n",
    "lgb_params = {'boosting_type': 'gbdt', 'tree_learner': 'feature', #''serial' or feature' or 'data' or 'voting'\n",
    "              'num_leaves': 31, 'max_depth': -1,\n",
    "              'learning_rate': 5e-2, 'n_estimators': 10000, 'importance_type': 'gain',\n",
    "              'subsample_for_bin': 200000, 'objective': 'regression', 'min_split_gain': 0.0, 'min_child_weight': 1e-3, 'min_child_samples': 20, \n",
    "              'bagging_freq': 0, 'bagging_fraction': 1.0, 'feature_fraction': 1.0,\n",
    "              'reg_alpha': 0.2, 'reg_lambda': 0.2,\n",
    "              'random_state': 43, 'data_random_seed': 1,\n",
    "              'n_jobs': -1, 'silent': False}\n",
    "\n",
    "xgb_params = {'base_score': 0.5, 'booster': 'gbtree', 'colsample_bylevel': 1, 'colsample_bynode': 1, 'colsample_bytree': 1, 'gamma': 0,\n",
    "              'learning_rate': 5e-2, 'n_estimators': 20000, 'importance_type': 'gain',\n",
    "              'max_delta_step': 0, 'max_depth': 6, 'min_child_weight': 0,\n",
    "              'objective': 'reg:squarederror', 'reg_alpha': 0.2, 'reg_lambda': 0.2, 'scale_pos_weight': 1,\n",
    "              'subsample': 0.9,\n",
    "              'silent': None, 'verbosity': 0,\n",
    "              'random_state': 43, 'seed': 43,\n",
    "              'tree_method': 'gpu_hist', 'gpu_id': 0}\n",
    "\n",
    "cat_params = {'iterations':10000, 'depth': 8, 'boosting_type': 'Ordered', #'Ordered', #'Plain',\n",
    "              'loss_function': 'RMSE', 'eval_metric': 'RMSE',\n",
    "              'learning_rate': 5e-2, 'leaf_estimation_method': 'Gradient', #'Newton', 'Exact'\n",
    "              'l2_leaf_reg': 1.0, 'random_strength': 1.0, 'bagging_temperature': 1.0, 'has_time': False,\n",
    "              'grow_policy': 'SymmetricTree', #'Depthwise', 'Lossguide'\n",
    "              'min_data_in_leaf': 1, 'max_leaves': 31,\n",
    "              'random_seed': 43,\n",
    "#              'one_hot_max_size': len(cat_features),\n",
    "              'task_type': 'GPU'}\n",
    "\n",
    "ngb_params = {'Base': default_tree_learner, #決定木。Ridge回帰の場合は、default_linear_learner\n",
    "              'Dist': Normal, \n",
    "              'Score': LogScore, #CRPS, MLEも可\n",
    "              'learning_rate': 1e-2, 'natural_gradient': True, 'verbose': True, 'verbose_eval': 100,\n",
    "              'tol': 1e-4, 'random_state': 43, 'n_estimators': 100, 'minibatch_frac': 0.5}\n",
    "\n",
    "logr_params = {'penalty':'l2',  'solver': 'newton-cg', #'newton-cg', 'lbfgs', 'sag' , 'saga'\n",
    "               'C': 0.05,\n",
    "#               'class_weight':'balanced', \n",
    "               'max_iter': 500, 'random_state': 43, 'n_jobs': -1}\n",
    "\n",
    "linr_params = {'fit_intercept': True, 'normalize': False, 'copy_X': True, 'n_jobs': -1}\n",
    "\n",
    "bayridge_params = {'alpha_1': 1e-06, 'alpha_2': 1e-06, 'alpha_init': None, 'compute_score': False,\n",
    "                   'copy_X': True, 'fit_intercept': True,\n",
    "                   'lambda_1': 1e-06, 'lambda_2': 1e-06, 'lambda_init': None,\n",
    "                   'n_iter': 200, 'normalize': False, 'tol': 1e-3,\n",
    "                   'verbose': True}\n",
    "\n",
    "rdg_params = {'alpha': 0.01, 'copy_X': True, 'fit_intercept': True,\n",
    "              'max_iter': 100, 'normalize': False,\n",
    "              'random_state': 43, 'solver': 'auto', 'tol': 1e-3}\n",
    "\n",
    "elastic_params = {'alpha': 0.0001, 'copy_X': True, 'fit_intercept': True, 'l1_ratio': 0.5,\n",
    "                  'max_iter': 200, 'normalize': False, 'positive': False, 'precompute': False,\n",
    "                  'random_state': 43, 'selection': 'cyclic', 'tol': 1e-4, 'warm_start': False}\n",
    "\n",
    "lasso_params = {'alpha': 0.0001, 'copy_X': True, 'fit_intercept': True, 'max_iter': 200,\n",
    "                'normalize': False, 'positive': False, 'precompute': False,\n",
    "                'random_state': 43, 'selection': 'random', 'tol': 1e-4, 'warm_start': False}\n",
    "\n",
    "sgd_params = {'alpha': 1e-4, 'average': False, 'early_stopping': True,\n",
    "              'epsilon': 1e-1, 'eta0': 1e-4, 'fit_intercept': True, 'l1_ratio': 0.15,\n",
    "              'learning_rate': 'invscaling', 'loss': 'squared_loss', 'penalty': 'l2', 'power_t': 0.25,\n",
    "              'max_iter': 3000, 'n_iter_no_change': 10, 'validation_fraction': 0.5,\n",
    "              'random_state': 43, 'shuffle': True, 'tol': 1e-3, 'verbose': False, 'warm_start': False}\n",
    "kn_params = {'n_neighbors': 5, 'weights': 'uniform', 'algorithm': 'auto',\n",
    "             'leaf_size': 30, 'p': 2, 'metric': 'minkowski', 'metric_params': None, 'n_jobs': -1}\n",
    "\n",
    "rfr_params = {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'mse',\n",
    "             'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None,\n",
    "             'min_impurity_decrease': 0.0, 'min_impurity_split': 1e-7, 'max_samples': None,\n",
    "             'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0,\n",
    "             'n_estimators': 1000, 'n_jobs': -1, 'oob_score': False,\n",
    "             'random_state': 43, 'verbose': 1, 'warm_start': False}\n",
    "\n",
    "etr_params = {'bootstrap': False, 'ccp_alpha': 0.0, 'criterion': 'mse',\n",
    "              'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None,\n",
    "              'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'max_samples': None,\n",
    "              'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0,  \n",
    "              'n_estimators': 100, 'n_jobs': -1, 'oob_score': False,\n",
    "              'random_state': 43, 'verbose': 1, 'warm_start': False}\n",
    "\n",
    "gbr_params = {'alpha': 0.9, 'ccp_alpha': 0.0, 'criterion': 'friedman_mse', 'init': None,\n",
    "              'learning_rate': 5e-2, 'n_estimators': 200, 'loss': 'ls',\n",
    "              'max_depth': 6, 'max_features': None, 'max_leaf_nodes': None,\n",
    "              'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2,\n",
    "              'min_weight_fraction_leaf': 0.0, 'subsample': 1.0, 'validation_fraction': 0.2,\n",
    "              'n_iter_no_change': None, 'presort': 'deprecated',\n",
    "              'random_state': 43, 'tol': 1e-4, 'verbose': 1, 'warm_start': False}\n",
    "\n",
    "bag_params = {'base_estimator': None,\n",
    "              'bootstrap': True, 'bootstrap_features': False, 'max_features': 1.0, 'max_samples': 1.0,\n",
    "              'n_estimators': 5, 'n_jobs': None, 'oob_score': False, \n",
    "              'random_state': 43, 'verbose': 1, 'warm_start': False}\n",
    "\n",
    "abr_params = {'base_estimator': None, \n",
    "              'learning_rate': 1.0, 'loss': 'linear', 'n_estimators': 5,\n",
    "              'random_state': 43}\n",
    "\n",
    "linsvr_params = {'epsilon': 0.0, 'tol': 0.0001, 'C': 1.0,\n",
    "                 'loss': 'epsilon_insensitive', 'fit_intercept': True, 'intercept_scaling': 1.0,\n",
    "                 'dual': True, 'verbose': 1, 'random_state': 43, 'max_iter': 1000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_regressor(train, test, target, target_skf, seed, n_folds, encoding):\n",
    "    lgb_params = {'boosting_type': 'gbdt',\n",
    "                  'objective' : 'regression',\n",
    "                  'metric' : 'rmse',  \n",
    "                  'tree_learner': 'feature', #''serial' or feature' or 'data' or 'voting'\n",
    "                  'max_depth': -1,\n",
    "                  'min_child_samples': 10, \n",
    "                  'min_split_gain': 0.01,\n",
    "                  'min_child_weight': 1e-2,\n",
    "                  'reg_alpha': 0.1,\n",
    "                  'reg_lambda': 1,\n",
    "                  'num_leaves': 35,\n",
    "                  'max_bin': 300,\n",
    "                  'learning_rate': 2e-2,\n",
    "                  'bagging_fraction': 0.9,\n",
    "                  'bagging_freq': 1,\n",
    "                  'bagging_seed': 4590,\n",
    "                  'feature_fraction': 0.85,\n",
    "                  'n_estimators': 50000,\n",
    "                  'importance_type': 'gain',\n",
    "                  'subsample_for_bin': 200000,\n",
    "                  'random_state': seed,\n",
    "                  'data_random_seed': seed,\n",
    "                  'n_jobs': -1,\n",
    "                  'silent': False}\n",
    "    \n",
    "    lgb_results = run_cv_model(train, test, target, target_skf, encoding, runLGB, lgb_params, rmsle, 'LGBMRegressor', cv=n_folds, repeats=1, seed=seed)\n",
    "    return lgb_results\n",
    "\n",
    "def xgb_regressor(train, test, target, target_skf, seed, n_folds, encoding):\n",
    "    xgb_params = {'base_score': 0.5,\n",
    "                  'booster': 'gbtree',\n",
    "                  'objective': 'reg:squarederror',\n",
    "                  'colsample_bylevel': 0.6,\n",
    "                  'colsample_bynode': 0.6,\n",
    "                  'colsample_bytree': 0.6,\n",
    "                  'gamma': 0,\n",
    "                  'learning_rate': 1e-2,\n",
    "                  'n_estimators': 50000,\n",
    "                  'importance_type': 'gain',\n",
    "                  'max_delta_step': 0,\n",
    "                  'max_depth': 8,\n",
    "                  'min_child_weight': 0,\n",
    "                  'reg_alpha': 0.1,\n",
    "                  'reg_lambda': 1,\n",
    "                  'scale_pos_weight': 1,\n",
    "                  'subsample': 0.8,\n",
    "                  'silent': None,\n",
    "                  'verbosity': 0,\n",
    "                  'random_state': seed,\n",
    "                  'seed': seed,\n",
    "                  'tree_method': 'gpu_hist',\n",
    "                  'gpu_id': 0}\n",
    "\n",
    "    xgb_results = run_cv_model(train, test, target, target_skf, encoding, runXGB, xgb_params, rmsle, 'XGBRegressor', cv=n_folds, repeats=1, seed=seed)\n",
    "    return xgb_results\n",
    "\n",
    "def catboost_regressor(train, test, target, target_skf, seed, n_folds, encoding):\n",
    "    cat_params = {'bootstrap_type': 'Bayesian', \n",
    "                  'boosting_type': 'Plain', #'Ordered', #'Plain',\n",
    "                  'iterations':50000,\n",
    "                  'depth': 8,\n",
    "                  'loss_function': 'RMSE',\n",
    "                  'eval_metric': 'RMSE',\n",
    "                  'learning_rate': 1e-2,\n",
    "                  'leaf_estimation_method': 'Gradient', #'Newton', 'Exact'\n",
    "                  'l2_leaf_reg': 1.0,\n",
    "                  'random_strength': 0.8,\n",
    "                  'bagging_temperature': 0.9,\n",
    "                  'has_time': False,\n",
    "                  'grow_policy': 'SymmetricTree', #'Depthwise', 'Lossguide'\n",
    "                  'min_data_in_leaf': 1,\n",
    "                  'max_leaves': 31,\n",
    "                  'random_seed': seed,\n",
    "                  #'one_hot_max_size': len(cat_features),\n",
    "                  'task_type': 'GPU'}\n",
    "\n",
    "    cat_results = run_cv_model(train, test, target, target_skf, encoding, runCAT, cat_params, rmsle, 'CatBoostRegressor', cv=n_folds, repeats=1, seed=seed)\n",
    "    return cat_results\n",
    "\n",
    "def ngboost_regressor(train, test, target, target_skf, seed, n_folds, encoding):\n",
    "    ngb_params['learning_rate'] = 5e-2\n",
    "    ngb_params['n_estimators'] = 500\n",
    "    ngb_params['minibatch_frac'] = 1.0\n",
    "    ngb_params['random_state'] = seed\n",
    "\n",
    "    ngb_results = run_cv_model(train, test, target, target_skf, encoding, runNGB, ngb_params, rmsle, 'NGBoost', cv=n_folds, repeats=1, seed=seed)\n",
    "    return ngb_results\n",
    "\n",
    "def logistic_regression(train, test, target, target_skf, seed, n_folds, encoding):\n",
    "    logr_params['max_iter'] = 500\n",
    "    logr_params['random_state'] = seed\n",
    "\n",
    "    logr_results = run_cv_model(train, test, target, target_skf, encoding, runLR, logr_params, rmsle, 'LogisticRegression', cv=n_folds, repeats=1, seed=seed)\n",
    "    return logr_results\n",
    "\n",
    "def lin_regression(train, test, target, target_skf, seed, n_folds, encoding):\n",
    "    linr_params['n_jobs'] = -1\n",
    "\n",
    "    linr_results = run_cv_model(train, test, target, target_skf, encoding, runLINR, linr_params, rmsle, 'LinearRegression', cv=n_folds, repeats=1, seed=seed)\n",
    "    return linr_results\n",
    "\n",
    "def bayesianridge(train, test, target, target_skf, seed, n_folds, encoding):\n",
    "    bayridge_params['alpha_1'] = 1e-06\n",
    "    bayridge_params['alpha_2'] = 1e-06\n",
    "    bayridge_params['lambda_1'] = 1.0\n",
    "    bayridge_params['lambda_2'] = 1e-07\n",
    "    bayridge_params['n_iter'] = 1000\n",
    "\n",
    "    bay_results = run_cv_model(train, test, target, target_skf, encoding, runBAYRIDGE, bayridge_params, rmsle, 'BayesianRidge', cv=n_folds, repeats=1, seed=seed)\n",
    "    return bay_results\n",
    "\n",
    "def ridge(train, test, target, target_skf, seed, n_folds, encoding):\n",
    "    rdg_params['alpha'] = 1.0\n",
    "    rdg_params['random_state'] = seed\n",
    "    rdg_params['max_iter'] = 1000\n",
    "\n",
    "    rdg_results = run_cv_model(train, test, target, target_skf, encoding, runRDG, rdg_params, rmsle, 'Ridge', cv=n_folds, repeats=1, seed=seed)\n",
    "    return rdg_results\n",
    "\n",
    "def elastic(train, test, target, target_skf, seed, n_folds, encoding):\n",
    "    elastic_params['alpha'] = 1e-04\n",
    "    elastic_params['l1_ratio'] = 0.5\n",
    "    elastic_params['random_state'] = seed\n",
    "    elastic_params['max_iter'] = 1000\n",
    "\n",
    "    elastic_results = run_cv_model(train, test, target, target_skf, encoding, runELASTIC, elastic_params, rmsle, 'ELasticNet', cv=n_folds, repeats=1, seed=seed)\n",
    "    return elastic_results\n",
    "\n",
    "def lasso(train, test, target, target_skf, seed, n_folds, encoding):\n",
    "    lasso_params['alpha'] = 1e-04\n",
    "    lasso_params['random_state'] = seed\n",
    "    lasso_params['max_iter'] = 1000\n",
    "\n",
    "    lasso_results = run_cv_model(train, test, target, target_skf, encoding, runLASSO, lasso_params, rmsle, 'Lasso', cv=n_folds, repeats=1, seed=seed)\n",
    "    return lasso_results\n",
    "\n",
    "def sgd(train, test, target, target_skf, seed, n_folds, encoding):\n",
    "    sgd_params['alpha'] = 1e-04\n",
    "    sgd_params['early_stopping'] = True\n",
    "    sgd_params['epsilon'] = 1e-1\n",
    "    sgd_params['eta0'] = 1e-4\n",
    "    sgd_params['l1_ratio'] = 0.15\n",
    "    sgd_params['learning_rate'] = 'invscaling'\n",
    "    sgd_params['loss'] = 'squared_loss'\n",
    "    sgd_params['validation_fraction'] = 0.2\n",
    "    sgd_params['random_state'] = seed\n",
    "\n",
    "    sgd_results = run_cv_model(train, test, target, target_skf, encoding, runSGD, sgd_params, rmsle, 'SGD', cv=n_folds, repeats=1, seed=seed)\n",
    "    return sgd_results\n",
    "\n",
    "def kn_regressor(train, test, target, target_skf, seed, n_folds, encoding):\n",
    "    kn_params['n_neighbors'] = 5\n",
    "    kn_params['weights'] = 'distance'\n",
    "    kn_params['algorithm'] = 'auto' #auto, ball_tree, kd_tree, brute\n",
    "    kn_params['leaf_size'] = 60\n",
    "\n",
    "    kn_results = run_cv_model(train, test, target, target_skf, encoding, runKN, kn_params, rmsle, 'KNeighbors', cv=n_folds, repeats=1, seed=seed)\n",
    "    return kn_results\n",
    "\n",
    "def rf_regressor(train, test, target, target_skf, seed, n_folds, encoding):\n",
    "    rfr_params['ccp_alpha'] = 0\n",
    "    rfr_params['criterion'] = 'mse'\n",
    "    rfr_params['max_depth'] = 63\n",
    "    rfr_params['min_samples_leaf'] = 20\n",
    "    rfr_params['min_samples_split'] = 50\n",
    "    rfr_params['random_state'] = seed\n",
    "    \n",
    "    rfr_results = run_cv_model(train, test, target, target_skf, encoding, runRFR, rfr_params, rmsle, 'RandomForestRegressor', cv=n_folds, repeats=1, seed=seed)\n",
    "    return rfr_results\n",
    "\n",
    "def et_regressor(train, test, target, target_skf, seed, n_folds, encoding):\n",
    "    etr_params['ccp_alpha'] = 0\n",
    "    etr_params['criterion'] = 'mse'\n",
    "    etr_params['max_depth'] = 63\n",
    "    etr_params['min_samples_leaf'] = 20\n",
    "    etr_params['min_samples_split'] = 50\n",
    "    etr_params['min_weight_fraction_leaf'] = 0.0\n",
    "    etr_params['n_estimators'] = 1000\n",
    "    etr_params['random_state'] = seed\n",
    "    \n",
    "    etr_results = run_cv_model(train, test, target, target_skf, encoding, runETR, etr_params, rmsle, 'ExtraTreesRegressor', cv=n_folds, repeats=1, seed=seed)\n",
    "    return etr_results\n",
    "\n",
    "def gb_regressor(train, test, target, target_skf, seed, n_folds, encoding):\n",
    "    gbr_params['alpha'] = 0.9\n",
    "    gbr_params['ccp_alpha'] = 0\n",
    "    gbr_params['criterion'] = 'friedman_mse'\n",
    "    gbr_params['learning_rate'] = 5e-2\n",
    "    gbr_params['n_estimators'] = 100\n",
    "    gbr_params['max_depth'] = 31\n",
    "    gbr_params['min_samples_leaf'] = 1\n",
    "    gbr_params['min_samples_split'] = 2\n",
    "    gbr_params['min_weight_fraction_leaf'] = 0.0\n",
    "    gbr_params['subsample'] = 1.0\n",
    "    gbr_params['validation_fraction'] = 0.2\n",
    "    gbr_params['random_state'] = seed\n",
    "    \n",
    "    gbr_results = run_cv_model(train, test, target, target_skf, encoding, runGBR, gbr_params, rmsle, 'GradientBoostingRegressor', cv=n_folds, repeats=1, seed=seed)\n",
    "    return gbr_results\n",
    "\n",
    "def bag_regressor(train, test, target, target_skf, seed, n_folds, encoding):\n",
    "    bag_params['base_estimator'] = BayesianRidge(n_iter=1000, lambda_1=1.0, lambda_2=1e-7)\n",
    "    bag_params['bootstrap'] = True\n",
    "    bag_params['bootstrap_features'] = True,\n",
    "    bag_params['max_features'] = 1.0\n",
    "    bag_params['max_samples'] = 1.0\n",
    "    bag_params['n_estimators'] = 96\n",
    "    bag_params['n_jobs'] = -1\n",
    "    bag_params['random_state'] = seed\n",
    "    \n",
    "    bag_results = run_cv_model(train, test, target, target_skf, encoding, runBAG, bag_params, rmsle, 'BaggingRegressor', cv=n_folds, repeats=1, seed=seed)\n",
    "    return bag_results\n",
    "\n",
    "def ada_regressor(train, test, target, target_skf, seed, n_folds, encoding):\n",
    "    abr_params['base_estimator'] = BayesianRidge(n_iter=1000, lambda_1=1.0, lambda_2=1e-7)\n",
    "    abr_params['learning_rate'] = 2.0\n",
    "    abr_params['loss'] = 'linear'\n",
    "    abr_params['n_estimators'] = 100\n",
    "    abr_params['random_state'] = seed\n",
    "    \n",
    "    abr_results = run_cv_model(train, test, target, target_skf, encoding, runABR, abr_params, rmsle, 'AdaBoostRegressor', cv=n_folds, repeats=1, seed=seed)\n",
    "    return abr_results\n",
    "\n",
    "def lin_svr(train, test, target, target_skf, seed, n_folds, encoding):\n",
    "    linsvr_params['loss'] = 'squared_epsilon_insensitive'\n",
    "    linsvr_params['max_iter'] = 1000\n",
    "    linsvr_params['random_state'] = seed\n",
    "    \n",
    "    linsvr_results = run_cv_model(train, test, target, target_skf, encoding, runLINSVR, linsvr_params, rmsle, 'LinearSVR', cv=n_folds, repeats=1, seed=seed)\n",
    "    return linsvr_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble & Stacking\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "NUM_DATASETS = 1\n",
    "stacking_train_lists = []\n",
    "stacking_test_lists  = []\n",
    "\n",
    "for j in range(NUM_DATASETS):\n",
    "    stacking_train_lists.append([\"XGBRegressor_train_SEED47_FOLDS8_0623\",\n",
    "                                 \"XGBRegressor2_train_SEED47_FOLDS8_0623\",\n",
    "                                 \"LGBMRegressor_train_SEED47_FOLDS8_0623\",\n",
    "                                 \"LGBMRegressor2_train_SEED47_FOLDS8_0623\",\n",
    "                                 \"CatBoostRegressor_train_SEED47_FOLDS8_0623\",\n",
    "                                 \"CatBoostRegressor2_train_SEED47_FOLDS8_0623\",\n",
    "                                 \"XGBRegressor_train_SEED47_FOLDS8_0624\",\n",
    "                                 \"XGBRegressor2_train_SEED47_FOLDS8_0624\",\n",
    "                                 \"LGBMRegressor_train_SEED47_FOLDS8_0624\",\n",
    "                                 \"LGBMRegressor2_train_SEED47_FOLDS8_0624\",\n",
    "                                 \"CatBoostRegressor_train_SEED47_FOLDS8_0624\",\n",
    "                                 \"CatBoostRegressor2_train_SEED47_FOLDS8_0624\",\n",
    "                                 \"XGBRegressor_train_SEED47_FOLDS8_addon_0622\",\n",
    "                                 \"LGBMRegressor_train_SEED47_FOLDS8_addon_0622\",\n",
    "                                 \"CatBoostRegressor_train_SEED47_FOLDS8_addon_0622\",\n",
    "                                 \"XGBRegressor_train_SEED47_FOLDS8_addon_0623\",\n",
    "                                 \"LGBMRegressor_train_SEED47_FOLDS8_addon_0623\",\n",
    "                                 \"CatBoostRegressor_train_SEED47_FOLDS8_addon_0623\",\n",
    "                                 \"Ridge_train_SEED51_FOLDS10\",\n",
    "                                 \"Ridge_train_SEED51_FOLDS10_0627\",\n",
    "                                 \"RandomForestRegressor_train_SEED47_FOLDS8\",\n",
    "                                 \"RandomForestRegressor_train_SEED47_FOLDS8_0627\",\n",
    "                                 \"ExtraTreesRegressor_train_SEED47_FOLDS8\",\n",
    "                                 \"ExtraTreesRegressor_train_SEED47_FOLDS8_0627\",\n",
    "                                 \"NN_train_SEED47_FOLDS10\",\n",
    "                                 \"NN2_train_SEED47_FOLDS10\",\n",
    "                                 \"NN_train_SEED47_FOLDS10_0626\",\n",
    "                                 \"NN2_train_SEED47_FOLDS10_0626\",\n",
    "                                 \"NN_train_SEED47_FOLDS10_0627\",\n",
    "                                 \"NN2_train_SEED47_FOLDS10_0627\"\n",
    "                                ])\n",
    "    stacking_test_lists.append([\"XGBRegressor_test_SEED47_FOLDS8_0623\",\n",
    "                                \"XGBRegressor2_test_SEED47_FOLDS8_0623\",\n",
    "                                \"LGBMRegressor_test_SEED47_FOLDS8_0623\",\n",
    "                                \"LGBMRegressor2_test_SEED47_FOLDS8_0623\",\n",
    "                                \"CatBoostRegressor_test_SEED47_FOLDS8_0623\",\n",
    "                                \"CatBoostRegressor2_test_SEED47_FOLDS8_0623\",\n",
    "                                \"XGBRegressor_test_SEED47_FOLDS8_0624\",\n",
    "                                \"XGBRegressor2_test_SEED47_FOLDS8_0624\",\n",
    "                                \"LGBMRegressor_test_SEED47_FOLDS8_0624\",\n",
    "                                \"LGBMRegressor2_test_SEED47_FOLDS8_0624\",\n",
    "                                \"CatBoostRegressor_test_SEED47_FOLDS8_0624\",\n",
    "                                \"CatBoostRegressor2_test_SEED47_FOLDS8_0624\",\n",
    "                                \"XGBRegressor_test_SEED47_FOLDS8_addon_0622\",\n",
    "                                \"LGBMRegressor_test_SEED47_FOLDS8_addon_0622\",\n",
    "                                \"CatBoostRegressor_test_SEED47_FOLDS8_addon_0622\",\n",
    "                                \"XGBRegressor_test_SEED47_FOLDS8_addon_0623\",\n",
    "                                \"LGBMRegressor_test_SEED47_FOLDS8_addon_0623\",\n",
    "                                \"CatBoostRegressor_test_SEED47_FOLDS8_addon_0623\",\n",
    "                                \"Ridge_test_SEED51_FOLDS10\",\n",
    "                                \"Ridge_test_SEED51_FOLDS10_0627\",\n",
    "                                \"RandomForestRegressor_test_SEED47_FOLDS8\",\n",
    "                                \"RandomForestRegressor_test_SEED47_FOLDS8_0627\",\n",
    "                                \"ExtraTreesRegressor_test_SEED47_FOLDS8\",\n",
    "                                \"ExtraTreesRegressor_test_SEED47_FOLDS8_0627\",\n",
    "                                \"NN_test_SEED47_FOLDS10\",\n",
    "                                \"NN2_test_SEED47_FOLDS10\",\n",
    "                                \"NN_test_SEED47_FOLDS10_0626\",\n",
    "                                \"NN2_test_SEED47_FOLDS10_0626\",\n",
    "                                \"NN_test_SEED47_FOLDS10_0627\",\n",
    "                                \"NN2_test_SEED47_FOLDS10_0627\"\n",
    "                               ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking 2層目\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "stacking_train_df_l = []\n",
    "stacking_test_df_l  = []\n",
    "\n",
    "pickle_l = glob.glob(f\"./{out_dir}/*.pickle\")\n",
    "\n",
    "for stacking_train_l in stacking_train_lists:\n",
    "    stacking_train_df = pd.DataFrame()\n",
    "    for j, stacking_train_f in enumerate(stacking_train_l):\n",
    "        stacking_train = [f for f in pickle_l if stacking_train_f in f][0]\n",
    "        with open(stacking_train, 'rb') as f:\n",
    "            stacking_train_df[f'stacking_{j}'] = pickle.load(f)\n",
    "    \n",
    "    stacking_train_df['stacking_addon1'] = pd.read_csv(f\"./{out_dir}/train_lgb_817.csv\").lgb_y\n",
    "    stacking_train_df['stacking_addon2'] = pd.read_csv(f\"./{out_dir}/train_lgb_0623.csv\").lgb_y\n",
    "    stacking_train_df['stacking_addon3'] = pd.read_csv(f\"./{out_dir}/train_lgb_0624.csv\").lgb_y\n",
    "    stacking_train_df['stacking_addon4'] = pd.read_csv(f\"./{out_dir}/train_lgb_0624_2.csv\").lgb_y\n",
    "    stacking_train_df_l.append(stacking_train_df)\n",
    "    \n",
    "for stacking_test_l in stacking_test_lists:\n",
    "    stacking_test_df = pd.DataFrame()\n",
    "    for j, stacking_test_f in enumerate(stacking_test_l):\n",
    "        stacking_test  = [f for f in pickle_l if stacking_test_f in f][0]\n",
    "        with open(stacking_test, 'rb') as f:\n",
    "            stacking_test_df[f'stacking_{j}'] = pickle.load(f)\n",
    "\n",
    "    stacking_test_df['stacking_addon1']  = pd.read_csv(f\"./{out_dir}/test_lgb_817.csv\").lgb_y\n",
    "    stacking_test_df['stacking_addon2']  = pd.read_csv(f\"./{out_dir}/test_lgb_0623.csv\").lgb_y\n",
    "    stacking_test_df['stacking_addon3']  = pd.read_csv(f\"./{out_dir}/test_lgb_0624.csv\").lgb_y\n",
    "    stacking_test_df['stacking_addon4']  = pd.read_csv(f\"./{out_dir}/test_lgb_0624_2.csv\").lgb_y\n",
    "    stacking_test_df_l.append(stacking_test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnc_l = {'LGBMRegressor': lgb_regressor,\n",
    "         'XGBRegressor': xgb_regressor,\n",
    "         'CatBoostRegressor': catboost_regressor,\n",
    "         'NGBRegressor': ngboost_regressor,\n",
    "         'LogisticRegression': logistic_regression,\n",
    "         'LinearRegression': lin_regression,\n",
    "         'BayesianRidge': bayesianridge,\n",
    "         'Ridge': ridge,\n",
    "         'ElasticNet': elastic,\n",
    "         'Lasso': lasso,\n",
    "         'SGDRegressor': sgd,\n",
    "         'KNeighborsRegressor': kn_regressor,\n",
    "         'RandomForestRegressor': rf_regressor,\n",
    "         'ExtraTreesRegressor': et_regressor,\n",
    "         'GradientBoostingRegressor': gb_regressor,\n",
    "         'BaggingRegressor': bag_regressor,\n",
    "         'AdaBoostRegressor': ada_regressor,\n",
    "         'LinearSVR': lin_svr}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "stacking_train2 = pd.DataFrame()\n",
    "stacking_test2  = pd.DataFrame()\n",
    "\n",
    "\n",
    "fnc_list = [fnc_l['LGBMRegressor'], fnc_l['XGBRegressor'], fnc_l['CatBoostRegressor'],\n",
    "            #fnc_l['BayesianRidge'], fnc_l['SGDRegressor'], fnc_l['KNeighborsRegressor'],\n",
    "            fnc_l['BayesianRidge'], fnc_l['SGDRegressor'],\n",
    "            fnc_l['RandomForestRegressor'], fnc_l['ExtraTreesRegressor']]\n",
    "\n",
    "for j, target_fn in enumerate(fnc_list):\n",
    "    keys = [k for k, v in fnc_l.items() if v == target_fn]\n",
    "    stacking_train_tmp = 0\n",
    "    stacking_test_tmp = 0\n",
    "    \n",
    "    for k, (stacking_train, stacking_test) in enumerate(zip(stacking_train_df_l, stacking_test_df_l)):\n",
    "        stacking_SEED = 47\n",
    "        stacking_N_FOLDS = 8\n",
    "        encoding = []\n",
    "        \n",
    "        stacking_train['mean'] = stacking_train.mean(axis=1)\n",
    "        stacking_test['mean'] = stacking_test.mean(axis=1)\n",
    "\n",
    "        results_stacking = target_fn(train=stacking_train, test=stacking_test, target=y, target_skf=None, \\\n",
    "                                     seed=stacking_SEED, n_folds=stacking_N_FOLDS, encoding=encoding)\n",
    "        submission_stacking = output_results(y, results_stacking, test_id, f\"STACKING_{keys[0]}_MODELSEL{k}\")\n",
    "\n",
    "        \n",
    "        oof_train = pd.DataFrame()\n",
    "        oof_test  = pd.DataFrame()\n",
    "        oof_train['id']=train_data['id']\n",
    "        oof_train['pred_y']=results_stacking['train']\n",
    "        oof_train['y'] = np.log1p(train_data['y'])\n",
    "        oof_test['id']=test_data['id']\n",
    "        oof_test['pred_y']=results_stacking['test']\n",
    "        oof_train.to_csv(f\"./{out_dir}/train_stacking_{keys[0]}_MODELSEL{k}.csv\",index=False)\n",
    "        oof_test.to_csv(f\"./{out_dir}/test_stacking_{keys[0]}_MODELSEL{k}.csv\",index=False)\n",
    "\n",
    "        stacking_train_tmp += results_stacking['train']\n",
    "        stacking_test_tmp  += results_stacking['test']\n",
    "        \n",
    "    stacking_train2[f'stacking2_{j}'] = stacking_train_tmp/len(stacking_train_df_l)\n",
    "    stacking_test2[f'stacking2_{j}']  = stacking_test_tmp/len(stacking_test_df_l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SPLITS = 10\n",
    "SEED = 47\n",
    "LEARNING_RATE = 1e-3\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 200\n",
    "PATIENCE = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_callbacks():\n",
    "    callbacks = []\n",
    "    \n",
    "    callbacks.append(EarlyStopping(monitor='val_root_mean_squared_error',\n",
    "                                   min_delta=0,\n",
    "                                   patience=PATIENCE,\n",
    "                                   verbose=1,\n",
    "                                   mode='auto',\n",
    "                                   baseline=None,\n",
    "                                   restore_best_weights=True))\n",
    "\n",
    "    # Update the learning rate every epoch\n",
    "    callbacks.append(ReduceLROnPlateau(monitor='val_root_mean_squared_error',\n",
    "                                       factor=0.95,\n",
    "                                       patience=1,\n",
    "                                       verbose=0,\n",
    "                                       mode='auto',\n",
    "                                       min_delta=1e-4,\n",
    "                                       cooldown=0,\n",
    "                                       min_lr=1e-6))\n",
    "\n",
    "    return callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn(lr, seed, input_shape):\n",
    "\n",
    "    model = Sequential([\n",
    "        Dense(2 ** 8, activation='relu', input_dim=input_shape, kernel_initializer=he_normal(seed=seed)),\n",
    "        Dense(2 ** 7, activation='relu', kernel_initializer=he_normal(seed=seed)),\n",
    "        Dense(2 ** 6, activation='relu', kernel_initializer=he_normal(seed=seed)),\n",
    "        Dense(2 ** 5, activation='relu', kernel_initializer=he_normal(seed=seed)),\n",
    "        Dense(2 ** 4, activation='relu', kernel_initializer=he_normal(seed=seed)),\n",
    "        Dense(2 ** 3, activation='relu', kernel_initializer=he_normal(seed=seed)),\n",
    "        \n",
    "        Dense(2 ** 3, activation='relu', kernel_initializer=he_normal(seed=seed)),\n",
    "        Dense(1)\n",
    "    ])\n",
    "\n",
    "    # COMPILE WITH ADAM OPTIMIZER AND CROSS ENTROPY COST    \n",
    "    adam_opt = Adam(learning_rate=lr, beta_1=0.9, beta_2=0.999, amsgrad=True)\n",
    "    nadam_opt = Nadam(learning_rate=lr, beta_1=0.9, beta_2=0.999)\n",
    "    ladam_opt = tfa.optimizers.LazyAdam(learning_rate=LEARNING_RATE, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False)\n",
    "    adamw_opt = tfa.optimizers.AdamW(learning_rate=LEARNING_RATE, weight_decay=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=True)\n",
    "    rmsprop_opt = RMSprop(learning_rate=lr, rho=0.9)\n",
    "    sgd_opt = SGD(learning_rate=lr, momentum=0.0, nesterov=False)\n",
    "    sgd_opt = SGD(learning_rate=lr, decay=1e-4, momentum=0.9, nesterov=True)\n",
    "\n",
    "    model.compile(optimizer=nadam_opt,\n",
    "                  loss='mean_squared_error', \n",
    "                  metrics=tf.keras.metrics.RootMeanSquaredError())\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn2(lr, seed, input_shape):\n",
    "\n",
    "    model = Sequential([\n",
    "        Dense(2 ** 8, activation='relu', input_dim=input_shape, kernel_initializer=he_normal(seed=seed)),\n",
    "        Dense(2 ** 7, activation='relu', kernel_initializer=he_normal(seed=seed)),\n",
    "        Dense(2 ** 6, activation='relu', kernel_initializer=he_normal(seed=seed)),\n",
    "        Dense(2 ** 5, activation='relu', kernel_initializer=he_normal(seed=seed)),\n",
    "        \n",
    "        Dense(2 ** 3, activation='relu', kernel_initializer=he_normal(seed=seed)),\n",
    "        Dense(1)\n",
    "    ])\n",
    "\n",
    "    # COMPILE WITH ADAM OPTIMIZER AND CROSS ENTROPY COST    \n",
    "    adam_opt = Adam(learning_rate=lr, beta_1=0.9, beta_2=0.999, amsgrad=True)\n",
    "    nadam_opt = Nadam(learning_rate=lr, beta_1=0.9, beta_2=0.999)\n",
    "    ladam_opt = tfa.optimizers.LazyAdam(learning_rate=LEARNING_RATE, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False)\n",
    "    adamw_opt = tfa.optimizers.AdamW(learning_rate=LEARNING_RATE, weight_decay=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=True)\n",
    "    rmsprop_opt = RMSprop(learning_rate=lr, rho=0.9)\n",
    "    sgd_opt = SGD(learning_rate=lr, momentum=0.0, nesterov=False)\n",
    "    sgd_opt = SGD(learning_rate=lr, decay=1e-4, momentum=0.9, nesterov=True)\n",
    "\n",
    "    model.compile(optimizer=nadam_opt,\n",
    "                  loss='mean_squared_error',\n",
    "                  metrics=tf.keras.metrics.RootMeanSquaredError())\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "history, history_false = [], []\n",
    "score, score_false = [], []\n",
    "pred_train = np.zeros((stacking_train.shape[0]))\n",
    "pred_full_test = 0\n",
    "\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "\n",
    "for fold_id, (train_idx, val_idx) in enumerate(tqdm(skf.split(stacking_train, y_bin))):\n",
    "    print(\"*\"*80)\n",
    "    print(f\"Started TF learning(1) fold:{fold_id+1} / {N_SPLITS}\")\n",
    "\n",
    "    # 全データで学習、予測\n",
    "    model = nn(lr=LEARNING_RATE, seed=SEED, input_shape=stacking_train.shape[1])\n",
    "    callbacks = create_callbacks()\n",
    "\n",
    "    tr_X, val_X = stacking_train.iloc[train_idx].copy(), stacking_train.iloc[val_idx].copy()\n",
    "    tr_y, val_y = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        \n",
    "    history.append(model.fit(tr_X, tr_y, batch_size=BATCH_SIZE,\n",
    "                             epochs=EPOCHS,\n",
    "                             verbose=2,\n",
    "                             validation_data=(val_X, val_y),\n",
    "                             callbacks=callbacks))\n",
    "    \n",
    "    pred_train[val_idx] = model.predict(val_X).reshape(-1)\n",
    "    score.append(model.evaluate(val_X, val_y, batch_size=BATCH_SIZE, verbose=0, return_dict=True))\n",
    "    pred_full_test = pred_full_test + model.predict(stacking_test)\n",
    "    \n",
    "    RMSLE = mean_squared_error(y[val_idx], pred_train[val_idx], squared=False)\n",
    "    print(f\"RMSLE={RMSLE}\")\n",
    "\n",
    "RMSLE_overall = mean_squared_error(y, pred_train, squared=False)\n",
    "print(f\"Overall RMSLE={RMSLE_overall}\")\n",
    "\n",
    "# Make submission\n",
    "print(\"Saving submission file\")\n",
    "submission = pd.DataFrame({'id': test_id, 'y': np.expm1((pred_full_test/N_SPLITS).reshape(-1))})\n",
    "submission.to_csv(f\"./{out_dir}/submission_STACKING_NN1_CV{RMSLE_overall:.6f}.csv\", index=False)\n",
    "\n",
    "oof_train = pd.DataFrame()\n",
    "oof_test  = pd.DataFrame()\n",
    "oof_train['id']=train_data['id']\n",
    "oof_train['pred_y']=pred_train\n",
    "oof_train['y'] = np.log1p(train_data['y'])\n",
    "oof_test['id']=test_data['id']\n",
    "oof_test['pred_y']=(pred_full_test/N_SPLITS).reshape(-1)\n",
    "oof_train.to_csv(f\"./{out_dir}/train_stacking_NN1.csv\",index=False)\n",
    "oof_test.to_csv(f\"./{out_dir}/test_stacking_NN1.csv\",index=False)\n",
    "\n",
    "stacking_train2[f'stacking2_{len(fnc_list)}'] = pred_train\n",
    "stacking_test2[f'stacking2_{len(fnc_list)}']  = pred_full_test/N_SPLITS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "history, history_false = [], []\n",
    "score, score_false = [], []\n",
    "pred_train = np.zeros((stacking_train.shape[0]))\n",
    "pred_full_test = 0\n",
    "\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "\n",
    "for fold_id, (train_idx, val_idx) in enumerate(tqdm(skf.split(stacking_train, y_bin))):\n",
    "    print(\"*\"*80)\n",
    "    print(f\"Started TF learning(2) fold:{fold_id+1} / {N_SPLITS}\")\n",
    "\n",
    "    # 全データで学習、予測\n",
    "    model = nn2(lr=LEARNING_RATE, seed=SEED, input_shape=stacking_train.shape[1])\n",
    "    callbacks = create_callbacks()\n",
    "\n",
    "    tr_X, val_X = stacking_train.iloc[train_idx].copy(), stacking_train.iloc[val_idx].copy()\n",
    "    tr_y, val_y = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        \n",
    "    history.append(model.fit(tr_X, tr_y, batch_size=BATCH_SIZE,\n",
    "                             epochs=EPOCHS,\n",
    "                             verbose=2,\n",
    "                             validation_data=(val_X, val_y),\n",
    "                             callbacks=callbacks))\n",
    "    \n",
    "    pred_train[val_idx] = model.predict(val_X).reshape(-1)\n",
    "    score.append(model.evaluate(val_X, val_y, batch_size=BATCH_SIZE, verbose=0, return_dict=True))\n",
    "    pred_full_test = pred_full_test + model.predict(stacking_test)\n",
    "    \n",
    "    RMSLE = mean_squared_error(y[val_idx], pred_train[val_idx], squared=False)\n",
    "    print(f\"RMSLE={RMSLE}\")\n",
    "\n",
    "RMSLE_overall = mean_squared_error(y, pred_train, squared=False)\n",
    "print(f\"Overall RMSLE={RMSLE_overall}\")\n",
    "\n",
    "# Make submission\n",
    "print(\"Saving submission file\")\n",
    "submission = pd.DataFrame({'id': test_id, 'y': np.expm1((pred_full_test/N_SPLITS).reshape(-1))})\n",
    "submission.to_csv(f\"./{out_dir}/submission_STACKING_NN2_CV{RMSLE_overall:.6f}.csv\", index=False)\n",
    "\n",
    "oof_train = pd.DataFrame()\n",
    "oof_test  = pd.DataFrame()\n",
    "oof_train['id']=train_data['id']\n",
    "oof_train['pred_y']=pred_train\n",
    "oof_train['y'] = np.log1p(train_data['y'])\n",
    "oof_test['id']=test_data['id']\n",
    "oof_test['pred_y']=(pred_full_test/N_SPLITS).reshape(-1)\n",
    "oof_train.to_csv(f\"./{out_dir}/train_stacking_NN2.csv\",index=False)\n",
    "oof_test.to_csv(f\"./{out_dir}/test_stacking_NN2.csv\",index=False)\n",
    "\n",
    "stacking_train2[f'stacking2_{len(fnc_list)+1}'] = pred_train\n",
    "stacking_test2[f'stacking2_{len(fnc_list)+1}']  = pred_full_test/N_SPLITS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking 3層目"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "stacking_train2.to_csv(f\"./{out_dir}/train_stacking2_AllModel.csv\",index=False)\n",
    "stacking_test2.to_csv(f\"./{out_dir}/test_stacking2_AllModel.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "fnc_list2 = [fnc_l['LinearRegression'], fnc_l['BaggingRegressor']]\n",
    "\n",
    "cols_to_stack = [c for c in stacking_train2.columns]\n",
    "\n",
    "for target_fn in fnc_list2:\n",
    "    keys = [k for k, v in fnc_l.items() if v == target_fn]\n",
    "    stacking_SEED = 51\n",
    "    stacking_N_FOLDS = 10\n",
    "    encoding = []\n",
    "\n",
    "    results_stacking2 = target_fn(train=stacking_train2[cols_to_stack], test=stacking_test2[cols_to_stack], target=y, target_skf=y_bin, \\\n",
    "                                  seed=stacking_SEED, n_folds=stacking_N_FOLDS, encoding=encoding)\n",
    "    submission_stacking2 = output_results(y, results_stacking2, test_id, f\"STACKING2_full_{keys[0]}\")\n",
    "        \n",
    "    oof_train = pd.DataFrame()\n",
    "    oof_test  = pd.DataFrame()\n",
    "    oof_train['id']=train_data['id']\n",
    "    oof_train['pred_y']=results_stacking2['train']\n",
    "    oof_train['y'] = np.log1p(train_data['y'])\n",
    "    oof_test['id']=test_data['id']\n",
    "    oof_test['pred_y']=results_stacking2['test']\n",
    "    oof_train.to_csv(f\"./{out_dir}/train_stacking2_full_{keys[0]}.csv\",index=False)\n",
    "    oof_test.to_csv(f\"./{out_dir}/test_stacking2_full_{keys[0]}.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble 3層目"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#         LGB   XGB   CAT   BAY   SGD   RFR   ETR   NN1   NN2\n",
    "coef_l = [0.05, 0.00, 0.00, 0.65, 0.00, 0.05, 0.00, 0.15, 0.10]\n",
    "\n",
    "results_train = 0\n",
    "results_test  = 0\n",
    "for j, coef in zip(range(stacking_train2.shape[1]), coef_l):\n",
    "    results_train += stacking_train2[f'stacking2_{j}'] * coef\n",
    "    results_test  += stacking_test2[f'stacking2_{j}'] * coef\n",
    "    \n",
    "results = {'train': results_train, 'test':  results_test}\n",
    "\n",
    "submission_ensemble = output_results(y, results, test_id, f\"ENSEMBLE2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
