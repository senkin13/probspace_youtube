{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ProbSpace: YouTube動画視聴回数予測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = \"out_tmp\"\n",
    "!mkdir -p $out_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "import itertools\n",
    "import os, datetime, gc, re, random\n",
    "import time, datetime\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import optuna\n",
    "import bhtsne, umap\n",
    "\n",
    "from janome.tokenizer import Tokenizer\n",
    "from janome.analyzer import Analyzer\n",
    "from janome.tokenfilter import *\n",
    "from janome.charfilter import UnicodeNormalizeCharFilter, RegexReplaceCharFilter\n",
    "import unicodedata\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import Pool, CatBoostRegressor, CatBoostClassifier\n",
    "\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.linear_model import BayesianRidge, ElasticNet, Lasso, LogisticRegression, Ridge, SGDRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor, BaggingRegressor\n",
    "from sklearn.ensemble import StackingRegressor, VotingRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor, GradientBoostingRegressor, RandomForestRegressor\n",
    "\n",
    "from ngboost import NGBRegressor\n",
    "from ngboost.ngboost import NGBoost\n",
    "from ngboost.learners import default_tree_learner\n",
    "from ngboost.scores import MLE, CRPS, LogScore\n",
    "from ngboost.distns import Normal, LogNormal\n",
    "\n",
    "from sklearn.linear_model import BayesianRidge, ElasticNet, Lasso, LogisticRegression, Ridge, SGDRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans, DBSCAN\n",
    "from sklearn.model_selection import KFold, RepeatedKFold, StratifiedKFold, cross_validate, cross_val_predict, train_test_split\n",
    "from sklearn.metrics import mean_squared_error, roc_auc_score, roc_curve, log_loss\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler, Normalizer, RobustScaler, QuantileTransformer, PowerTransformer\n",
    "from sklearn.feature_selection import SelectFromModel, RFE, SelectPercentile, SelectKBest\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.metrics import MeanSquaredError\n",
    "from tensorflow.keras.optimizers import Adadelta, Adagrad, Adam, Adamax, Ftrl, Nadam, RMSprop, SGD\n",
    "from tensorflow.keras.regularizers import l1, l2, l1_l2\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, EarlyStopping, TensorBoard, LambdaCallback, ReduceLROnPlateau\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Concatenate, Lambda\n",
    "from tensorflow.keras.layers import Activation, Average, Dense, Dropout, Flatten, BatchNormalization, LeakyReLU, Input\n",
    "from tensorflow.keras.layers import GaussianDropout, GaussianNoise\n",
    "from tensorflow.keras.layers import Conv2D, SeparableConv2D, MaxPooling2D, AveragePooling2D, GlobalAveragePooling2D\n",
    "\n",
    "from tensorflow.keras.losses import MeanSquaredLogarithmicError\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import missingno as msno \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for variable description\n",
    "def description(df):\n",
    "    summary = pd.DataFrame(df.dtypes, columns=['dtypes'])\n",
    "    summary = summary.reset_index()\n",
    "    summary[\"Name\"] = summary['index']\n",
    "    summary = summary[[\"Name\",'dtypes']]\n",
    "    summary[\"Missing\"] = df.isnull().sum().values    \n",
    "    summary[\"Uniques\"] = df.nunique().values\n",
    "    summary[\"Mean\"] = np.nanmean(df, axis=0).astype(df.dtypes)\n",
    "    summary[\"Std\"] = np.nanstd(df, axis=0).astype(df.dtypes)\n",
    "    summary[\"Minimum\"] = np.nanmin(df, axis=0).astype(df.dtypes)\n",
    "    summary[\"Maximum\"] = np.nanmax(df, axis=0).astype(df.dtypes)\n",
    "    summary[\"First Value\"] = df.iloc[0].values\n",
    "    summary[\"Second Value\"] = df.iloc[1].values\n",
    "    summary[\"Third Value\"] = df.iloc[2].values\n",
    "    summary[\"dimension\"] = str(df.shape)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hist(target):\n",
    "    plt.hist(target, bins=100)\n",
    "\n",
    "    print(\"max:  {:>10,.6f}\".format(target.max()))\n",
    "    print(\"min:  {:>10,.6f}\".format(target.min()))\n",
    "    print(\"mean: {:>10,.6f}\".format(target.mean()))\n",
    "    print(\"std:  {:>10,.6f}\".format(target.std()))\n",
    "    \n",
    "    return\n",
    "\n",
    "def get_hist4(target1, title1, target2, title2, target3, title3, target4, title4):\n",
    "    fig = plt.figure(figsize=(18, 18))\n",
    "\n",
    "    ax1 = fig.add_subplot(5,1,1)\n",
    "    ax2 = fig.add_subplot(5,1,2)\n",
    "    ax3 = fig.add_subplot(5,1,3)\n",
    "    ax4 = fig.add_subplot(5,1,4)\n",
    "    ax5 = fig.add_subplot(5,1,5)\n",
    "\n",
    "    ax1.set_title(title1)\n",
    "    ax2.set_title(title2)\n",
    "    ax3.set_title(title3)\n",
    "    ax4.set_title(title4)\n",
    "    ax5.set_title(\"OVERALL\")\n",
    "    \n",
    "    ax1.hist(target1, bins=100)\n",
    "    ax2.hist(target2, bins=100)\n",
    "    ax3.hist(target3, bins=100)\n",
    "    ax4.hist(target4, bins=100)\n",
    "\n",
    "    ax5.hist(target1, bins=100, alpha=0.2, color='red')\n",
    "    ax5.hist(target2, bins=100, alpha=0.2, color='green')\n",
    "    ax5.hist(target3, bins=100, alpha=0.2, color='blue')\n",
    "    #ax5.hist(target4, bins=100, alpha=0.2, color='grey')\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "  hist = pd.DataFrame(history.history)\n",
    "  hist['epoch'] = history.epoch\n",
    "  \n",
    "  plt.figure()\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel('Mean Squared Log Error [y]')\n",
    "  plt.plot(hist['epoch'], hist['root_mean_squared_error'],\n",
    "           label='Train Error')\n",
    "  plt.plot(hist['epoch'], hist['val_root_mean_squared_error'],\n",
    "           label = 'Val Error')\n",
    "  plt.ylim([0,5])\n",
    "  plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# for train/test data\n",
    "train_data = pd.read_csv(\"./input/train_data.csv\")\n",
    "test_data = pd.read_csv(\"./input/test_data.csv\")\n",
    "\n",
    "y = np.log1p(train_data['y']).copy()\n",
    "test_id = test_data.id\n",
    "\n",
    "train = train_data.drop(['id', 'y'], axis=1).copy()\n",
    "test  = test_data.drop(['id'], axis=1).copy()\n",
    "\n",
    "cols_to_log = ['likes', 'dislikes', 'comment_count']\n",
    "train[cols_to_log] = np.log1p(train[cols_to_log])\n",
    "test[cols_to_log]  = np.log1p(test[cols_to_log])\n",
    "\n",
    "traintest = pd.concat([train, test]).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 目的変数の分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_hist(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in train_data.columns:\n",
    "    print(\"{:<20}: {} ({:.2f}%)\".format(col, train_data[col].isnull().sum(), train_data[col].isnull().sum()/train_data.shape[0]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in test_data.columns:\n",
    "    print(\"{:<20}: {} ({:.2f}%)\".format(col, test_data[col].isnull().sum(), test_data[col].isnull().sum()/test_data.shape[0]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# seedの固定化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=1234):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特徴量生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(df, features):\n",
    "    # 欠損値処理\n",
    "    df['tags'].fillna(\"[none]\", inplace=True)\n",
    "    df['description'].fillna(df['tags'].replace(\"|\", \" \") + df['title'], inplace=True)\n",
    "\n",
    "    tokenizer = Tokenizer()\n",
    "    char_filters = [UnicodeNormalizeCharFilter(), RegexReplaceCharFilter(r\"[0123456789!#$%&()=~|\\-^\\\\@`{;:+*},./\\<>?_♪®」—]\", \"\")]\n",
    "    token_filters = [POSKeepFilter(['名詞'])]\n",
    "    #token_filters = [POSStopFilter(['接続詞', '接頭辞', '接尾辞', '記号', '助詞', '助動詞']), TokenCountFilter()]\n",
    "    a = Analyzer(char_filters, tokenizer, token_filters=token_filters)\n",
    "\n",
    "    df.loc[df['tags']==\"[none]\", 'tags'] = \\\n",
    "    df['title'][df['tags']==\"[none]\"].str.lower().apply(lambda x: \"|\".join([word.surface for word in a.analyze(x)]))\n",
    "    \n",
    "    # ------------------------------------------\n",
    "    # ビニング\n",
    "    # ------------------------------------------\n",
    "    df['likes_qcut'] = pd.qcut(df['likes'], 10000, False, duplicates='drop')\n",
    "    df['dislikes_qcut'] = pd.qcut(df['dislikes'], 10000, False, duplicates='drop')\n",
    "    df['comment_count_qcut'] = pd.qcut(df['comment_count'], 10000, False, duplicates='drop')\n",
    "    features[\"cat\"] += ['likes_qcut', 'dislikes_qcut', 'comment_count_qcut']\n",
    "\n",
    "    df['likes_cut'] = pd.cut(df['likes'], np.ceil(df['likes'].max()+1).astype('int'), False, labels=False, duplicates='drop', include_lowest=True)\n",
    "    df['dislikes_cut'] = pd.cut(df['dislikes'], np.ceil(df['dislikes'].max()+1).astype('int'), False, labels=False, duplicates='drop', include_lowest=True)\n",
    "    df['comment_count_cut'] = pd.cut(df['comment_count'], np.ceil(df['comment_count'].max()+1).astype('int'), False, labels=False, duplicates='drop', include_lowest=True)\n",
    "    features[\"cat\"] += ['likes_cut', 'dislikes_cut', 'comment_count_cut']\n",
    "\n",
    "    # ------------------------------------------\n",
    "    # 比率\n",
    "    # ------------------------------------------\n",
    "    df['likes_dislikes_ratio'] = df['likes']/(df['dislikes']+1)\n",
    "    df['comment_count_likes_ratio'] = df['comment_count']/(df['likes']+1)\n",
    "    df['comment_count_dislikes_ratio'] = df['comment_count']/(df['dislikes']+1)\n",
    "\n",
    "    features[\"num\"] += ['likes_dislikes_ratio', 'comment_count_likes_ratio', 'comment_count_dislikes_ratio']\n",
    "\n",
    "    # ------------------------------------------\n",
    "    # 積算\n",
    "    # ------------------------------------------\n",
    "    df['mul_likes_comments_disabled'] = df['likes'] * df['comments_disabled']\n",
    "    df['mul_dislikes_comments_disabled'] = df['dislikes'] * df['comments_disabled']\n",
    "    df['mul_comment_count_ratings_disabled'] = df['comment_count'] * df['ratings_disabled']\n",
    "\n",
    "    features[\"num\"] += ['mul_likes_comments_disabled', 'mul_dislikes_comments_disabled', 'mul_comment_count_ratings_disabled']\n",
    "    \n",
    "    # ------------------------------------------\n",
    "    # 出現頻度\n",
    "    # ------------------------------------------\n",
    "    for col in ['channelId', 'channelTitle', 'categoryId']:\n",
    "        df['_'.join(list(map(str, ['freq', col])))] = df[col].map(df[col].value_counts())\n",
    "        \n",
    "        features[\"num\"] += ['_'.join(list(map(str, ['freq', col])))]\n",
    "\n",
    "    # ------------------------------------------\n",
    "    # 'n_tags'数特徴量の生成\n",
    "    # ------------------------------------------    \n",
    "    df['n_tags'] = df['tags'].astype(str).apply(lambda x: len(x.split(\"|\")))\n",
    "    features[\"num\"] += ['n_tags']\n",
    "    \n",
    "    # ------------------------------------------\n",
    "    # 'description'関連の特徴量生成\n",
    "    # ------------------------------------------    \n",
    "    df['http_count_in_desc'] = df['description'].apply(lambda x: x.lower().count(\"http\"))\n",
    "    df['len_description'] = df['description'].apply(lambda x: len(x))\n",
    "    df['len_title'] = df['title'].apply(lambda x: len(x))\n",
    "\n",
    "    features[\"num\"] += ['http_count_in_desc', 'len_description', 'len_title']\n",
    "    \n",
    "    # ------------------------------------------\n",
    "    # 'title'/'tag'/'description'内の記述言語関連特徴量の生成\n",
    "    # ------------------------------------------    \n",
    "    def checkJapanese(word):\n",
    "        for ch in word:\n",
    "            try:\n",
    "                name = unicodedata.name(ch) \n",
    "                if \"CJK UNIFIED\" in name \\\n",
    "                or \"HIRAGANA\" in name \\\n",
    "                or \"KATAKANA\" in name:\n",
    "                    return True\n",
    "            except:\n",
    "              continue\n",
    "        return False\n",
    "\n",
    "    def checkAlnum(word):\n",
    "        alnum = re.compile(r'^[a-zA-Z0-9]+$')\n",
    "        result = alnum.match(word) is not None\n",
    "        return result\n",
    "\n",
    "    # is japanese\n",
    "    df['isJa_title'] = df['title'].apply(lambda x: checkJapanese(x))\n",
    "    df['isJa_tags'] = df['tags'].apply(lambda x: checkJapanese(x))\n",
    "    df['isJa_description'] = df['description'].apply(lambda x: checkJapanese(x))\n",
    "\n",
    "    features[\"ohe\"] += ['isJa_title', 'isJa_tags', 'isJa_description']\n",
    "    \n",
    "    # isalnum\n",
    "    df['isalnum_title'] = df['title'].apply(lambda x: checkAlnum(x))\n",
    "    df['isalnum_tags'] = df['tags'].apply(lambda x: checkAlnum(x))\n",
    "    df['isalnum_description'] = df['description'].apply(lambda x: checkAlnum(x))\n",
    "\n",
    "    features[\"ohe\"] += ['isalnum_title', 'isalnum_tags', 'isalnum_description']\n",
    "    \n",
    "    # cotain english\n",
    "    df['inclEn_title'] = df['title'].apply(lambda x: len(re.findall(r'[a-zA-Z0-9]', x.lower())))\n",
    "    df['inclEn_tags'] = df['tags'].apply(lambda x: len(re.findall(r'[a-zA-Z0-9]', x.lower())))\n",
    "    df['inclEn_description'] = df['description'].apply(lambda x: len(re.findall(r'[a-zA-Z0-9]', x.lower())))\n",
    "    \n",
    "    features[\"num\"] += ['inclEn_title', 'inclEn_tags', 'inclEn_description']\n",
    "\n",
    "    # ------------------------------------------\n",
    "    # 投稿時期、採取時期からの期間、日時関連特徴量の生成\n",
    "    # ------------------------------------------    \n",
    "    # publishedAt\n",
    "    df['publishedAt'] = pd.to_datetime(df['publishedAt'], utc=True)\n",
    "    df['publishedAt_year'] = df['publishedAt'].apply(lambda x: x.year)\n",
    "    df['publishedAt_month'] = df['publishedAt'].apply(lambda x: x.month)\n",
    "    df['publishedAt_day'] = df['publishedAt'].apply(lambda x: x.day)\n",
    "    df['publishedAt_hour'] = df['publishedAt'].apply(lambda x: x.hour)\n",
    "    df['publishedAt_minute'] = df['publishedAt'].apply(lambda x: x.minute)\n",
    "    df['publishedAt_second'] = df['publishedAt'].apply(lambda x: x.second)\n",
    "    df['publishedAt_dayofweek'] = df['publishedAt'].apply(lambda x: x.dayofweek)\n",
    "\n",
    "    df['collection_date'] = \\\n",
    "    df['collection_date'].map(lambda x: x.split('.')).map(lambda x: '20'+x[0]+'-'+x[2]+'-'+x[1]+'T00:00:00.000Z')\n",
    "        \n",
    "    # collection_date\n",
    "    df['collection_date'] = pd.to_datetime(df['collection_date'], utc=True)\n",
    "    df['collection_date_year'] = df['collection_date'].apply(lambda x: x.year)\n",
    "    df['collection_date_month'] = df['collection_date'].apply(lambda x: x.month)\n",
    "    df['collection_date_day'] = df['collection_date'].apply(lambda x: x.day)\n",
    "\n",
    "    # delta\n",
    "    df['delta'] = (df['collection_date'] - df['publishedAt']).apply(lambda x: x.days)\n",
    "    df['log_delta'] = np.log(df['delta'])\n",
    "    df['sqrt_delta'] = np.sqrt(df['delta'])\n",
    "    df['pow_delta'] = pow(df['delta'], 2)\n",
    "    df['log_pow_delta'] = pow(np.log(df['delta']), 2)\n",
    "    df['publishedAt_delta'] = (df['publishedAt'] - df['publishedAt'].min()).apply(lambda x: x.days)\n",
    "    df['collection_delta'] = (df['collection_date'] - df['collection_date'].min()).apply(lambda x: x.days)\n",
    "    \n",
    "    features[\"cat\"] += ['publishedAt_year', 'publishedAt_month', 'publishedAt_day', \\\n",
    "                        'publishedAt_hour', 'publishedAt_minute', 'publishedAt_second', 'publishedAt_dayofweek', \\\n",
    "                        'collection_date_year', 'collection_date_month', 'collection_date_day']\n",
    "    \n",
    "    features[\"num\"] += ['delta', 'log_delta', 'sqrt_delta', 'pow_delta', 'log_pow_delta', \\\n",
    "                        'publishedAt_delta', 'collection_delta']\n",
    "    \n",
    "    return df, features\n",
    "    \n",
    "def create_features2(df, features, cols_groupby, cols_transform, target_func, option):\n",
    "    # ------------------------------------------\n",
    "    # 'cols_groupby'ごとの特徴量生成\n",
    "    # ------------------------------------------   \n",
    "    \n",
    "    cols_to_transform = list(set(cols_transform) - set(cols_groupby))\n",
    "\n",
    "    #cols_to_transform = [c for c in df.columns if ('likes' in c) | ('dislikes' in c) | ('comment_count' in c)]\n",
    "    for col_base in cols_groupby:\n",
    "        for col in cols_to_transform:\n",
    "            for func in target_func:\n",
    "                df['_'.join(list(map(str, [col_base, col, func])))] = df.groupby(col_base)[col].transform(func)\n",
    "                features['num'] += ['_'.join(list(map(str, [col_base, col, func])))]\n",
    "                \n",
    "                if option[\"log\"]:\n",
    "                    df['_'.join(list(map(str, [col_base, col, 'log', func])))] = np.log1p(df.groupby(col_base)[col].transform(func))\n",
    "                    features['num'] += ['_'.join(list(map(str, [col_base, col, 'log', func])))]\n",
    "                if option[\"sqrt\"]:\n",
    "                    df['_'.join(list(map(str, [col_base, col, \"sqrt\", func])))] = np.sqrt(df.groupby(col_base)[col].transform(func))\n",
    "                    features[\"num\"] += ['_'.join(list(map(str, [col_base, col, \"sqrt\", func])))]\n",
    "                if option[\"sqrt_log\"]:\n",
    "                    df['_'.join(list(map(str, [col_base, col, \"sqrt_log\", func])))] = np.log1p(np.sqrt(df.groupby(col_base)[col].transform(func)))\n",
    "                    features[\"num\"] += ['_'.join(list(map(str, [col_base, col, \"sqrt_log\", func])))]\n",
    "                if option[\"log_sqrt\"]:\n",
    "                    df['_'.join(list(map(str, [col_base, col, \"log_sqrt\", func])))] = np.sqrt(np.log1p(df.groupby(col_base)[col].transform(func)))\n",
    "                    features[\"num\"] += ['_'.join(list(map(str, [col_base, col, \"log_sqrt\", func])))]\n",
    "                if option[\"pow\"]:\n",
    "                    df['_'.join(list(map(str, [col_base, col, \"pow\", func])))] = pow(df.groupby(col_base)[col].transform(func), 2)\n",
    "                    features[\"num\"] += ['_'.join(list(map(str, [col_base, col, \"pow\", func])))]\n",
    "                if option[\"pow_log\"]:\n",
    "                    df['_'.join(list(map(str, [col_base, col, 'pow_log', func])))] = 2*np.log1p(df.groupby(col_base)[col].transform(func))\n",
    "                    features['num'] += ['_'.join(list(map(str, [col_base, col, 'pow_log', func])))]\n",
    "                if option[\"log_pow\"]:\n",
    "                    df['_'.join(list(map(str, [col_base, col, 'log_pow', func])))] = pow(np.log1p(df.groupby(col_base)[col].transform(func)), 2)\n",
    "                    features['num'] += ['_'.join(list(map(str, [col_base, col, 'log_pow', func])))]\n",
    "\n",
    "    return df, features\n",
    "\n",
    "def create_features3(df, features, cols_transform, option):\n",
    "    for col in cols_transform:\n",
    "        if option[\"log\"]:\n",
    "            df['_'.join(list(map(str, ['log', col])))] = np.log1p(df[col])\n",
    "            features[\"num\"] += ['_'.join(list(map(str, ['log', col])))]\n",
    "\n",
    "        if option[\"sqrt\"]:\n",
    "            df['_'.join(list(map(str, ['sqrt', col])))] = np.sqrt(df[col])\n",
    "            features[\"num\"] += ['_'.join(list(map(str, ['sqrt', col])))]\n",
    "\n",
    "        if option[\"sqrt_log\"]:\n",
    "            df['_'.join(list(map(str, ['sqrt', 'log', col])))] = np.log1p(np.sqrt(df[col]))\n",
    "            features[\"num\"] += ['_'.join(list(map(str, ['sqrt', 'log', col])))]\n",
    "        \n",
    "        if option[\"log_sqrt\"]:\n",
    "            df['_'.join(list(map(str, ['log', 'sqrt', col])))] = np.sqrt(np.log1p(df[col]))\n",
    "            features[\"num\"] += ['_'.join(list(map(str, ['log', 'sqrt', col])))]\n",
    "        \n",
    "        if option[\"pow\"]:\n",
    "            df['_'.join(list(map(str, ['pow', col])))] = pow(df[col], 2)\n",
    "            features[\"num\"] += ['_'.join(list(map(str, ['pow', col])))]\n",
    "        \n",
    "        if option[\"pow_log\"]:\n",
    "            df['_'.join(list(map(str, ['pow', 'log', col])))] = np.log1p(pow(df[col], 2))\n",
    "            features[\"num\"] += ['_'.join(list(map(str, ['pow', 'log', col])))]\n",
    "        \n",
    "        if option[\"log_pow\"]:\n",
    "            df['_'.join(list(map(str, ['log', 'pow', col])))] = pow(np.log1p(df[col]), 2)\n",
    "            features[\"num\"] += ['_'.join(list(map(str, ['log', 'pow', col])))]\n",
    "        \n",
    "    return df, features\n",
    "\n",
    "def create_features4(df, features, option):\n",
    "    if (not option[\"title\"]) and (not option[\"tags\"]) and (not option[\"description\"]):\n",
    "                return df, features\n",
    "        \n",
    "    tokenizer = Tokenizer()\n",
    "    feats_increased = []\n",
    "   \n",
    "    #title_words = [\"video\", \"official\", \"music\", \"公式\"]\n",
    "    title_words = [\"video\", \"official\"]\n",
    "    #tags_words = [\"music\", \"video\", \"official\", \"song\", \"remastered\", \"vevo\", \"lyric\", \"rock\", \"you\", \"pop\", \"live\", \"queen\"]\n",
    "    tags_words = [\"music\", \"video\", \"official\"]\n",
    "    #desc_words= [\"http\", \"www\", \"smarturl\", \"super\", \"simple\", \"video\", \"music\", \"facebook\", \"youtube\", \"twitter\", \"official\", \"instagram\"]\n",
    "    desc_words= [\"com\", \"http\"]\n",
    "    \n",
    "    for j, (title_sentence, tags_sentence, desc_sentence) in enumerate(tqdm(zip(df['title'].str.lower(), df['tags'].str.lower(), df['description'].str.lower()))):\n",
    "        if option[\"title\"]:\n",
    "            title_text = \" \".join(tokenizer.tokenize(title_sentence, wakati=True))\n",
    "            for word in title_words:\n",
    "                if (word in title_text) | (f\"{word}s\" in title_text):\n",
    "                    #df['likes'][df.index==j] = df['likes'].iloc[j]*1.2\n",
    "                    #df['dislikes'][df.index==j] = df['dislikes'].iloc[j]*1.2\n",
    "                    #df['comment_count'][df.index==j] = df['comment_count'].iloc[j]*1.2\n",
    "                    if word == \"公式\":\n",
    "                        word = \"official\"\n",
    "                    df.loc[df.index==j, f'title_{word}'] = 1\n",
    "                    if not f'title_{word}' in feats_increased:\n",
    "                        feats_increased += [f'title_{word}']\n",
    "                \n",
    "        if option[\"tags\"]:\n",
    "            tags_text = \" \".join(tokenizer.tokenize(tags_sentence, wakati=True))\n",
    "            for word in tags_words:\n",
    "                if (word in tags_text) | (f\"{word}s\" in tags_text):\n",
    "                    #df['likes'][df.index==j] = df['likes'].iloc[j]*1.2\n",
    "                    #df['dislikes'][df.index==j] = df['dislikes'].iloc[j]*1.2\n",
    "                    #df['comment_count'][df.index==j] = df['comment_count'].iloc[j]*1.2\n",
    "                    df.loc[df.index==j, f'tags_{word}'] = 1\n",
    "                    if not f'tags_{word}' in feats_increased:\n",
    "                        feats_increased += [f'tags_{word}']\n",
    "\n",
    "        if option[\"description\"]:\n",
    "            desc_text = \" \".join(tokenizer.tokenize(desc_sentence, wakati=True))\n",
    "            for word in desc_words:\n",
    "                if (word in desc_text) | (f\"{word}s\" in desc_text):\n",
    "                    #df['likes'][df.index==j] = df['likes'].iloc[j]*1.2\n",
    "                    #df['dislikes'][df.index==j] = df['dislikes'].iloc[j]*1.2\n",
    "                    #df['comment_count'][df.index==j] = df['comment_count'].iloc[j]*1.2\n",
    "                    df.loc[df.index==j, f'desc_{word}'] = 1\n",
    "                    if not f'desc_{word}' in feats_increased:\n",
    "                        feats_increased += [f'desc_{word}']\n",
    "\n",
    "    features[\"ohe\"] += feats_increased\n",
    "    feats_increased_dict = {k: 0 for k in feats_increased}\n",
    "    df.fillna(feats_increased_dict, inplace=True)\n",
    "    df[feats_increased] = df[feats_increased].astype('int')\n",
    "        \n",
    "    return df, features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ラベルエンコーディング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encoder(df, cols_to_encode=[]):\n",
    "    lbl_enc_columns = cols_to_encode #cat_features + date_features + ohe_features\n",
    "\n",
    "    # Transforming all the labels of all variables\n",
    "    label_encoders = [LabelEncoder() for _ in range(len(lbl_enc_columns))]\n",
    "\n",
    "    for col, column in enumerate(lbl_enc_columns):\n",
    "        unique_values = pd.Series(df[column].unique())\n",
    "        unique_values = unique_values[unique_values.notnull()]\n",
    "        label_encoders[col].fit(unique_values)\n",
    "        df.loc[df[column].notnull(), column] = label_encoders[col].transform(df.loc[df[column].notnull(), column])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 標準化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardization(df, fnc_name=\"Standard\", cols_to_std=[]):\n",
    "    fnc_scaler = {\"Standard\": StandardScaler(copy=True, with_mean=True, with_std=True),\n",
    "                  \"MinMax\": MinMaxScaler(feature_range=(-1, 1), copy=True),\n",
    "                  \"MaxAbs\": MaxAbsScaler(copy=True),\n",
    "                  \"Normalize\": Normalizer(norm='l2'),\n",
    "                  \"Robust\": RobustScaler(with_centering=True, with_scaling=True, quantile_range=(25.0, 75.0), copy=True),\n",
    "                  \"Quantile\": QuantileTransformer(n_quantiles=1000, output_distribution='normal', ignore_implicit_zeros=False, \\\n",
    "                                                  subsample=100000, random_state=None, copy=True),\n",
    "                  #\"box-cox\": PowerTransformer(method='box-cox'),\n",
    "                  \"yeo\": PowerTransformer(method='yeo-johnson', standardize=True, copy=True)\n",
    "                 }\n",
    "    scaler = fnc_scaler[fnc_name]\n",
    "    df[cols_to_std] = scaler.fit_transform(df[cols_to_std])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# featuresの削除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_features(df, features, features_to_drop):\n",
    "    cols_to_drop = features_to_drop[\"num\"]+features_to_drop[\"cat\"]+features_to_drop[\"date\"]+features_to_drop[\"ohe\"]\n",
    "    df.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "    for col in features_to_drop[\"num\"]:\n",
    "        features[\"num\"].remove(col)\n",
    "\n",
    "    for col in features_to_drop[\"cat\"]:\n",
    "        features[\"cat\"].remove(col)\n",
    "\n",
    "    for col in features_to_drop[\"date\"]:\n",
    "        features[\"date\"].remove(col)\n",
    "\n",
    "    for col in features_to_drop[\"ohe\"]:\n",
    "        features[\"ohe\"].remove(col)\n",
    "\n",
    "    return df, features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 学習/予測用データの準備 (#1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_features(df, features):\n",
    "    print(\"-\"*40)\n",
    "    print(f\"実特徴量数: {len(df.columns)} / 計算上の特徴量数: {len(features['num'])+len(features['cat'])+len(features['date'])+len(features['ohe'])}\")\n",
    "    print(\"-\"*40)\n",
    "    print(f\"(内訳) num_features: {len(features['num'])}, cat_features: {len(features['cat'])}, date_features: {len(features['date'])}, ohe_features: {len(features['ohe'])}\")\n",
    "    print(\"-\"*40)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "seed_everything(seed=47)\n",
    "\n",
    "df = pd.concat([train, test]).reset_index(drop=True)\n",
    "\n",
    "df.loc[df['categoryId']==43, 'categoryId'] = 30\n",
    "\n",
    "# 特徴量の分類\n",
    "features = {\"cat\": ['video_id', 'title', 'channelId', 'channelTitle', 'categoryId', 'tags', 'thumbnail_link', 'description', 'comments_disabled', 'ratings_disabled'],\n",
    "            \"date\": ['publishedAt', 'collection_date'],\n",
    "            \"num\": ['likes', 'dislikes', 'comment_count'],\n",
    "            \"ohe\": []\n",
    "           }\n",
    "\n",
    "# ラベルエンコーディング\n",
    "print(\"ラベルエンコーディング\")\n",
    "df = label_encoder(df, cols_to_encode=['categoryId'])\n",
    "\n",
    "# 特徴量生成\n",
    "print(\"特徴量生成\")\n",
    "df, features = create_features(df, features)\n",
    "\n",
    "# 特徴量生成\n",
    "print(\"特徴量生成4\")\n",
    "df, features = create_features4(df, features, option={\"title\": False, \"tags\": False, \"description\": False})\n",
    "\n",
    "# ラベルエンコーディング\n",
    "print(\"ラベルエンコーディング\")\n",
    "df = label_encoder(df, cols_to_encode=features[\"cat\"]+features[\"date\"])\n",
    "    \n",
    "# 特徴量生成2\n",
    "print(\"特徴量生成2 categoryId - likes/dislikes/comment_count/channelId\")\n",
    "df, features = create_features2(df, features,\n",
    "                                cols_groupby=['categoryId', 'comments_disabled', 'ratings_disabled', 'n_tags', 'len_description'],\n",
    "                                cols_transform=['likes', 'dislikes', 'comment_count', 'channelId'],\n",
    "                                                #'likes_cut', 'dislikes_cut', 'comment_count_cut',\n",
    "                                                #'likes_qcut', 'dislikes_qcut', 'comment_count_qcut'],\n",
    "                                target_func=['max', 'min', 'mean'],\n",
    "                                option={\"log\": False,\\\n",
    "                                        \"sqrt\": False, \"sqrt_log\": False, \"log_sqrt\": False,\\\n",
    "                                        \"pow\": False, \"pow_log\": False, \"log_pow\": False})\n",
    "\n",
    "print(\"特徴量生成2 channelId - all features\")\n",
    "df, features = create_features2(df, features,\n",
    "                                cols_groupby=['channelId'],\n",
    "                                cols_transform=list(set(features['num'] + features['cat'] + features['ohe'])),\n",
    "                                target_func=['max', 'min', 'mean'],\n",
    "                                option={\"log\": False, \\\n",
    "                                        \"sqrt\": False, \"sqrt_log\": False, \"log_sqrt\": False,\\\n",
    "                                        \"pow\": False, \"pow_log\": False, \"log_pow\": False})\n",
    "# 特徴量生成3\n",
    "print(\"特徴量生成3\")\n",
    "df, features = create_features3(df, features,\n",
    "                                cols_transform=[c for c in df.columns if ('likes' in c)|('dislikes' in  c)|('comment_count' in c)],\n",
    "                                option={\"log\": False,\\\n",
    "                                        \"sqrt\": True, \"sqrt_log\": False, \"log_sqrt\": False,\\\n",
    "                                        \"pow\": True, \"pow_log\": False, \"log_pow\": True})\n",
    "# 特徴量数のチェック\n",
    "print(\"特徴量のチェック\")\n",
    "check_features(df, features)\n",
    "\n",
    "# 特徴量削除\n",
    "print(\"特徴量削除\")\n",
    "feats_to_drop = {\"cat\": ['video_id', 'channelId', 'title', 'channelTitle', 'tags', 'thumbnail_link', 'description'],\n",
    "                 \"date\": ['publishedAt', 'collection_date'],\n",
    "                 \"ohe\": [],\n",
    "                 \"num\": []}\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------\n",
    "# CV: 0.725691, LB: 0.723\n",
    "#-----------------------------------------------------------------------------------------------------------\n",
    "feats_to_drop[\"cat\"] += ['publishedAt_second', 'publishedAt_minute', 'publishedAt_hour', 'publishedAt_day', 'publishedAt_dayofweek']\n",
    "df, features = drop_features(df, features, feats_to_drop)\n",
    "\n",
    "# nunique()==1の特徴量を削除\n",
    "for col in df.loc[:, df.nunique()==1].columns:\n",
    "    features[\"num\"].remove(col)\n",
    "df.drop(df.loc[:, df.nunique()==1].columns, axis=1, inplace=True)\n",
    "\n",
    "# データ型変換\n",
    "print(\"データ型変換\")\n",
    "#df[features[\"num\"]] = df[features[\"num\"]].astype('float32')\n",
    "df = df.astype('float')\n",
    "df[features[\"cat\"]] = df[features[\"cat\"]].astype('int')\n",
    "df[features[\"ohe\"]] = df[features[\"ohe\"]].astype('int')\n",
    "\n",
    "print(\"標準化\")\n",
    "df = standardization(df, fnc_name=\"MinMax\", cols_to_std=features[\"num\"])\n",
    "\n",
    "# 学習、予測データ分割\n",
    "X_train = df.iloc[:y.shape[0], :].reset_index(drop=True)\n",
    "X_test  = df.iloc[y.shape[0]:, :].reset_index(drop=True)\n",
    "\n",
    "# 特徴量数のチェック\n",
    "print(\"特徴量のチェック\")\n",
    "check_features(df, features)\n",
    "\n",
    "# 欠損値、無限大/無限小値有無のチェック\n",
    "print(f\"学習データ中の欠損値数: {X_train.isnull().sum().sum()}\")\n",
    "print(f\"学習データ中の無限値数: {np.count_nonzero(np.isinf(X_train))}\")\n",
    "print(f\"予測データ中の欠損値数: {X_test.isnull().sum().sum()}\")\n",
    "print(f\"予測データ中の無現値数: {np.count_nonzero(np.isinf(X_test))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "display(description(X_train))\n",
    "display(description(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データ分割\n",
    "### comments_disabled(True/False)によるデータ分割\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "X_train_false = X_train[(train['comments_disabled']==False)]\n",
    "y_false       = y[(train['comments_disabled']==False)]\n",
    "X_test_false  = X_test[test['comments_disabled']==False]\n",
    "\n",
    "X_train_false_index = X_train_false.index\n",
    "X_test_false_index  = X_test_false.index\n",
    "\n",
    "X_train_true = X_train[train['comments_disabled']==True]\n",
    "y_true       = y[train['comments_disabled']==True]\n",
    "X_test_true  = X_test[test['comments_disabled']==True]\n",
    "\n",
    "X_train_true_index = X_train_true.index\n",
    "X_test_true_index  = X_test_true.index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Model\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SPLITS = 10\n",
    "SEED = 47\n",
    "LEARNING_RATE = 5e-4\n",
    "BATCH_SIZE = 32\n",
    "PATIENCE = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_callbacks():\n",
    "    callbacks = []\n",
    "    \n",
    "    callbacks.append(EarlyStopping(monitor='val_loss',\n",
    "                                   min_delta=0,\n",
    "                                   patience=PATIENCE,\n",
    "                                   verbose=0,\n",
    "                                   mode='auto',\n",
    "                                   baseline=None,\n",
    "                                   restore_best_weights=True))\n",
    "    \n",
    "    # Update the learning rate every epoch\n",
    "    callbacks.append(LearningRateScheduler(lambda x: LEARNING_RATE * 0.95 ** x))\n",
    "    \n",
    "    return callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn model def."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn(lr, input_shape):\n",
    "\n",
    "    model = Sequential([\n",
    "        Dense(2 ** 8, activation='relu', input_dim=input_shape, kernel_initializer='he_normal'),\n",
    "        Dense(2 ** 7, activation='relu', kernel_initializer='he_normal'),\n",
    "        Dense(2 ** 6, activation='relu', kernel_initializer='he_normal'),\n",
    "        Dense(2 ** 5, activation='relu', kernel_initializer='he_normal'),\n",
    "        Dense(2 ** 4, activation='relu', kernel_initializer='he_normal'),\n",
    "        Dense(2 ** 3, activation='relu', kernel_initializer='he_normal'),\n",
    "\n",
    "        Dense(2 ** 3, activation='relu', kernel_initializer='he_normal'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "\n",
    "    # COMPILE WITH ADAM OPTIMIZER AND CROSS ENTROPY COST    \n",
    "    adam_opt = Adam(learning_rate=lr, beta_1=0.9, beta_2=0.999, amsgrad=True)\n",
    "    nadam_opt = Nadam(learning_rate=lr, beta_1=0.9, beta_2=0.999)\n",
    "    ladam_opt = tfa.optimizers.LazyAdam(learning_rate=LEARNING_RATE, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False)\n",
    "    adamw_opt = tfa.optimizers.AdamW(learning_rate=LEARNING_RATE, weight_decay=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=True)\n",
    "    rmsprop_opt = RMSprop(learning_rate=lr, rho=0.9)\n",
    "    sgd_opt = SGD(learning_rate=lr, momentum=0.0, nesterov=False)\n",
    "    sgd_opt = SGD(learning_rate=lr, decay=1e-4, momentum=0.9, nesterov=True)\n",
    "\n",
    "    model.compile(optimizer=nadam_opt,loss='mean_squared_error', metrics=tf.keras.metrics.RootMeanSquaredError())\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn2 model def."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn2(lr, input_shape):\n",
    "\n",
    "    model = Sequential([\n",
    "        Dense(2 ** 8, activation='relu', input_dim=input_shape, kernel_initializer='he_normal'),\n",
    "        Dense(2 ** 7, activation='relu', kernel_initializer='he_normal'),\n",
    "        Dense(2 ** 6, activation='relu', kernel_initializer='he_normal'),\n",
    "        \n",
    "        Dense(2 ** 3, activation='relu', kernel_initializer='he_normal'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "\n",
    "    # COMPILE WITH ADAM OPTIMIZER AND CROSS ENTROPY COST    \n",
    "    adam_opt = Adam(learning_rate=lr, beta_1=0.9, beta_2=0.999, amsgrad=True)\n",
    "    nadam_opt = Nadam(learning_rate=lr, beta_1=0.9, beta_2=0.999)\n",
    "    ladam_opt = tfa.optimizers.LazyAdam(learning_rate=LEARNING_RATE, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False)\n",
    "    adamw_opt = tfa.optimizers.AdamW(learning_rate=LEARNING_RATE, weight_decay=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=True)\n",
    "    rmsprop_opt = RMSprop(learning_rate=lr, rho=0.9)\n",
    "    sgd_opt = SGD(learning_rate=lr, momentum=0.0, nesterov=False)\n",
    "    sgd_opt = SGD(learning_rate=lr, decay=1e-4, momentum=0.9, nesterov=True)\n",
    "\n",
    "    model.compile(optimizer=nadam_opt,loss='mean_squared_error', metrics=tf.keras.metrics.RootMeanSquaredError())\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn model学習、予測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "history, history_false = [], []\n",
    "score, score_false = [], []\n",
    "pred_train, pred_train_false = np.zeros((X_train.shape[0])), np.zeros((X_train_false.shape[0]))\n",
    "pred_full_test, pred_full_test_false = 0, 0\n",
    "\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "\n",
    "for fold_id, (train_idx, val_idx) in enumerate(tqdm(skf.split(X_train, (train.comments_disabled)))):\n",
    "    print(\"*\"*80)\n",
    "    print(f\"Started TF learning(1) fold:{fold_id+1} / {N_SPLITS}\")\n",
    "\n",
    "    # 全データで学習、予測\n",
    "    model = nn(lr=LEARNING_RATE, input_shape=X_train.shape[1])\n",
    "    callbacks = create_callbacks()\n",
    "\n",
    "    tr_X, val_X = X_train.iloc[train_idx].copy(), X_train.iloc[val_idx].copy()\n",
    "    tr_y, val_y = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        \n",
    "    history.append(model.fit(tr_X, tr_y, batch_size=BATCH_SIZE,\n",
    "                             epochs=300,\n",
    "                             verbose=2,\n",
    "                             validation_data=(val_X, val_y),\n",
    "                             callbacks=callbacks))\n",
    "    \n",
    "    pred_train[val_idx] = model.predict(val_X).reshape(-1)\n",
    "    score.append(model.evaluate(val_X, val_y, batch_size=BATCH_SIZE, verbose=0, return_dict=True))\n",
    "    pred_full_test = pred_full_test + model.predict(X_test)\n",
    "    \n",
    "    RMSLE = mean_squared_error(y[val_idx], pred_train[val_idx], squared=False)\n",
    "    print(f\"RMSLE={RMSLE}\")\n",
    "\n",
    "RMSLE_overall = mean_squared_error(y, pred_train, squared=False)\n",
    "print(f\"Overall RMSLE={RMSLE_overall}\")\n",
    "\n",
    "# Make submission\n",
    "print(\"Saving submission file\")\n",
    "submission = pd.DataFrame({'id': test_id, 'y': np.expm1((pred_full_test/N_SPLITS).reshape(-1))})\n",
    "submission.to_csv(f\"./{out_dir}/submission_NN_SEED{SEED}_FOLDS{N_SPLITS}_CV{RMSLE_overall:.6f}.csv\", index=False)\n",
    "\n",
    "with open(f\"./{out_dir}/NN_train_SEED{SEED}_FOLDS{N_SPLITS}.pickle\", 'wb') as f:\n",
    "    pickle.dump(pred_train, f)\n",
    "with open(f\"./{out_dir}/NN_test_SEED{SEED}_FOLDS{N_SPLITS}.pickle\", 'wb') as f:\n",
    "    pickle.dump((pred_full_test/N_SPLITS).reshape(-1), f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for j in range(N_SPLITS):\n",
    "    hist = pd.DataFrame(history[j].history)\n",
    "    hist['epoch'] = history[j].epoch\n",
    "    display(hist.tail(PATIENCE+2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for j in range(N_SPLITS):\n",
    "    plot_history(history[j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn2 model学習、予測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "history, history_false = [], []\n",
    "score, score_false = [], []\n",
    "pred_train, pred_train_false = np.zeros((X_train.shape[0])), np.zeros((X_train_false.shape[0]))\n",
    "pred_full_test, pred_full_test_false = 0, 0\n",
    "\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "\n",
    "for fold_id, (train_idx, val_idx) in enumerate(tqdm(skf.split(X_train, (train.comments_disabled)))):\n",
    "    print(\"*\"*80)\n",
    "    print(f\"Started TF learning(1) fold:{fold_id+1} / {N_SPLITS}\")\n",
    "\n",
    "    # 全データで学習、予測\n",
    "    model = nn(lr=LEARNING_RATE, input_shape=X_train.shape[1])\n",
    "    callbacks = create_callbacks()\n",
    "\n",
    "    tr_X, val_X = X_train.iloc[train_idx].copy(), X_train.iloc[val_idx].copy()\n",
    "    tr_y, val_y = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        \n",
    "    history.append(model.fit(tr_X, tr_y, batch_size=BATCH_SIZE,\n",
    "                             epochs=300,\n",
    "                             verbose=2,\n",
    "                             validation_data=(val_X, val_y),\n",
    "                             callbacks=callbacks))\n",
    "    \n",
    "    pred_train[val_idx] = model.predict(val_X).reshape(-1)\n",
    "    score.append(model.evaluate(val_X, val_y, batch_size=BATCH_SIZE, verbose=0, return_dict=True))\n",
    "    pred_full_test = pred_full_test + model.predict(X_test)\n",
    "    \n",
    "    RMSLE = mean_squared_error(y[val_idx], pred_train[val_idx], squared=False)\n",
    "    print(f\"RMSLE={RMSLE}\")\n",
    "\n",
    "RMSLE_overall = mean_squared_error(y, pred_train, squared=False)\n",
    "print(f\"Overall RMSLE={RMSLE_overall}\")\n",
    "\n",
    "# Make submission\n",
    "print(\"Saving submission file\")\n",
    "submission = pd.DataFrame({'id': test_id, 'y': np.expm1((pred_full_test/N_SPLITS).reshape(-1))})\n",
    "submission.to_csv(f\"./{out_dir}/submission_NN2_SEED{SEED}_FOLDS{N_SPLITS}_CV{RMSLE_overall:.6f}.csv\", index=False)\n",
    "\n",
    "with open(f\"./{out_dir}/NN2_train_SEED{SEED}_FOLDS{N_SPLITS}.pickle\", 'wb') as f:\n",
    "    pickle.dump(pred_train, f)\n",
    "with open(f\"./{out_dir}/NN2_test_SEED{SEED}_FOLDS{N_SPLITS}.pickle\", 'wb') as f:\n",
    "    pickle.dump((pred_full_test/N_SPLITS).reshape(-1), f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for j in range(N_SPLITS):\n",
    "    hist = pd.DataFrame(history[j].history)\n",
    "    hist['epoch'] = history[j].epoch\n",
    "    display(hist.tail(PATIENCE+2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for j in range(N_SPLITS):\n",
    "    plot_history(history[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datetime.datetime.now()-start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
