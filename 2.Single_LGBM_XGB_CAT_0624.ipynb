{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ProbSpace: YouTube動画視聴回数予測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SKF = False\n",
    "\n",
    "LGB = True\n",
    "XGB = True\n",
    "CAT = True\n",
    "\n",
    "DATASET = 1\n",
    "\n",
    "out_dir = \"out_tmp\"\n",
    "!mkdir -p $out_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "import itertools\n",
    "import os, datetime, gc, glob, re, random\n",
    "import time, datetime\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import optuna\n",
    "import bhtsne, umap\n",
    "\n",
    "from janome.tokenizer import Tokenizer\n",
    "from janome.analyzer import Analyzer\n",
    "from janome.tokenfilter import *\n",
    "from janome.charfilter import UnicodeNormalizeCharFilter, RegexReplaceCharFilter\n",
    "import unicodedata\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import Pool, CatBoostRegressor, CatBoostClassifier\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.linear_model import BayesianRidge, ElasticNet, Lasso, LogisticRegression, Ridge, SGDRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor, BaggingRegressor\n",
    "from sklearn.ensemble import StackingRegressor, VotingRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor, GradientBoostingRegressor, RandomForestRegressor\n",
    "\n",
    "from ngboost import NGBRegressor\n",
    "from ngboost.ngboost import NGBoost\n",
    "from ngboost.learners import default_tree_learner\n",
    "from ngboost.scores import MLE, CRPS, LogScore\n",
    "from ngboost.distns import Normal, LogNormal\n",
    "\n",
    "from sklearn.linear_model import BayesianRidge, ElasticNet, Lasso, LogisticRegression, Ridge, SGDRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans, DBSCAN\n",
    "from sklearn.model_selection import KFold, RepeatedKFold, StratifiedKFold, ShuffleSplit, GroupShuffleSplit, StratifiedShuffleSplit\n",
    "from sklearn.model_selection import cross_validate, cross_val_predict, train_test_split\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, roc_auc_score, log_loss\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler, Normalizer, RobustScaler, QuantileTransformer, PowerTransformer\n",
    "from sklearn.feature_selection import SelectFromModel, RFE, SelectPercentile, SelectKBest\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.optimizers import Adadelta, Adagrad, Adam, Adamax, Ftrl, Nadam, RMSprop, SGD\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, EarlyStopping, TensorBoard, LambdaCallback\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Concatenate, Lambda\n",
    "from tensorflow.keras.layers import Activation, Average, Dense, Dropout, Flatten, BatchNormalization, LeakyReLU, Input\n",
    "from tensorflow.keras.layers import Conv2D, SeparableConv2D, MaxPooling2D, AveragePooling2D, GlobalAveragePooling2D\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import missingno as msno \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for variable description\n",
    "def description(df):\n",
    "    summary = pd.DataFrame(df.dtypes, columns=['dtypes'])\n",
    "    summary = summary.reset_index()\n",
    "    summary['Name'] = summary['index']\n",
    "    summary = summary[['Name','dtypes']]\n",
    "    summary['Missing'] = df.isnull().sum().values    \n",
    "    summary['Uniques'] = df.nunique().values\n",
    "    summary['Mean'] = np.nanmean(df, axis=0).astype(df.dtypes)\n",
    "    summary['Std'] = np.nanstd(df, axis=0).astype(df.dtypes)\n",
    "    summary['Minimum'] = np.nanmin(df, axis=0).astype(df.dtypes)\n",
    "    summary['Maximum'] = np.nanmax(df, axis=0).astype(df.dtypes)\n",
    "    summary['First Value'] = df.iloc[0].values\n",
    "    summary['Second Value'] = df.iloc[1].values\n",
    "    summary['Third Value'] = df.iloc[2].values\n",
    "    summary['dimension'] = str(df.shape)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hist(target):\n",
    "    plt.hist(target, bins=100)\n",
    "\n",
    "    print(\"max:  {:>10,.6f}\".format(target.max()))\n",
    "    print(\"min:  {:>10,.6f}\".format(target.min()))\n",
    "    print(\"mean: {:>10,.6f}\".format(target.mean()))\n",
    "    print(\"std:  {:>10,.6f}\".format(target.std()))\n",
    "    \n",
    "    return\n",
    "\n",
    "def get_hist4(target1, title1, target2, title2, target3, title3, target4, title4):\n",
    "    fig = plt.figure(figsize=(18, 18))\n",
    "\n",
    "    ax1 = fig.add_subplot(5,1,1)\n",
    "    ax2 = fig.add_subplot(5,1,2)\n",
    "    ax3 = fig.add_subplot(5,1,3)\n",
    "    ax4 = fig.add_subplot(5,1,4)\n",
    "    ax5 = fig.add_subplot(5,1,5)\n",
    "\n",
    "    ax1.set_title(title1)\n",
    "    ax2.set_title(title2)\n",
    "    ax3.set_title(title3)\n",
    "    ax4.set_title(title4)\n",
    "    ax5.set_title(\"OVERALL\")\n",
    "    \n",
    "    ax1.hist(target1, bins=100)\n",
    "    ax2.hist(target2, bins=100)\n",
    "    ax3.hist(target3, bins=100)\n",
    "    ax4.hist(target4, bins=100)\n",
    "\n",
    "    ax5.hist(target1, bins=100, alpha=0.2, color='red')\n",
    "    ax5.hist(target2, bins=100, alpha=0.2, color='green')\n",
    "    ax5.hist(target3, bins=100, alpha=0.2, color='blue')\n",
    "    #ax5.hist(target4, bins=100, alpha=0.2, color='grey')\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# for train/test data\n",
    "train_data = pd.read_csv(\"./input/train_data.csv\")\n",
    "test_data = pd.read_csv(\"./input/test_data.csv\")\n",
    "\n",
    "# use predicted dislikes,likes,comment_out\n",
    "dislikes_pred = pd.read_csv(\"./input/dislikes_pred_new.csv\")\n",
    "likes_pred = pd.read_csv(\"./input/likes_pred_new.csv\")\n",
    "comments_pred = pd.read_csv(\"./input/comment_count_pred_new.csv\")\n",
    "\n",
    "y = np.log1p(train_data['y']).copy()\n",
    "y_bin = pd.cut(train_data['y'], [0, 10, 100,1000,10000,100000,1000000,10000000000], labels=[1,2,3,4,5,6,7])\n",
    "y_bin = y_bin.astype(int)\n",
    "test_id = test_data.id\n",
    "\n",
    "train = train_data.drop(['id', 'y'], axis=1).copy()\n",
    "test  = test_data.drop(['id'], axis=1).copy()\n",
    "\n",
    "traintest = pd.concat([train, test]).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 目的変数の分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_hist(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in train_data.columns:\n",
    "    print(\"{:<20}: {} ({:.2f}%)\".format(col, train_data[col].isnull().sum(), train_data[col].isnull().sum()/train_data.shape[0]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in test_data.columns:\n",
    "    print(\"{:<20}: {} ({:.2f}%)\".format(col, test_data[col].isnull().sum(), test_data[col].isnull().sum()/test_data.shape[0]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# seedの固定化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=1234):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    #torch.manual_seed(seed)\n",
    "    #torch.cuda.manual_seed(seed)\n",
    "    #torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特徴量生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(df, features):\n",
    "    # 欠損値処理\n",
    "    df['tags'].fillna(\"[none]\", inplace=True)\n",
    "    df['description'].fillna(df['tags'].replace(\"|\", \" \") + df['title'], inplace=True)\n",
    "\n",
    "    tokenizer = Tokenizer()\n",
    "    char_filters = [UnicodeNormalizeCharFilter(), RegexReplaceCharFilter(r\"[0123456789!#$%&()=~|\\-^\\\\@`{;:+*},./\\<>?_♪®」—]\", \"\")]\n",
    "    token_filters = [POSKeepFilter(['名詞'])]\n",
    "    #token_filters = [POSStopFilter(['接続詞', '接頭辞', '接尾辞', '記号', '助詞', '助動詞']), TokenCountFilter()]\n",
    "    a = Analyzer(char_filters, tokenizer, token_filters=token_filters)\n",
    "\n",
    "    df.loc[df['tags']==\"[none]\", 'tags'] = \\\n",
    "    df['title'][df['tags']==\"[none]\"].str.lower().apply(lambda x: \"|\".join([word.surface for word in a.analyze(x)]))\n",
    "    \n",
    "    # ------------------------------------------\n",
    "    # 比率\n",
    "    # ------------------------------------------\n",
    "    df['likes_dislikes_ratio'] = df['likes']/(df['dislikes']+1)\n",
    "    df['comment_count_likes_ratio'] = df['comment_count']/(df['likes']+1)\n",
    "    df['comment_count_dislikes_ratio'] = df['comment_count']/(df['dislikes']+1)\n",
    "\n",
    "    features['num'] += ['likes_dislikes_ratio', 'comment_count_likes_ratio', 'comment_count_dislikes_ratio']\n",
    "\n",
    "    # ------------------------------------------\n",
    "    # 積算\n",
    "    # ------------------------------------------\n",
    "    df['mul_likes_comments_disabled'] = df['likes'] * df['comments_disabled']\n",
    "    df['mul_dislikes_comments_disabled'] = df['dislikes'] * df['comments_disabled']\n",
    "    df['mul_comment_count_ratings_disabled'] = df['comment_count'] * df['ratings_disabled']\n",
    "\n",
    "    features['num'] += ['mul_likes_comments_disabled', 'mul_dislikes_comments_disabled', 'mul_comment_count_ratings_disabled']\n",
    "    \n",
    "    # ------------------------------------------\n",
    "    # 出現頻度\n",
    "    # ------------------------------------------\n",
    "    for col in ['channelId', 'channelTitle', 'categoryId']:\n",
    "        df['_'.join(list(map(str, ['freq', col])))] = df[col].map(df[col].value_counts())\n",
    "        \n",
    "        features['num'] += ['_'.join(list(map(str, ['freq', col])))]\n",
    "\n",
    "    # ------------------------------------------\n",
    "    # 'n_tags'数特徴量の生成\n",
    "    # ------------------------------------------    \n",
    "    df['n_tags'] = df['tags'].astype(str).apply(lambda x: len(x.split(\"|\")))\n",
    "    features['num'] += ['n_tags']\n",
    "    \n",
    "    # ------------------------------------------\n",
    "    # 'description'関連の特徴量生成\n",
    "    # ------------------------------------------    \n",
    "    df['http_count_in_desc'] = df['description'].apply(lambda x: x.lower().count(\"http\"))\n",
    "    df['len_description'] = df['description'].apply(lambda x: len(x))\n",
    "    df['len_title'] = df['title'].apply(lambda x: len(x))\n",
    "\n",
    "    features['num'] += ['http_count_in_desc', 'len_description', 'len_title']\n",
    "    \n",
    "    # ------------------------------------------\n",
    "    # 'title'/'tag'/'description'内の記述言語関連特徴量の生成\n",
    "    # ------------------------------------------    \n",
    "    def checkJapanese(word):\n",
    "        for ch in word:\n",
    "            try:\n",
    "                name = unicodedata.name(ch) \n",
    "                if \"CJK UNIFIED\" in name \\\n",
    "                or \"HIRAGANA\" in name \\\n",
    "                or \"KATAKANA\" in name:\n",
    "                    return True\n",
    "            except:\n",
    "              continue\n",
    "        return False\n",
    "\n",
    "    def checkAlnum(word):\n",
    "        alnum = re.compile(r'^[a-zA-Z0-9]+$')\n",
    "        result = alnum.match(word) is not None\n",
    "        return result\n",
    "\n",
    "    # is japanese\n",
    "    df['isJa_title'] = df['title'].apply(lambda x: checkJapanese(x))\n",
    "    df['isJa_tags'] = df['tags'].apply(lambda x: checkJapanese(x))\n",
    "    df['isJa_description'] = df['description'].apply(lambda x: checkJapanese(x))\n",
    "\n",
    "    features['ohe'] += ['isJa_title', 'isJa_tags', 'isJa_description']\n",
    "    \n",
    "    # isalnum\n",
    "    df['isalnum_title'] = df['title'].apply(lambda x: checkAlnum(x))\n",
    "    df['isalnum_tags'] = df['tags'].apply(lambda x: checkAlnum(x))\n",
    "    df['isalnum_description'] = df['description'].apply(lambda x: checkAlnum(x))\n",
    "\n",
    "    features['ohe'] += ['isalnum_title', 'isalnum_tags', 'isalnum_description']\n",
    "    \n",
    "    # cotain english\n",
    "    df['inclEn_title'] = df['title'].apply(lambda x: len(re.findall(r'[a-zA-Z0-9]', x.lower())))\n",
    "    df['inclEn_tags'] = df['tags'].apply(lambda x: len(re.findall(r'[a-zA-Z0-9]', x.lower())))\n",
    "    df['inclEn_description'] = df['description'].apply(lambda x: len(re.findall(r'[a-zA-Z0-9]', x.lower())))\n",
    "    \n",
    "    features['num'] += ['inclEn_title', 'inclEn_tags', 'inclEn_description']\n",
    "\n",
    "    # ------------------------------------------\n",
    "    # 投稿時期、採取時期からの期間、日時関連特徴量の生成\n",
    "    # ------------------------------------------    \n",
    "    # publishedAt\n",
    "    df['publishedAt'] = pd.to_datetime(df['publishedAt'], utc=True)\n",
    "    df['publishedAt_year'] = df['publishedAt'].apply(lambda x: x.year)\n",
    "    df['publishedAt_month'] = df['publishedAt'].apply(lambda x: x.month)\n",
    "    df['publishedAt_day'] = df['publishedAt'].apply(lambda x: x.day)\n",
    "    df['publishedAt_hour'] = df['publishedAt'].apply(lambda x: x.hour)\n",
    "    df['publishedAt_minute'] = df['publishedAt'].apply(lambda x: x.minute)\n",
    "    df['publishedAt_second'] = df['publishedAt'].apply(lambda x: x.second)\n",
    "    df['publishedAt_dayofweek'] = df['publishedAt'].apply(lambda x: x.dayofweek)\n",
    "\n",
    "    df['collection_date'] = \\\n",
    "    df['collection_date'].map(lambda x: x.split('.')).map(lambda x: '20'+x[0]+'-'+x[2]+'-'+x[1]+'T00:00:00.000Z')\n",
    "        \n",
    "    # collection_date\n",
    "    df['collection_date'] = pd.to_datetime(df['collection_date'], utc=True)\n",
    "    df['collection_date_year'] = df['collection_date'].apply(lambda x: x.year)\n",
    "    df['collection_date_month'] = df['collection_date'].apply(lambda x: x.month)\n",
    "    df['collection_date_day'] = df['collection_date'].apply(lambda x: x.day)\n",
    "\n",
    "    # delta\n",
    "    df['delta'] = (df['collection_date'] - df['publishedAt']).apply(lambda x: x.days)\n",
    "    df['log_delta'] = np.log(df['delta'])\n",
    "    df['sqrt_delta'] = np.sqrt(df['delta'])\n",
    "    df['pow_delta'] = pow(df['delta'], 2)\n",
    "    df['log_pow_delta'] = pow(np.log(df['delta']), 2)\n",
    "    df['publishedAt_delta'] = (df['publishedAt'] - df['publishedAt'].min()).apply(lambda x: x.days)\n",
    "    df['collection_delta'] = (df['collection_date'] - df['collection_date'].min()).apply(lambda x: x.days)\n",
    "    \n",
    "    features['cat'] += ['publishedAt_year', 'publishedAt_month', 'publishedAt_day', \\\n",
    "                        'publishedAt_hour', 'publishedAt_minute', 'publishedAt_second', 'publishedAt_dayofweek', \\\n",
    "                        'collection_date_year', 'collection_date_month', 'collection_date_day']\n",
    "    \n",
    "    features['num'] += ['delta', 'log_delta', 'sqrt_delta', 'pow_delta', 'log_pow_delta', \\\n",
    "                        'publishedAt_delta', 'collection_delta']\n",
    "    \n",
    "    return df, features\n",
    "    \n",
    "def create_features2(df, features, cols_groupby, cols_transform, target_func, option):\n",
    "    # ------------------------------------------\n",
    "    # 'cols_groupby'ごとの特徴量生成\n",
    "    # ------------------------------------------   \n",
    "    \n",
    "    cols_to_transform = list(set(cols_transform) - set(cols_groupby))\n",
    "\n",
    "    #cols_to_transform = [c for c in df.columns if ('likes' in c) | ('dislikes' in c) | ('comment_count' in c)]\n",
    "    for col_base in cols_groupby:\n",
    "        for col in cols_to_transform:\n",
    "            for func in target_func:\n",
    "                df['_'.join(list(map(str, [col_base, col, func])))] = df.groupby(col_base)[col].transform(func)\n",
    "                features['num'] += ['_'.join(list(map(str, [col_base, col, func])))]\n",
    "                \n",
    "                if option['log']:\n",
    "                    df['_'.join(list(map(str, [col_base, col, 'log', func])))] = np.log1p(df.groupby(col_base)[col].transform(func))\n",
    "                    features['num'] += ['_'.join(list(map(str, [col_base, col, 'log', func])))]\n",
    "                if option['sqrt']:\n",
    "                    df['_'.join(list(map(str, [col_base, col, 'sqrt', func])))] = np.sqrt(df.groupby(col_base)[col].transform(func))\n",
    "                    features['num'] += ['_'.join(list(map(str, [col_base, col, 'sqrt', func])))]\n",
    "                if option['sqrt_log']:\n",
    "                    df['_'.join(list(map(str, [col_base, col, 'sqrt_log', func])))] = np.log1p(np.sqrt(df.groupby(col_base)[col].transform(func)))\n",
    "                    features['num'] += ['_'.join(list(map(str, [col_base, col, 'sqrt_log', func])))]\n",
    "                if option['log_sqrt']:\n",
    "                    df['_'.join(list(map(str, [col_base, col, 'log_sqrt', func])))] = np.sqrt(np.log1p(df.groupby(col_base)[col].transform(func)))\n",
    "                    features['num'] += ['_'.join(list(map(str, [col_base, col, 'log_sqrt', func])))]\n",
    "                if option['pow']:\n",
    "                    df['_'.join(list(map(str, [col_base, col, 'pow', func])))] = pow(df.groupby(col_base)[col].transform(func), 2)\n",
    "                    features['num'] += ['_'.join(list(map(str, [col_base, col, 'pow', func])))]\n",
    "                if option['pow_log']:\n",
    "                    df['_'.join(list(map(str, [col_base, col, 'pow_log', func])))] = 2*np.log1p(df.groupby(col_base)[col].transform(func))\n",
    "                    features['num'] += ['_'.join(list(map(str, [col_base, col, 'pow_log', func])))]\n",
    "                if option['log_pow']:\n",
    "                    df['_'.join(list(map(str, [col_base, col, 'log_pow', func])))] = pow(np.log1p(df.groupby(col_base)[col].transform(func)), 2)\n",
    "                    features['num'] += ['_'.join(list(map(str, [col_base, col, 'log_pow', func])))]\n",
    "\n",
    "    return df, features\n",
    "\n",
    "def create_features3(df, features, cols_transform, option):\n",
    "    for col in cols_transform:\n",
    "        if option['log']:\n",
    "            df['_'.join(list(map(str, ['log', col])))] = np.log1p(df[col])\n",
    "            features['num'] += ['_'.join(list(map(str, ['log', col])))]\n",
    "\n",
    "        if option['sqrt']:\n",
    "            df['_'.join(list(map(str, ['sqrt', col])))] = np.sqrt(df[col])\n",
    "            features['num'] += ['_'.join(list(map(str, ['sqrt', col])))]\n",
    "\n",
    "        if option['sqrt_log']:\n",
    "            df['_'.join(list(map(str, ['sqrt', 'log', col])))] = np.log1p(np.sqrt(df[col]))\n",
    "            features['num'] += ['_'.join(list(map(str, ['sqrt', 'log', col])))]\n",
    "        \n",
    "        if option['log_sqrt']:\n",
    "            df['_'.join(list(map(str, ['log', 'sqrt', col])))] = np.sqrt(np.log1p(df[col]))\n",
    "            features['num'] += ['_'.join(list(map(str, ['log', 'sqrt', col])))]\n",
    "        \n",
    "        if option['pow']:\n",
    "            df['_'.join(list(map(str, ['pow', col])))] = pow(df[col], 2)\n",
    "            features['num'] += ['_'.join(list(map(str, ['pow', col])))]\n",
    "        \n",
    "        if option['pow_log']:\n",
    "            df['_'.join(list(map(str, ['pow', 'log', col])))] = np.log1p(pow(df[col], 2))\n",
    "            features['num'] += ['_'.join(list(map(str, ['pow', 'log', col])))]\n",
    "        \n",
    "        if option['log_pow']:\n",
    "            df['_'.join(list(map(str, ['log', 'pow', col])))] = pow(np.log1p(df[col]), 2)\n",
    "            features['num'] += ['_'.join(list(map(str, ['log', 'pow', col])))]\n",
    "\n",
    "    return df, features\n",
    "\n",
    "def create_features4(df, features, option):\n",
    "    tokenizer = Tokenizer()\n",
    "\n",
    "    feats_increased = []\n",
    "    \n",
    "    title_words = [\"video\", \"official\", \"music\", \"公式\"]\n",
    "    tags_words = [\"music\", \"video\", \"official\", \"song\", \"remastered\", \"vevo\", \"lyric\", \"rock\", \"you\", \"pop\", \"live\", \"queen\"]\n",
    "    desc_words= [\"http\", \"www\", \"smarturl\", \"super\", \"simple\", \"video\", \"music\", \"facebook\", \"youtube\", \"twitter\", \"official\", \"instagram\"]\n",
    "    \n",
    "    for j, (title_sentence, tags_sentence, desc_sentence) in enumerate(tqdm(zip(df['title'].str.lower(), df['tags'].str.lower(), df['description'].str.lower()))):\n",
    "        if option['title']:\n",
    "            title_text = \" \".join(tokenizer.tokenize(title_sentence, wakati=True))\n",
    "            for word in title_words:\n",
    "                if (word in title_text) | (f\"{word}s\" in title_text):\n",
    "                    df.loc[df.index==j, f'title_{word}'] = 1\n",
    "                    if not f'title_{word}' in feats_increased:\n",
    "                        feats_increased += [f'title_{word}']\n",
    "                \n",
    "        if option['tags']:\n",
    "            tags_text = \" \".join(tokenizer.tokenize(tags_sentence, wakati=True))\n",
    "            for word in tags_words:\n",
    "                if (word in tags_text) | (f\"{word}s\" in tags_text):\n",
    "                    df.loc[df.index==j, f'tags_{word}'] = 1\n",
    "                    if not f'tags_{word}' in feats_increased:\n",
    "                        feats_increased += [f'tags_{word}']\n",
    "\n",
    "        if option['description']:\n",
    "            desc_text = \" \".join(tokenizer.tokenize(desc_sentence, wakati=True))\n",
    "            for word in desc_words:\n",
    "                if (word in desc_text) | (f\"{word}s\" in desc_text):\n",
    "                    df.loc[df.index==j, f'desc_{word}'] = 1\n",
    "                    if not f'desc_{word}' in feats_increased:\n",
    "                        feats_increased += [f'desc_{word}']\n",
    "\n",
    "    features['ohe'] += feats_increased\n",
    "    feats_increased_dict = {k: 0 for k in feats_increased}\n",
    "    df.fillna(feats_increased_dict, inplace=True)\n",
    "    df[feats_increased] = df[feats_increased].astype('int')\n",
    "        \n",
    "    return df, features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ラベルエンコーディング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encoder(df, cols_to_encode=[]):\n",
    "    lbl_enc_columns = cols_to_encode #cat_features + date_features + ohe_features\n",
    "\n",
    "    # Transforming all the labels of all variables\n",
    "    label_encoders = [LabelEncoder() for _ in range(len(lbl_enc_columns))]\n",
    "\n",
    "    for col, column in enumerate(lbl_enc_columns):\n",
    "        unique_values = pd.Series(df[column].unique())\n",
    "        unique_values = unique_values[unique_values.notnull()]\n",
    "        label_encoders[col].fit(unique_values)\n",
    "        df.loc[df[column].notnull(), column] = label_encoders[col].transform(df.loc[df[column].notnull(), column])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 標準化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardization(df, fnc_name='Standard', cols_to_std=[]):\n",
    "    fnc_scaler = {'Standard': StandardScaler(copy=True, with_mean=True, with_std=False),\n",
    "                  'MinMax': MinMaxScaler(feature_range=(-1, 1), copy=True),\n",
    "                  'MaxAbs': MaxAbsScaler(copy=True),\n",
    "                  'Normalize': Normalizer(norm='max'),\n",
    "                  'Robust': RobustScaler(with_centering=True, with_scaling=True, quantile_range=(25.0, 75.0), copy=True),\n",
    "                  'Quantile': QuantileTransformer(n_quantiles=1000, output_distribution='normal', ignore_implicit_zeros=False, \\\n",
    "                                                  subsample=100000, random_state=None, copy=True),\n",
    "                  #'box-cox': PowerTransformer(method='box-cox'),\n",
    "                  'yeo': PowerTransformer(method='yeo-johnson', standardize=True, copy=True)\n",
    "                 }\n",
    "    scaler = fnc_scaler[fnc_name]\n",
    "    df[cols_to_std] = scaler.fit_transform(df[cols_to_std])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# featuresの削除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_features(df, features, features_to_drop):\n",
    "    cols_to_drop = features_to_drop['num']+features_to_drop['cat']+features_to_drop['date']+features_to_drop['ohe']\n",
    "    df.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "    for col in features_to_drop['num']:\n",
    "        features['num'].remove(col)\n",
    "\n",
    "    for col in features_to_drop['cat']:\n",
    "        features['cat'].remove(col)\n",
    "\n",
    "    for col in features_to_drop['date']:\n",
    "        features['date'].remove(col)\n",
    "\n",
    "    for col in features_to_drop['ohe']:\n",
    "        features['ohe'].remove(col)\n",
    "\n",
    "    return df, features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学習、予測の共通処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIFF_THRESHOLD = 5\n",
    "################################################################################\n",
    "# METRICS\n",
    "################################################################################\n",
    "def rmsle(y, pred_y):\n",
    "    return mean_squared_error(y, pred_y, squared=False)\n",
    "    \n",
    "################################################################################\n",
    "# CROSS-VALIDATION\n",
    "################################################################################\n",
    "def print_cv_scores(label, cv_scores):\n",
    "    print(\"*\"*40)\n",
    "    print(f\"type(cv_scores): {type(cv_scores)}\")\n",
    "    print(f\"{label} cv scores : {cv_scores}\")\n",
    "    print(f\"{label} cv mean score : {np.mean(cv_scores)}\")\n",
    "    print(f\"{label} cv std score : {np.std(cv_scores)}\")\n",
    "    \n",
    "def run_cv_model(train, test, target, target_skf, encoding, model_fn, params={}, \n",
    "                 eval_fn=None, label='model', cv=5,  repeats=5, seed=43):\n",
    "\n",
    "    if repeats==1:\n",
    "        if target_skf is None:\n",
    "            kf = KFold(n_splits=cv, shuffle=True, random_state=seed)\n",
    "            target_y = target\n",
    "        else:\n",
    "            kf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=seed)\n",
    "            target_y = target_skf\n",
    "        divide_counts = cv\n",
    "    else:\n",
    "        if target_skf is None:\n",
    "            kf = RepeatedKFold(n_splits=cv,n_repeats=repeats, random_state=seed)\n",
    "            target_y = target\n",
    "        else:\n",
    "            kf = RepeatedStratifiedKFold(n_splits=cv, n_repeats=repeats, random_state=seed)\n",
    "            target_y = target_skf\n",
    "        divide_counts = kf.get_n_splits()\n",
    "        \n",
    "    cv_scores = []\n",
    "    pred_full_test = 0\n",
    "    pred_train = np.zeros((train.shape[0]))\n",
    "\n",
    "#    for fold_id, (train_idx, val_idx) in enumerate(kf.split(train, target)):\n",
    "    for fold_id, (train_idx, val_idx) in enumerate(kf.split(train, target_y)):\n",
    "        print(\"*\"*40)\n",
    "        print(f\"Started {label} fold:{fold_id+1} / {divide_counts}\")\n",
    "        #tr_X, val_X = train[train_idx], train[val_idx]\n",
    "        #tr_y, val_y = target[train_idx], target[val_idx]\n",
    "        tr_X, val_X = train.iloc[train_idx].copy(), train.iloc[val_idx].copy()\n",
    "        tr_y, val_y = target.iloc[train_idx], target.iloc[val_idx]\n",
    "        #print(Counter(tr_y), Counter(val_y))\n",
    "        \n",
    "        # TARGET ENCODING\n",
    "        if encoding:\n",
    "            for c in encoding:\n",
    "                # 学習データ全体で各カテゴリにおけるtargetの平均を計算\n",
    "                data_tmp = pd.DataFrame({c: tr_X[c], 'target': tr_y})\n",
    "                target_mean = data_tmp.groupby(c)['target'].mean()\n",
    "                # バリデーションデータのカテゴリを置換\n",
    "                val_X.loc[:, c] = val_X[c].map(target_mean)\n",
    "\n",
    "                # 学習データの変換後の値を格納する配列を準備\n",
    "                tmp = np.repeat(np.nan, tr_X.shape[0])\n",
    "                kf_encoding = KFold(n_splits=4, shuffle=True, random_state=seed)\n",
    "                for idx_1, idx_2 in kf_encoding.split(tr_X):\n",
    "                    # out-of-foldで各カテゴリにおける目的変数の平均を計算\n",
    "                    target_mean = data_tmp.iloc[idx_1].groupby(c)['target'].mean()\n",
    "                    # 変換後の値を一次配列に格納\n",
    "                    tmp[idx_2] = tr_X[c].iloc[idx_2].map(target_mean)\n",
    "\n",
    "\n",
    "                tr_X.loc[:, c] = tmp\n",
    "        # TARGET ENCODING\n",
    "        \n",
    "        params2 = params.copy() \n",
    "        model, pred_val_y, pred_test_y = model_fn(\n",
    "            tr_X, tr_y, val_X, val_y, test, params2)\n",
    "        \n",
    "        pred_full_test = pred_full_test + pred_test_y\n",
    "        pred_train[val_idx] = pred_val_y\n",
    "        if eval_fn is not None:\n",
    "            cv_score = eval_fn(val_y, pred_val_y)\n",
    "            cv_scores.append(cv_score)\n",
    "            print(f\"{label} cv score {fold_id+1}: {cv_score}\")\n",
    "            \n",
    "    \n",
    "    print_cv_scores(label, cv_scores)    \n",
    "    pred_full_test = pred_full_test / divide_counts\n",
    "    results = {'label': label,\n",
    "               'train': pred_train, \n",
    "               'test': pred_full_test, \n",
    "               'cv': cv_scores}\n",
    "    \n",
    "    RMSLE = mean_squared_error(target.values, results['train'], squared=False)\n",
    "    print(f\"Overall RMSLE={RMSLE}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "################################################################################\n",
    "# RESULTS\n",
    "################################################################################\n",
    "def submit_results(target, results, test_id, MODEL):\n",
    "    RMSLE = mean_squared_error(target.values, results['train'], squared=False)\n",
    "    #print(f\"Overall RMSLE={RMSLE}\")\n",
    "\n",
    "    # Make submission\n",
    "    print(\"Saving submission file\")\n",
    "    submission = pd.DataFrame({'id': test_id, 'y': np.expm1(results['test'])})\n",
    "    submission.to_csv(f\"./{out_dir}/submission_{MODEL}_CV{RMSLE:.6f}.csv\", index=False)\n",
    "        \n",
    "    return submission\n",
    "\n",
    "def check_results(y, results):\n",
    "    y_diff = np.abs(np.expm1(y) - np.expm1(results['train']))\n",
    "    y_log1p_diff = np.abs(y - results['train'])\n",
    "    display(y_diff[y_log1p_diff>DIFF_THRESHOLD].index.values)\n",
    "    display(train_data[y_log1p_diff>DIFF_THRESHOLD])\n",
    "    display(pd.concat([pd.DataFrame(y[y_log1p_diff>DIFF_THRESHOLD], columns=['y']), \\\n",
    "                       pd.DataFrame(results['train'][y_log1p_diff>DIFF_THRESHOLD], \\\n",
    "                                    index=y_diff[y_log1p_diff>DIFF_THRESHOLD].index.values, columns=['pred_train'])], axis=1))    \n",
    "\n",
    "    get_hist4(results['train'], \"pred_train\", \\\n",
    "              y, \"y\", \\\n",
    "              results['test'], \"pred_test\", \\\n",
    "              y_log1p_diff, \"diff\")\n",
    "    \n",
    "    display(pd.concat([pd.DataFrame(results['train'], columns=['pred_train']), \\\n",
    "                       pd.DataFrame(y, columns=['y']), \\\n",
    "                       y_log1p_diff.rename('y_log1p_diff')], \\\n",
    "                       axis=1).describe())\n",
    "    \n",
    "    display(pd.DataFrame(results['test'], columns=['pred_test']).describe())\n",
    "    \n",
    "    RMSLE = mean_squared_error(y, results['train'], squared=False)\n",
    "    display(f\"Overall RMSLE={RMSLE:.6f}\")\n",
    "    \n",
    "################################################################################\n",
    "# MODEL\n",
    "################################################################################\n",
    "def runLGB(train_X, train_y, val_X, val_y, test_X, params):\n",
    "    model = lgb.LGBMRegressor(**params)\n",
    "    model.fit(train_X, train_y, eval_set=(val_X, val_y), early_stopping_rounds=500, eval_metric='rmse', verbose=500)\n",
    "    pred_val_y = model.predict(val_X)\n",
    "    pred_test_y = model.predict(test_X)\n",
    "    return model, pred_val_y, pred_test_y\n",
    "\n",
    "def runXGB(train_X, train_y, val_X, val_y, test_X, params):\n",
    "    model = xgb.XGBRegressor(**params)\n",
    "    model.fit(train_X, train_y, eval_set=[[val_X, val_y]], early_stopping_rounds=500, eval_metric='rmse', verbose=500)\n",
    "    pred_val_y = model.predict(val_X)\n",
    "    pred_test_y = model.predict(test_X)\n",
    "    return model, pred_val_y, pred_test_y\n",
    "\n",
    "def runCAT(train_X, train_y, val_X, val_y, test_X, params):\n",
    "    model = CatBoostRegressor(**params)\n",
    "    model.fit(train_X, train_y, eval_set=(val_X, val_y), early_stopping_rounds=500, use_best_model=True, verbose=500)\n",
    "    pred_val_y = model.predict(val_X)\n",
    "    pred_test_y = model.predict(test_X)\n",
    "    return model, pred_val_y, pred_test_y\n",
    "\n",
    "################################################################################\n",
    "# MODEL PARAMETERS\n",
    "################################################################################\n",
    "lgb_params = {'boosting_type': 'gbdt', 'tree_learner': 'feature', #''serial' or feature' or 'data' or 'voting'\n",
    "              'num_leaves': 31, 'max_depth': -1,\n",
    "              'learning_rate': 5e-2, 'n_estimators': 20000, 'importance_type': 'gain',\n",
    "              'subsample_for_bin': 200000, 'objective': 'rmse', 'min_split_gain': 0.0, 'min_child_weight': 1e-3, 'min_child_samples': 20, \n",
    "              'bagging_freq': 0, 'bagging_fraction': 1.0, 'feature_fraction': 1.0,\n",
    "              'reg_alpha': 0.2, 'reg_lambda': 0.2,\n",
    "              'random_state': 43, 'data_random_seed': 1,\n",
    "              'n_jobs': -1, 'silent': False}\n",
    "\n",
    "xgb_params = {'base_score': 0.5, 'booster': 'gbtree', 'colsample_bylevel': 1, 'colsample_bynode': 1, 'colsample_bytree': 1, 'gamma': 0,\n",
    "              'learning_rate': 5e-2, 'n_estimators': 20000, 'importance_type': 'gain',\n",
    "              'max_delta_step': 0, 'max_depth': 6, 'min_child_weight': 0,\n",
    "              'objective': 'reg:squarederror', 'reg_alpha': 0.2, 'reg_lambda': 0.2, 'scale_pos_weight': 1,\n",
    "              'subsample': 0.9,\n",
    "              'silent': None, 'verbosity': 0,\n",
    "              'random_state': 43, 'seed': 43,\n",
    "              'tree_method': 'gpu_hist', 'gpu_id': 0}\n",
    "\n",
    "cat_params = {'iterations':20000, 'depth': 6, 'boosting_type': 'Ordered', #'Ordered', #'Plain',\n",
    "              'loss_function': 'RMSE', 'eval_metric': 'RMSE',\n",
    "              'learning_rate': 5e-2, 'leaf_estimation_method': 'Gradient', #'Newton', 'Exact'\n",
    "              'l2_leaf_reg': 1.0, 'random_strength': 1.0, 'bagging_temperature': 1.0, 'has_time': False,\n",
    "              'grow_policy': 'SymmetricTree', #'Depthwise', 'Lossguide'\n",
    "              'min_data_in_leaf': 1, 'max_leaves': 31,\n",
    "              'random_seed': 43,\n",
    "#              'one_hot_max_size': len(cat_features),\n",
    "              'task_type': 'GPU'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 学習/予測用データの準備 (#1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([train, test]).reset_index(drop=True)\n",
    "\n",
    "df.loc[df['categoryId']==43, 'categoryId'] = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_features(df, features):\n",
    "    print(\"-\"*40)\n",
    "    print(f\"実特徴量数: {len(df.columns)} / 計算上の特徴量数: {len(features['num'])+len(features['cat'])+len(features['date'])+len(features['ohe'])}\")\n",
    "    print(\"-\"*40)\n",
    "    print(f\"(内訳) num_features: {len(features['num'])}, cat_features: {len(features['cat'])}, date_features: {len(features['date'])}, ohe_features: {len(features['ohe'])}\")\n",
    "    print(\"-\"*40)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "seed_everything(seed=47)\n",
    "\n",
    "df = pd.concat([train, test]).reset_index(drop=True)\n",
    "\n",
    "df.loc[df['categoryId']==43, 'categoryId'] = 30\n",
    "\n",
    "# 特徴量の分類\n",
    "features = {'cat': ['video_id', 'title', 'channelId', 'channelTitle', 'categoryId', 'tags', 'thumbnail_link', 'description', 'comments_disabled', 'ratings_disabled'],\n",
    "            'date': ['publishedAt', 'collection_date'],\n",
    "            'num': ['likes', 'dislikes', 'comment_count'],\n",
    "            'ohe': []\n",
    "           }\n",
    "\n",
    "# ラベルエンコーディング\n",
    "print(\"ラベルエンコーディング\")\n",
    "df = label_encoder(df, cols_to_encode=['categoryId'])\n",
    "\n",
    "# 特徴量生成\n",
    "print(\"特徴量生成\")\n",
    "df, features = create_features(df, features)\n",
    "\n",
    "# ラベルエンコーディング\n",
    "print(\"ラベルエンコーディング\")\n",
    "df = label_encoder(df, cols_to_encode=features['cat']+features['date'])\n",
    "\n",
    "print(\"特徴量生成2 categoryId\")\n",
    "df, features = create_features2(df, features,\n",
    "                                cols_groupby=['categoryId'],\n",
    "                                cols_transform=['likes', 'dislikes', 'comment_count', 'channelId'],\n",
    "                                target_func=['max', 'min', 'mean'],\n",
    "                                option={\"log\": False,\\\n",
    "                                        \"sqrt\": False, \"sqrt_log\": False, \"log_sqrt\": False,\\\n",
    "                                        \"pow\": False, \"pow_log\": False, \"log_pow\": False})\n",
    "print(\"特徴量生成2 comments_disabled\")\n",
    "df, features = create_features2(df, features,\n",
    "                                cols_groupby=['comments_disabled'],\n",
    "                                cols_transform=['likes'],\n",
    "                                target_func=['max', 'min', 'mean'],\n",
    "                                option={\"log\": False,\\\n",
    "                                        \"sqrt\": False, \"sqrt_log\": False, \"log_sqrt\": False,\\\n",
    "                                        \"pow\": False, \"pow_log\": False, \"log_pow\": False})\n",
    "print(\"特徴量生成2 channelId\")\n",
    "df, features = create_features2(df, features,\n",
    "                                cols_groupby=['channelId'],\n",
    "                                cols_transform=list(set(features['num'] + features['cat'] + features['ohe'])),\n",
    "                                target_func=['max', 'min', 'mean'],\n",
    "                                option={\"log\": False, \\\n",
    "                                        \"sqrt\": False, \"sqrt_log\": False, \"log_sqrt\": False,\\\n",
    "                                        \"pow\": False, \"pow_log\": False, \"log_pow\": False})\n",
    "print(\"特徴量生成3\")\n",
    "df, features = create_features3(df, features,\n",
    "                                cols_transform=[c for c in df.columns if (('likes' in c)|('dislikes' in  c)|('comment_count' in c))&\n",
    "                                               (('diff_likes' not in c)&('diff_dislikes' not in c)&('diff_comments' not in c))],\n",
    "                                option={\"log\": False,\\\n",
    "                                        \"sqrt\": True, \"sqrt_log\": False, \"log_sqrt\": False,\\\n",
    "                                        \"pow\": True, \"pow_log\": False, \"log_pow\": True})\n",
    "\n",
    "# 特徴量数のチェック\n",
    "print(\"特徴量のチェック\")\n",
    "check_features(df, features)\n",
    "\n",
    "# df/featuresの退避\n",
    "df_save = df.copy()\n",
    "features_save = {\"cat\":  features[\"cat\"].copy(),\n",
    "                 \"date\": features[\"date\"].copy(),\n",
    "                 \"num\":  features[\"num\"].copy(),\n",
    "                 \"ohe\":  features[\"ohe\"].copy()}\n",
    "\n",
    "# 削除する特徴量のリスト\n",
    "feats_to_drop_list = []\n",
    "if DATASET >= 1:\n",
    "    feats_to_drop_list += [['len_description', 'len_title',\n",
    "                           'channelId_channelTitle_max',  'channelId_channelTitle_min', 'channelId_channelTitle_mean',\n",
    "                           'channelId_len_description_max',  'channelId_len_description_min', 'channelId_len_description_mean',\n",
    "                           'channelId_len_title_max', 'channelId_len_title_min', 'channelId_len_title_mean',\n",
    "                           'channelId_tags_max', 'channelId_tags_min', 'channelId_tags_mean',\n",
    "                           'inclEn_title', 'inclEn_description']]\n",
    "if DATASET >= 2:\n",
    "    feats_to_drop_list += [['len_description', 'len_title',\n",
    "                           #'channelId_channelTitle_max',  'channelId_channelTitle_min', 'channelId_channelTitle_mean',\n",
    "                           'channelId_len_description_max',  'channelId_len_description_min', 'channelId_len_description_mean',\n",
    "                           'channelId_len_title_max', 'channelId_len_title_min', 'channelId_len_title_mean',\n",
    "                           'channelId_tags_max', 'channelId_tags_min', 'channelId_tags_mean',\n",
    "                           'inclEn_title', 'inclEn_description']]\n",
    "if DATASET >= 3:\n",
    "    feats_to_drop_list += [['len_description', 'len_title',\n",
    "                           'channelId_channelTitle_max',  'channelId_channelTitle_min', 'channelId_channelTitle_mean',\n",
    "                           #'channelId_len_description_max',  'channelId_len_description_min', 'channelId_len_description_mean',\n",
    "                           'channelId_len_title_max', 'channelId_len_title_min', 'channelId_len_title_mean',\n",
    "                           'channelId_tags_max', 'channelId_tags_min', 'channelId_tags_mean',\n",
    "                           'inclEn_title', 'inclEn_description']]\n",
    "if DATASET >=4:\n",
    "    feats_to_drop_list += [['len_description', 'len_title',\n",
    "                           'channelId_channelTitle_max',  'channelId_channelTitle_min', 'channelId_channelTitle_mean',\n",
    "                           'channelId_len_description_max',  'channelId_len_description_min', 'channelId_len_description_mean',\n",
    "                           #'channelId_len_title_max', 'channelId_len_title_min', 'channelId_len_title_mean',\n",
    "                           'channelId_tags_max', 'channelId_tags_min', 'channelId_tags_mean',\n",
    "                           'inclEn_title', 'inclEn_description']]\n",
    "if DATASET >=5:\n",
    "    feats_to_drop_list += [['len_description', 'len_title',\n",
    "                           'channelId_channelTitle_max',  'channelId_channelTitle_min', 'channelId_channelTitle_mean',\n",
    "                           'channelId_len_description_max',  'channelId_len_description_min', 'channelId_len_description_mean',\n",
    "                           'channelId_len_title_max', 'channelId_len_title_min', 'channelId_len_title_mean',\n",
    "                           #'channelId_tags_max', 'channelId_tags_min', 'channelId_tags_mean',\n",
    "                           'inclEn_title', 'inclEn_description']]\n",
    "\n",
    "# 特徴量バリエーションを含むtrain/testデータのリスト\n",
    "X_train_list = []\n",
    "X_test_list  = []\n",
    "\n",
    "for feats_to_drop_num in tqdm(feats_to_drop_list):\n",
    "    # df/featuresの復帰\n",
    "    df = df_save.copy()\n",
    "    features[\"cat\"]  = features_save[\"cat\"].copy()\n",
    "    features[\"date\"] = features_save[\"date\"].copy()\n",
    "    features[\"ohe\"]  = features_save[\"ohe\"].copy()\n",
    "    features[\"num\"]  = features_save[\"num\"].copy()\n",
    "\n",
    "    # 特徴量削除\n",
    "    print(\"特徴量削除\")\n",
    "    feats_to_drop = {\"cat\": ['video_id', 'channelId', 'title', 'channelTitle', 'tags', 'thumbnail_link', 'description'],\n",
    "                     \"date\": ['publishedAt', 'collection_date'],\n",
    "                     \"ohe\": [],\n",
    "                     \"num\": []}\n",
    "\n",
    "    #-----------------------------------------------------------------------------------------------------------\n",
    "    # CV: 0.725691, LB: 0.723\n",
    "    #-----------------------------------------------------------------------------------------------------------\n",
    "    feats_to_drop[\"cat\"] += ['publishedAt_second', 'publishedAt_minute', 'publishedAt_hour', 'publishedAt_day', 'publishedAt_dayofweek']\n",
    "    feats_to_drop[\"num\"] += feats_to_drop_num\n",
    "\n",
    "    #-----------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    df, features = drop_features(df, features, feats_to_drop)\n",
    "\n",
    "    # nunique()==1の特徴量を削除\n",
    "    for col in df.loc[:, df.nunique()==1].columns:\n",
    "        features[\"num\"].remove(col)\n",
    "    df.drop(df.loc[:, df.nunique()==1].columns, axis=1, inplace=True)\n",
    "\n",
    "    # データ型変換\n",
    "    print(\"データ型変換\")\n",
    "    #df[features[\"num\"]] = df[features[\"num\"]].astype('float32')\n",
    "    df = df.astype('float')\n",
    "    df[features[\"cat\"]] = df[features[\"cat\"]].astype('int')\n",
    "    df[features[\"ohe\"]] = df[features[\"ohe\"]].astype('int')\n",
    "\n",
    "    # 標準化\n",
    "    print(\"標準化\")\n",
    "    #df = standardization(df, fnc_name=\"Standard\", cols_to_std=features[\"num\"])\n",
    "    df = standardization(df, fnc_name=\"MinMax\", cols_to_std=features[\"num\"])\n",
    "\n",
    "    # 特徴量数のチェック\n",
    "    #print(\"特徴量のチェック\")\n",
    "    check_features(df, features)\n",
    "\n",
    "    # 学習、予測データ分割\n",
    "    X_train = df.iloc[:y.shape[0], :].reset_index(drop=True).copy()\n",
    "    X_test  = df.iloc[y.shape[0]:, :].reset_index(drop=True).copy()\n",
    "\n",
    "    # 欠損値、無限大/無限小値有無のチェック\n",
    "    print(f\"学習データ中の欠損値数: {X_train.isnull().sum().sum()}\")\n",
    "    print(f\"学習データ中の無限値数: {np.count_nonzero(np.isinf(X_train))}\")\n",
    "    print(f\"予測データ中の欠損値数: {X_test.isnull().sum().sum()}\")\n",
    "    print(f\"予測データ中の無現値数: {np.count_nonzero(np.isinf(X_test))}\")\n",
    "\n",
    "    X_train_list.append(X_train)\n",
    "    X_test_list.append(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for X_train, X_test in tqdm(zip(X_train_list, X_test_list)):\n",
    "    display(description(X_train))\n",
    "    display(description(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3x Single Model\n",
    "### LGBMRegressor, XGBRegressor, CatBoostRegressor\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def output_results(results, test_id, seed, N_FOLDS, dataid, out_dir):\n",
    "    submission = submit_results(y, results, test_id, f\"{results['label']}_SEED{seed}_FOLDS{N_FOLDS}_0624\")\n",
    "    with open(f\"./{out_dir}/{results['label']}_train_SEED{seed}_FOLDS{N_FOLDS}_0624.pickle\", 'wb') as f:\n",
    "        pickle.dump(results['train'], f)\n",
    "    with open(f\"./{out_dir}/{results['label']}_test_SEED{seed}_FOLDS{N_FOLDS}_0624.pickle\", 'wb') as f:\n",
    "        pickle.dump(results['test'], f)\n",
    "    return submission\n",
    "\n",
    "if LGB:\n",
    "    lgb_results_l = []\n",
    "if XGB:\n",
    "    xgb_results_l = []\n",
    "if CAT:\n",
    "    cat_results_l = []\n",
    "    \n",
    "for dataid, (X_train, X_test) in enumerate(tqdm(zip(X_train_list, X_test_list))):\n",
    "    X_train_cmntf = X_train[(X_train['comments_disabled']==False)]\n",
    "    y_cmntf       = y[(X_train['comments_disabled']==False)]\n",
    "    y_bin_cmntf   = y_bin[(X_train['comments_disabled']==False)]\n",
    "    X_test_cmntf  = X_test[(X_test['comments_disabled']==False)]\n",
    "\n",
    "    X_train_rest = X_train[(X_train['comments_disabled']==True)]\n",
    "    X_test_rest  = X_test[(X_test['comments_disabled']==True)]\n",
    "    y_rest       = y[(X_train['comments_disabled']==True)]\n",
    "\n",
    "    seed_l = [47]\n",
    "    N_FOLDS, encoding = 8, []\n",
    "    \n",
    "    for k, seed in enumerate(tqdm(seed_l)):\n",
    "        print(\"*\"*80)\n",
    "        print(f\"seed = {seed}\")\n",
    "        print(\"*\"*80)\n",
    "\n",
    "        if LGB:\n",
    "            #-----------------------------------------------\n",
    "            # LGB MODEL 1\n",
    "            #-----------------------------------------------\n",
    "            # df[df['comments_disabled']==False]の学習、予測\n",
    "            lgb_params = {'boosting_type': 'gbdt',\n",
    "                          'objective' : 'regression',\n",
    "                          'metric' : 'rmse',  \n",
    "                          'tree_learner': 'feature', #''serial' or feature' or 'data' or 'voting'\n",
    "                          'max_depth': 16,\n",
    "                          'min_child_samples': 10, \n",
    "                          'min_split_gain': 0.01,\n",
    "                          'min_child_weight': 1e-2,\n",
    "                          'reg_alpha': 0.1,\n",
    "                          'reg_lambda': 1,\n",
    "                          'num_leaves': 35,\n",
    "                          'max_bin': 300,\n",
    "                          'learning_rate': 1e-2,\n",
    "                          'bagging_fraction': 0.9,\n",
    "                          'bagging_freq': 1,\n",
    "                          'bagging_seed': 4590,\n",
    "                          'feature_fraction': 0.85,\n",
    "                          'n_estimators': 50000,\n",
    "                          'importance_type': 'gain',\n",
    "                          'subsample_for_bin': 200000,\n",
    "                          'random_state': seed,\n",
    "                          'data_random_seed': seed,\n",
    "                          'n_jobs': -1,\n",
    "                          'silent': False}\n",
    "            if SKF:\n",
    "                lgb_results_cmntf = run_cv_model(train=X_train_cmntf, test=X_test_cmntf, target=y_cmntf, target_skf=y_bin_cmntf, encoding=[],\n",
    "                                                 model_fn=runLGB, params=lgb_params, eval_fn=rmsle, label='LGBMRegressor',\n",
    "                                                 cv=N_FOLDS, repeats=1, seed=seed)\n",
    "                # df全体の学習、予測\n",
    "                lgb_results = run_cv_model(train=X_train, test=X_test, target=y, target_skf=y_bin, encoding=[],\n",
    "                                           model_fn=runLGB, params=lgb_params, eval_fn=rmsle, label='LGBMRegressor',\n",
    "                                           cv=N_FOLDS, repeats=1, seed=seed)\n",
    "            else:\n",
    "                lgb_results_cmntf = run_cv_model(train=X_train_cmntf, test=X_test_cmntf, target=y_cmntf, target_skf=None, encoding=[],\n",
    "                                                 model_fn=runLGB, params=lgb_params, eval_fn=rmsle, label='LGBMRegressor',\n",
    "                                                 cv=N_FOLDS, repeats=1, seed=seed)\n",
    "                # df全体の学習、予測\n",
    "                lgb_results = run_cv_model(train=X_train, test=X_test, target=y, target_skf=None, encoding=[],\n",
    "                                           model_fn=runLGB, params=lgb_params, eval_fn=rmsle, label='LGBMRegressor',\n",
    "                                           cv=N_FOLDS, repeats=1, seed=seed)\n",
    "                \n",
    "            lgb_results_merged = {\n",
    "                \"label\": \"LGBMRegressor\",\n",
    "                \"train\": pd.concat([pd.Series(lgb_results_cmntf['train'], index=X_train_cmntf.index),\n",
    "                                    pd.Series(lgb_results['train'][X_train_rest.index], index=X_train_rest.index)]).sort_index(),\n",
    "                \"test\":  pd.concat([pd.Series(lgb_results_cmntf['test'], index=X_test_cmntf.index),\n",
    "                                    pd.Series(lgb_results['test'][X_test_rest.index], index=X_test_rest.index)]).sort_index()}\n",
    "\n",
    "            print(\"-\"*80)\n",
    "            RMSLE_base    = mean_squared_error(y, lgb_results['train'], squared=False)\n",
    "            print(f\"Base    RMSLE={RMSLE_base}\")\n",
    "            RMSLE_base_false = mean_squared_error(y_cmntf, lgb_results['train'][X_train_cmntf.index], squared=False)\n",
    "            print(f\"f in B  RMSLE={RMSLE_base_false}\")\n",
    "            RMSLE_false   = mean_squared_error(y_cmntf, lgb_results_cmntf['train'], squared=False)\n",
    "            print(f\"False   RMSLE={RMSLE_false}\")\n",
    "            RMSLE_merged  = mean_squared_error(y, lgb_results_merged['train'], squared=False)\n",
    "            print(f\"merged  RMSLE={RMSLE_merged}\")\n",
    "            print(\"-\"*80)\n",
    "\n",
    "            if RMSLE_base_false > RMSLE_false:\n",
    "                results = lgb_results_merged\n",
    "            else:\n",
    "                results = lgb_results\n",
    "\n",
    "            y_pred = pd.Series(results['train']).copy()\n",
    "            y_pred.where((y_pred<14)&(X_train.comments_disabled==True), y_pred*0.99965, inplace=True)\n",
    "            y_pred.where((y_pred>14)&(X_train.comments_disabled==True), y_pred*1.0001, inplace=True)\n",
    "            y_pred.where((y_pred<14)&(X_train.ratings_disabled==True), y_pred*0.99965, inplace=True)\n",
    "            y_pred.where((y_pred>14)&(X_train.ratings_disabled==True), y_pred*1.0001, inplace=True)\n",
    "\n",
    "            RMSLE_merged  = mean_squared_error(y, y_pred, squared=False)\n",
    "            print(f\"merged2 RMSLE={RMSLE_merged}\")\n",
    "            print(\"-\"*80)\n",
    "\n",
    "            sunmission = output_results(results, test_id, seed, N_FOLDS, dataid, out_dir)\n",
    "\n",
    "            #-----------------------------------------------\n",
    "            # LGB MODEL 2\n",
    "            #-----------------------------------------------\n",
    "            # df[df['comments_disabled']==False]の学習、予測\n",
    "            lgb_params = {'boosting_type': 'gbdt',\n",
    "                          'objective' : 'regression',\n",
    "                          'metric' : 'rmse',  \n",
    "                          'tree_learner': 'feature', #''serial' or feature' or 'data' or 'voting'\n",
    "                          'max_depth': -1,\n",
    "                          'min_child_samples': 10, \n",
    "                          'min_split_gain': 0.01,\n",
    "                          'min_child_weight': 1e-2,\n",
    "                          'reg_alpha': 0.1,\n",
    "                          'reg_lambda': 1,\n",
    "                          'num_leaves': 35,\n",
    "                          'max_bin': 300,\n",
    "                          'learning_rate': 2e-2,\n",
    "                          'bagging_fraction': 0.9,\n",
    "                          'bagging_freq': 1,\n",
    "                          'bagging_seed': 4590,\n",
    "                          'feature_fraction': 0.85,\n",
    "                          'n_estimators': 50000,\n",
    "                          'importance_type': 'gain',\n",
    "                          'subsample_for_bin': 200000,\n",
    "                          'random_state': seed,\n",
    "                          'data_random_seed': seed,\n",
    "                          'n_jobs': -1,\n",
    "                          'silent': False}\n",
    "            if SKF:\n",
    "                lgb_results_cmntf = run_cv_model(train=X_train_cmntf, test=X_test_cmntf, target=y_cmntf, target_skf=y_bin_cmntf, encoding=[],\n",
    "                                                 model_fn=runLGB, params=lgb_params, eval_fn=rmsle, label='LGBMRegressor2',\n",
    "                                                 cv=N_FOLDS, repeats=1, seed=seed)\n",
    "                # df全体の学習、予測\n",
    "                lgb_results = run_cv_model(train=X_train, test=X_test, target=y, target_skf=y_bin, encoding=[],\n",
    "                                           model_fn=runLGB, params=lgb_params, eval_fn=rmsle, label='LGBMRegressor2',\n",
    "                                           cv=N_FOLDS, repeats=1, seed=seed)\n",
    "            else:\n",
    "                lgb_results_cmntf = run_cv_model(train=X_train_cmntf, test=X_test_cmntf, target=y_cmntf, target_skf=None, encoding=[],\n",
    "                                                 model_fn=runLGB, params=lgb_params, eval_fn=rmsle, label='LGBMRegressor2',\n",
    "                                                 cv=N_FOLDS, repeats=1, seed=seed)\n",
    "                # df全体の学習、予測\n",
    "                lgb_results = run_cv_model(train=X_train, test=X_test, target=y, target_skf=None, encoding=[],\n",
    "                                           model_fn=runLGB, params=lgb_params, eval_fn=rmsle, label='LGBMRegressor2',\n",
    "                                           cv=N_FOLDS, repeats=1, seed=seed)\n",
    "                \n",
    "            lgb_results_merged = {\n",
    "                \"label\": \"LGBMRegressor2\",\n",
    "                \"train\": pd.concat([pd.Series(lgb_results_cmntf['train'], index=X_train_cmntf.index),\n",
    "                                    pd.Series(lgb_results['train'][X_train_rest.index], index=X_train_rest.index)]).sort_index(),\n",
    "                \"test\":  pd.concat([pd.Series(lgb_results_cmntf['test'], index=X_test_cmntf.index),\n",
    "                                    pd.Series(lgb_results['test'][X_test_rest.index], index=X_test_rest.index)]).sort_index()}\n",
    "\n",
    "            print(\"-\"*80)\n",
    "            RMSLE_base    = mean_squared_error(y, lgb_results['train'], squared=False)\n",
    "            print(f\"Base    RMSLE={RMSLE_base}\")\n",
    "            RMSLE_base_false = mean_squared_error(y_cmntf, lgb_results['train'][X_train_cmntf.index], squared=False)\n",
    "            print(f\"f in B  RMSLE={RMSLE_base_false}\")\n",
    "            RMSLE_false   = mean_squared_error(y_cmntf, lgb_results_cmntf['train'], squared=False)\n",
    "            print(f\"False   RMSLE={RMSLE_false}\")\n",
    "            RMSLE_merged  = mean_squared_error(y, lgb_results_merged['train'], squared=False)\n",
    "            print(f\"merged  RMSLE={RMSLE_merged}\")\n",
    "            print(\"-\"*80)\n",
    "\n",
    "            if RMSLE_base_false > RMSLE_false:\n",
    "                results = lgb_results_merged\n",
    "            else:\n",
    "                results = lgb_results\n",
    "            \n",
    "            y_pred = pd.Series(results['train']).copy()\n",
    "            y_pred.where((y_pred<14)&(X_train.comments_disabled==True), y_pred*0.99965, inplace=True)\n",
    "            y_pred.where((y_pred>14)&(X_train.comments_disabled==True), y_pred*1.0001, inplace=True)\n",
    "            y_pred.where((y_pred<14)&(X_train.ratings_disabled==True), y_pred*0.99965, inplace=True)\n",
    "            y_pred.where((y_pred>14)&(X_train.ratings_disabled==True), y_pred*1.0001, inplace=True)\n",
    "\n",
    "            RMSLE_merged  = mean_squared_error(y, y_pred, squared=False)\n",
    "            print(f\"merged2 RMSLE={RMSLE_merged}\")\n",
    "            print(\"-\"*80)\n",
    "\n",
    "            sunmission = output_results(results, test_id, seed, N_FOLDS, dataid, out_dir)\n",
    "            \n",
    "        if XGB:\n",
    "            #-----------------------------------------------\n",
    "            # XGB MODEL 1\n",
    "            #-----------------------------------------------\n",
    "            # df[df['comments_disabled']==False]の学習、予測\n",
    "            xgb_params = {'base_score': 0.5,\n",
    "                          'booster': 'gbtree',\n",
    "                          'objective': 'reg:squarederror',\n",
    "                          'colsample_bylevel': 0.6,\n",
    "                          'colsample_bynode': 0.6,\n",
    "                          'colsample_bytree': 0.6,\n",
    "                          'gamma': 0,\n",
    "                          'learning_rate': 1e-2,\n",
    "                          'n_estimators': 50000,\n",
    "                          'importance_type': 'gain',\n",
    "                          'max_delta_step': 0,\n",
    "                          'max_depth': 6,\n",
    "                          'min_child_weight': 0,\n",
    "                          'reg_alpha': 0.1,\n",
    "                          'reg_lambda': 1,\n",
    "                          'scale_pos_weight': 1,\n",
    "                          'subsample': 0.9,\n",
    "                          'silent': None,\n",
    "                          'verbosity': 0,\n",
    "                          'random_state': seed,\n",
    "                          'seed': seed,\n",
    "                          'tree_method': 'gpu_hist',\n",
    "                          'gpu_id': 0}\n",
    "            if SKF:\n",
    "                xgb_results_cmntf = run_cv_model(train=X_train_cmntf, test=X_test_cmntf, target=y_cmntf, target_skf=y_bin_cmntf, encoding=[],\n",
    "                                                 model_fn=runXGB, params=xgb_params, eval_fn=rmsle, label='XGBRegressor',\n",
    "                                                 cv=N_FOLDS, repeats=1, seed=seed)\n",
    "                # df全体の学習、予測\n",
    "                xgb_results = run_cv_model(train=X_train, test=X_test, target=y, target_skf=y_bin, encoding=[],\n",
    "                                           model_fn=runXGB, params=xgb_params, eval_fn=rmsle, label='XGBRegressor',\n",
    "                                           cv=N_FOLDS, repeats=1, seed=seed)\n",
    "            else:\n",
    "                xgb_results_cmntf = run_cv_model(train=X_train_cmntf, test=X_test_cmntf, target=y_cmntf, target_skf=None, encoding=[],\n",
    "                                                 model_fn=runXGB, params=xgb_params, eval_fn=rmsle, label='XGBRegressor',\n",
    "                                                 cv=N_FOLDS, repeats=1, seed=seed)\n",
    "                # df全体の学習、予測\n",
    "                xgb_results = run_cv_model(train=X_train, test=X_test, target=y, target_skf=None, encoding=[],\n",
    "                                           model_fn=runXGB, params=xgb_params, eval_fn=rmsle, label='XGBRegressor',\n",
    "                                           cv=N_FOLDS, repeats=1, seed=seed)\n",
    "                \n",
    "            xgb_results_merged = {\n",
    "                \"label\": \"XGBRegressor\",\n",
    "                \"train\": pd.concat([pd.Series(xgb_results_cmntf['train'], index=X_train_cmntf.index),\n",
    "                                    pd.Series(xgb_results['train'][X_train_rest.index], index=X_train_rest.index)]).sort_index(),\n",
    "                \"test\":  pd.concat([pd.Series(xgb_results_cmntf['test'], index=X_test_cmntf.index),\n",
    "                                    pd.Series(xgb_results['test'][X_test_rest.index], index=X_test_rest.index)]).sort_index()}\n",
    "            \n",
    "            print(\"-\"*80)\n",
    "            RMSLE_base    = mean_squared_error(y, xgb_results['train'], squared=False)\n",
    "            print(f\"Base    RMSLE={RMSLE_base}\")\n",
    "            RMSLE_base_false = mean_squared_error(y_cmntf, xgb_results['train'][X_train_cmntf.index], squared=False)\n",
    "            print(f\"f in B  RMSLE={RMSLE_base_false}\")\n",
    "            RMSLE_false   = mean_squared_error(y_cmntf, xgb_results_cmntf['train'], squared=False)\n",
    "            print(f\"False   RMSLE={RMSLE_false}\")\n",
    "            RMSLE_merged  = mean_squared_error(y, xgb_results_merged['train'], squared=False)\n",
    "            print(f\"merged  RMSLE={RMSLE_merged}\")\n",
    "            print(\"-\"*80)\n",
    "\n",
    "            if RMSLE_base_false > RMSLE_false:\n",
    "                results = xgb_results_merged\n",
    "            else:\n",
    "                results = xgb_results\n",
    "            \n",
    "            y_pred = pd.Series(results['train']).copy()\n",
    "            y_pred.where((y_pred<14)&(X_train.comments_disabled==True), y_pred*0.99965, inplace=True)\n",
    "            y_pred.where((y_pred>14)&(X_train.comments_disabled==True), y_pred*1.0001, inplace=True)\n",
    "            y_pred.where((y_pred<14)&(X_train.ratings_disabled==True), y_pred*0.99965, inplace=True)\n",
    "            y_pred.where((y_pred>14)&(X_train.ratings_disabled==True), y_pred*1.0001, inplace=True)\n",
    "\n",
    "            RMSLE_merged  = mean_squared_error(y, y_pred, squared=False)\n",
    "            print(f\"merged2 RMSLE={RMSLE_merged}\")\n",
    "            print(\"-\"*80)\n",
    "\n",
    "            sunmission = output_results(results, test_id, seed, N_FOLDS, dataid, out_dir)\n",
    "\n",
    "            #-----------------------------------------------\n",
    "            # XGB MODEL 2\n",
    "            #-----------------------------------------------\n",
    "            # df[df['comments_disabled']==False]の学習、予測\n",
    "            xgb_params = {'base_score': 0.5,\n",
    "                          'booster': 'gbtree',\n",
    "                          'objective': 'reg:squarederror',\n",
    "                          'colsample_bylevel': 0.6,\n",
    "                          'colsample_bynode': 0.6,\n",
    "                          'colsample_bytree': 0.6,\n",
    "                          'gamma': 0,\n",
    "                          'learning_rate': 1e-2,\n",
    "                          'n_estimators': 50000,\n",
    "                          'importance_type': 'gain',\n",
    "                          'max_delta_step': 0,\n",
    "                          'max_depth': 8,\n",
    "                          'min_child_weight': 0,\n",
    "                          'reg_alpha': 0.1,\n",
    "                          'reg_lambda': 1,\n",
    "                          'scale_pos_weight': 1,\n",
    "                          'subsample': 0.8,\n",
    "                          'silent': None,\n",
    "                          'verbosity': 0,\n",
    "                          'random_state': seed,\n",
    "                          'seed': seed,\n",
    "                          'tree_method': 'gpu_hist',\n",
    "                          'gpu_id': 0}\n",
    "            if SKF:\n",
    "                xgb_results_cmntf = run_cv_model(train=X_train_cmntf, test=X_test_cmntf, target=y_cmntf, target_skf=y_bin_cmntf, encoding=[],\n",
    "                                                 model_fn=runXGB, params=xgb_params, eval_fn=rmsle, label='XGBRegressor2',\n",
    "                                                 cv=N_FOLDS, repeats=1, seed=seed)\n",
    "                # df全体の学習、予測\n",
    "                xgb_results = run_cv_model(train=X_train, test=X_test, target=y, target_skf=y_bin, encoding=[],\n",
    "                                           model_fn=runXGB, params=xgb_params, eval_fn=rmsle, label='XGBRegressor2',\n",
    "                                           cv=N_FOLDS, repeats=1, seed=seed)\n",
    "            else:\n",
    "                xgb_results_cmntf = run_cv_model(train=X_train_cmntf, test=X_test_cmntf, target=y_cmntf, target_skf=None, encoding=[],\n",
    "                                                 model_fn=runXGB, params=xgb_params, eval_fn=rmsle, label='XGBRegressor2',\n",
    "                                                 cv=N_FOLDS, repeats=1, seed=seed)\n",
    "                # df全体の学習、予測\n",
    "                xgb_results = run_cv_model(train=X_train, test=X_test, target=y, target_skf=None, encoding=[],\n",
    "                                           model_fn=runXGB, params=xgb_params, eval_fn=rmsle, label='XGBRegressor2',\n",
    "                                           cv=N_FOLDS, repeats=1, seed=seed)\n",
    "                \n",
    "            xgb_results_merged = {\n",
    "                \"label\": \"XGBRegressor2\",\n",
    "                \"train\": pd.concat([pd.Series(xgb_results_cmntf['train'], index=X_train_cmntf.index),\n",
    "                                    pd.Series(xgb_results['train'][X_train_rest.index], index=X_train_rest.index)]).sort_index(),\n",
    "                \"test\":  pd.concat([pd.Series(xgb_results_cmntf['test'], index=X_test_cmntf.index),\n",
    "                                    pd.Series(xgb_results['test'][X_test_rest.index], index=X_test_rest.index)]).sort_index()}\n",
    "            \n",
    "            print(\"-\"*80)\n",
    "            RMSLE_base    = mean_squared_error(y, xgb_results['train'], squared=False)\n",
    "            print(f\"Base    RMSLE={RMSLE_base}\")\n",
    "            RMSLE_base_false = mean_squared_error(y_cmntf, xgb_results['train'][X_train_cmntf.index], squared=False)\n",
    "            print(f\"f in B  RMSLE={RMSLE_base_false}\")\n",
    "            RMSLE_false   = mean_squared_error(y_cmntf, xgb_results_cmntf['train'], squared=False)\n",
    "            print(f\"False   RMSLE={RMSLE_false}\")\n",
    "            RMSLE_merged  = mean_squared_error(y, xgb_results_merged['train'], squared=False)\n",
    "            print(f\"merged  RMSLE={RMSLE_merged}\")\n",
    "            print(\"-\"*80)\n",
    "\n",
    "            if RMSLE_base_false > RMSLE_false:\n",
    "                results = xgb_results_merged\n",
    "            else:\n",
    "                results = xgb_results\n",
    "            \n",
    "            y_pred = pd.Series(results['train']).copy()\n",
    "            y_pred.where((y_pred<14)&(X_train.comments_disabled==True), y_pred*0.99965, inplace=True)\n",
    "            y_pred.where((y_pred>14)&(X_train.comments_disabled==True), y_pred*1.0001, inplace=True)\n",
    "            y_pred.where((y_pred<14)&(X_train.ratings_disabled==True), y_pred*0.99965, inplace=True)\n",
    "            y_pred.where((y_pred>14)&(X_train.ratings_disabled==True), y_pred*1.0001, inplace=True)\n",
    "\n",
    "            RMSLE_merged  = mean_squared_error(y, y_pred, squared=False)\n",
    "            print(f\"merged2 RMSLE={RMSLE_merged}\")\n",
    "            print(\"-\"*80)\n",
    "\n",
    "            sunmission = output_results(results, test_id, seed, N_FOLDS, dataid, out_dir)\n",
    "\n",
    "        if CAT:\n",
    "            #-----------------------------------------------\n",
    "            # CATBOOST MODEL 1\n",
    "            #-----------------------------------------------\n",
    "            # df[df['comments_disabled']==False]の学習、予測\n",
    "            cat_params = {'bootstrap_type': 'Bayesian', \n",
    "                          'boosting_type': 'Plain', #'Ordered', #'Plain',\n",
    "                          'iterations':50000,\n",
    "                          'depth': 8,\n",
    "                          'loss_function': 'RMSE',\n",
    "                          'eval_metric': 'RMSE',\n",
    "                          'learning_rate': 1e-2,\n",
    "                          'leaf_estimation_method': 'Gradient', #'Newton', 'Exact'\n",
    "                          'l2_leaf_reg': 1.0,\n",
    "                          'random_strength': 0.8,\n",
    "                          'bagging_temperature': 0.9,\n",
    "                          'has_time': False,\n",
    "                          'grow_policy': 'SymmetricTree', #'Depthwise', 'Lossguide'\n",
    "                          'min_data_in_leaf': 1,\n",
    "                          'max_leaves': 31,\n",
    "                          'random_seed': seed,\n",
    "                          #'one_hot_max_size': len(cat_features),\n",
    "                          'task_type': 'GPU'}\n",
    "            if SKF:\n",
    "                cat_results_cmntf = run_cv_model(train=X_train_cmntf, test=X_test_cmntf, target=y_cmntf, target_skf=y_bin_cmntf, encoding=[],\n",
    "                                                 model_fn=runCAT, params=cat_params, eval_fn=rmsle, label='CatBoostRegressor',\n",
    "                                                 cv=N_FOLDS, repeats=1, seed=seed)\n",
    "                # df全体の学習、予測\n",
    "                cat_results = run_cv_model(train=X_train, test=X_test, target=y, target_skf=y_bin, encoding=[],\n",
    "                                           model_fn=runCAT, params=cat_params, eval_fn=rmsle, label='CatBoostRegressor',\n",
    "                                           cv=N_FOLDS, repeats=1, seed=seed)\n",
    "            else:\n",
    "                cat_results_cmntf = run_cv_model(train=X_train_cmntf, test=X_test_cmntf, target=y_cmntf, target_skf=None, encoding=[],\n",
    "                                                 model_fn=runCAT, params=cat_params, eval_fn=rmsle, label='CatBoostRegressor',\n",
    "                                                 cv=N_FOLDS, repeats=1, seed=seed)\n",
    "                # df全体の学習、予測\n",
    "                cat_results = run_cv_model(train=X_train, test=X_test, target=y, target_skf=None, encoding=[],\n",
    "                                           model_fn=runCAT, params=cat_params, eval_fn=rmsle, label='CatBoostRegressor',\n",
    "                                           cv=N_FOLDS, repeats=1, seed=seed)\n",
    "\n",
    "            cat_results_merged = {\n",
    "                \"label\": \"CatBoostRegressor\",\n",
    "                \"train\": pd.concat([pd.Series(cat_results_cmntf['train'], index=X_train_cmntf.index),\n",
    "                                    pd.Series(cat_results['train'][X_train_rest.index], index=X_train_rest.index)]).sort_index(),\n",
    "                \"test\":  pd.concat([pd.Series(cat_results_cmntf['test'], index=X_test_cmntf.index),\n",
    "                                    pd.Series(cat_results['test'][X_test_rest.index], index=X_test_rest.index)]).sort_index()}\n",
    "\n",
    "            print(\"-\"*80)\n",
    "            RMSLE_base    = mean_squared_error(y, cat_results['train'], squared=False)\n",
    "            print(f\"Base    RMSLE={RMSLE_base}\")\n",
    "            RMSLE_base_false = mean_squared_error(y_cmntf, cat_results['train'][X_train_cmntf.index], squared=False)\n",
    "            print(f\"f in B  RMSLE={RMSLE_base_false}\")\n",
    "            RMSLE_false   = mean_squared_error(y_cmntf, cat_results_cmntf['train'], squared=False)\n",
    "            print(f\"False   RMSLE={RMSLE_false}\")\n",
    "            RMSLE_merged  = mean_squared_error(y, cat_results_merged['train'], squared=False)\n",
    "            print(f\"merged  RMSLE={RMSLE_merged}\")\n",
    "            print(\"-\"*80)\n",
    "\n",
    "            if RMSLE_base_false > RMSLE_false:\n",
    "                results = cat_results_merged\n",
    "            else:\n",
    "                results = cat_results\n",
    "\n",
    "            y_pred = pd.Series(results['train']).copy()\n",
    "            y_pred.where((y_pred<14)&(X_train.comments_disabled==True), y_pred*0.99965, inplace=True)\n",
    "            y_pred.where((y_pred>14)&(X_train.comments_disabled==True), y_pred*1.0001, inplace=True)\n",
    "            y_pred.where((y_pred<14)&(X_train.ratings_disabled==True), y_pred*0.99965, inplace=True)\n",
    "            y_pred.where((y_pred>14)&(X_train.ratings_disabled==True), y_pred*1.0001, inplace=True)\n",
    "\n",
    "            RMSLE_merged  = mean_squared_error(y, y_pred, squared=False)\n",
    "            print(f\"merged2 RMSLE={RMSLE_merged}\")\n",
    "            print(\"-\"*80)\n",
    "\n",
    "            sunmission = output_results(results, test_id, seed, N_FOLDS, dataid, out_dir)\n",
    "\n",
    "            #-----------------------------------------------\n",
    "            # CATBOOST MODEL 2\n",
    "            #-----------------------------------------------\n",
    "            # df[df['comments_disabled']==False]の学習、予測\n",
    "            cat_params = {'bootstrap_type': 'Bayesian', \n",
    "                          'boosting_type': 'Plain', #'Ordered', #'Plain',\n",
    "                          'iterations':50000,\n",
    "                          'depth': 16,\n",
    "                          'loss_function': 'RMSE',\n",
    "                          'eval_metric': 'RMSE',\n",
    "                          'learning_rate': 1e-2,\n",
    "                          'leaf_estimation_method': 'Gradient', #'Newton', 'Exact'\n",
    "                          'l2_leaf_reg': 1.0,\n",
    "                          'random_strength': 0.8,\n",
    "                          'bagging_temperature': 0.9,\n",
    "                          'has_time': False,\n",
    "                          'grow_policy': 'Lossguide', # 'SymmetricTree', #'Depthwise'\n",
    "                          'min_data_in_leaf': 1,\n",
    "                          'max_leaves': 31,\n",
    "                          'random_seed': seed,\n",
    "                          #'one_hot_max_size': len(cat_features),\n",
    "                          'task_type': 'GPU'}\n",
    "            if SKF:\n",
    "                cat_results_cmntf = run_cv_model(train=X_train_cmntf, test=X_test_cmntf, target=y_cmntf, target_skf=y_bin_cmntf, encoding=[],\n",
    "                                                 model_fn=runCAT, params=cat_params, eval_fn=rmsle, label='CatBoostRegressor2',\n",
    "                                                 cv=N_FOLDS, repeats=1, seed=seed)\n",
    "                # df全体の学習、予測\n",
    "                cat_results = run_cv_model(train=X_train, test=X_test, target=y, target_skf=y_bin, encoding=[],\n",
    "                                           model_fn=runCAT, params=cat_params, eval_fn=rmsle, label='CatBoostRegressor2',\n",
    "                                           cv=N_FOLDS, repeats=1, seed=seed)\n",
    "            else:\n",
    "                cat_results_cmntf = run_cv_model(train=X_train_cmntf, test=X_test_cmntf, target=y_cmntf, target_skf=None, encoding=[],\n",
    "                                                 model_fn=runCAT, params=cat_params, eval_fn=rmsle, label='CatBoostRegressor2',\n",
    "                                                 cv=N_FOLDS, repeats=1, seed=seed)\n",
    "                # df全体の学習、予測\n",
    "                cat_results = run_cv_model(train=X_train, test=X_test, target=y, target_skf=None, encoding=[],\n",
    "                                           model_fn=runCAT, params=cat_params, eval_fn=rmsle, label='CatBoostRegressor2',\n",
    "                                           cv=N_FOLDS, repeats=1, seed=seed)\n",
    "\n",
    "            cat_results_merged = {\n",
    "                \"label\": \"CatBoostRegressor2\",\n",
    "                \"train\": pd.concat([pd.Series(cat_results_cmntf['train'], index=X_train_cmntf.index),\n",
    "                                    pd.Series(cat_results['train'][X_train_rest.index], index=X_train_rest.index)]).sort_index(),\n",
    "                \"test\":  pd.concat([pd.Series(cat_results_cmntf['test'], index=X_test_cmntf.index),\n",
    "                                    pd.Series(cat_results['test'][X_test_rest.index], index=X_test_rest.index)]).sort_index()}\n",
    "\n",
    "            print(\"-\"*80)\n",
    "            RMSLE_base    = mean_squared_error(y, cat_results['train'], squared=False)\n",
    "            print(f\"Base    RMSLE={RMSLE_base}\")\n",
    "            RMSLE_base_false = mean_squared_error(y_cmntf, cat_results['train'][X_train_cmntf.index], squared=False)\n",
    "            print(f\"f in B  RMSLE={RMSLE_base_false}\")\n",
    "            RMSLE_false   = mean_squared_error(y_cmntf, cat_results_cmntf['train'], squared=False)\n",
    "            print(f\"False   RMSLE={RMSLE_false}\")\n",
    "            RMSLE_merged  = mean_squared_error(y, cat_results_merged['train'], squared=False)\n",
    "            print(f\"merged  RMSLE={RMSLE_merged}\")\n",
    "            print(\"-\"*80)\n",
    "\n",
    "            if RMSLE_base_false > RMSLE_false:\n",
    "                results = cat_results_merged\n",
    "            else:\n",
    "                results = cat_results\n",
    "\n",
    "            sunmission = output_results(results, test_id, seed, N_FOLDS, dataid, out_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datetime.datetime.now()-start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
