{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ProbSpace: YouTube動画視聴回数予測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = \"out_tmp\"\n",
    "!mkdir -p $out_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "import itertools\n",
    "import os, datetime, gc, glob, re, random\n",
    "import time, datetime\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import optuna\n",
    "import bhtsne, umap\n",
    "\n",
    "from janome.tokenizer import Tokenizer\n",
    "from janome.analyzer import Analyzer\n",
    "from janome.tokenfilter import *\n",
    "from janome.charfilter import UnicodeNormalizeCharFilter, RegexReplaceCharFilter\n",
    "import unicodedata\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import Pool, CatBoostRegressor, CatBoostClassifier\n",
    "\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.linear_model import LinearRegression, BayesianRidge, ElasticNet, Lasso, LogisticRegression, Ridge, SGDRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor, BaggingRegressor\n",
    "from sklearn.ensemble import StackingRegressor, VotingRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor, GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.svm import LinearSVR\n",
    "\n",
    "from ngboost import NGBRegressor\n",
    "from ngboost.ngboost import NGBoost\n",
    "from ngboost.learners import default_tree_learner\n",
    "from ngboost.scores import MLE, CRPS, LogScore\n",
    "from ngboost.distns import Normal, LogNormal\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor, LocalOutlierFactor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans, DBSCAN\n",
    "from sklearn.model_selection import KFold, RepeatedKFold, StratifiedKFold, cross_validate, cross_val_predict, train_test_split\n",
    "from sklearn.metrics import mean_squared_error, roc_auc_score, roc_curve, log_loss\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler, Normalizer, RobustScaler, QuantileTransformer, PowerTransformer\n",
    "from sklearn.feature_selection import SelectFromModel, RFE, SelectPercentile, SelectKBest\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.optimizers import Adadelta, Adagrad, Adam, Adamax, Ftrl, Nadam, RMSprop, SGD\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, EarlyStopping, TensorBoard, LambdaCallback\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Concatenate, Lambda\n",
    "from tensorflow.keras.layers import Activation, Average, Dense, Dropout, Flatten, BatchNormalization, LeakyReLU, Input\n",
    "from tensorflow.keras.layers import Conv2D, SeparableConv2D, MaxPooling2D, AveragePooling2D, GlobalAveragePooling2D\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import missingno as msno \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for variable description\n",
    "def description(df):\n",
    "    summary = pd.DataFrame(df.dtypes, columns=['dtypes'])\n",
    "    summary = summary.reset_index()\n",
    "    summary[\"Name\"] = summary['index']\n",
    "    summary = summary[[\"Name\",'dtypes']]\n",
    "    summary[\"Missing\"] = df.isnull().sum().values    \n",
    "    summary[\"Uniques\"] = df.nunique().values\n",
    "    summary[\"Mean\"] = np.nanmean(df, axis=0).astype(df.dtypes)\n",
    "    summary[\"Std\"] = np.nanstd(df, axis=0).astype(df.dtypes)\n",
    "    summary[\"Minimum\"] = np.nanmin(df, axis=0).astype(df.dtypes)\n",
    "    summary[\"Maximum\"] = np.nanmax(df, axis=0).astype(df.dtypes)\n",
    "    summary[\"First Value\"] = df.iloc[0].values\n",
    "    summary[\"Second Value\"] = df.iloc[1].values\n",
    "    summary[\"Third Value\"] = df.iloc[2].values\n",
    "    summary[\"dimension\"] = str(df.shape)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hist(target):\n",
    "    plt.hist(target, bins=100)\n",
    "\n",
    "    print(\"max:  {:>10,.6f}\".format(target.max()))\n",
    "    print(\"min:  {:>10,.6f}\".format(target.min()))\n",
    "    print(\"mean: {:>10,.6f}\".format(target.mean()))\n",
    "    print(\"std:  {:>10,.6f}\".format(target.std()))\n",
    "    \n",
    "    return\n",
    "\n",
    "def get_hist4(target1, title1, target2, title2, target3, title3, target4, title4):\n",
    "    fig = plt.figure(figsize=(18, 18))\n",
    "\n",
    "    ax1 = fig.add_subplot(5,1,1)\n",
    "    ax2 = fig.add_subplot(5,1,2)\n",
    "    ax3 = fig.add_subplot(5,1,3)\n",
    "    ax4 = fig.add_subplot(5,1,4)\n",
    "    ax5 = fig.add_subplot(5,1,5)\n",
    "\n",
    "    ax1.set_title(title1)\n",
    "    ax2.set_title(title2)\n",
    "    ax3.set_title(title3)\n",
    "    ax4.set_title(title4)\n",
    "    ax5.set_title(\"OVERALL\")\n",
    "    \n",
    "    ax1.hist(target1, bins=100)\n",
    "    ax2.hist(target2, bins=100)\n",
    "    ax3.hist(target3, bins=100)\n",
    "    ax4.hist(target4, bins=100)\n",
    "\n",
    "    ax5.hist(target1, bins=100, alpha=0.2, color='red')\n",
    "    ax5.hist(target2, bins=100, alpha=0.2, color='green')\n",
    "    ax5.hist(target3, bins=100, alpha=0.2, color='blue')\n",
    "    #ax5.hist(target4, bins=100, alpha=0.2, color='grey')\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# for train/test data\n",
    "train_data = pd.read_csv(\"./input/train_data.csv\")\n",
    "test_data = pd.read_csv(\"./input/test_data.csv\")\n",
    "\n",
    "y = np.log1p(train_data['y']).copy()\n",
    "y_bin = pd.cut(train_data['y'], [0, 10, 100,1000,10000,100000,1000000,10000000000], labels=[1,2,3,4,5,6,7])\n",
    "y_bin = y_bin.astype(int)\n",
    "test_id = test_data.id\n",
    "\n",
    "train = train_data.drop(['id', 'y'], axis=1).copy()\n",
    "test  = test_data.drop(['id'], axis=1).copy()\n",
    "\n",
    "cols_to_log = ['likes', 'dislikes', 'comment_count']\n",
    "train[cols_to_log] = np.log1p(train[cols_to_log])\n",
    "test[cols_to_log]  = np.log1p(test[cols_to_log])\n",
    "\n",
    "traintest = pd.concat([train, test]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 目的変数の分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_hist(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in train_data.columns:\n",
    "    print(\"{:<20}: {} ({:.2f}%)\".format(col, train_data[col].isnull().sum(), train_data[col].isnull().sum()/train_data.shape[0]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in test_data.columns:\n",
    "    print(\"{:<20}: {} ({:.2f}%)\".format(col, test_data[col].isnull().sum(), test_data[col].isnull().sum()/test_data.shape[0]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# seedの固定化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=1234):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特徴量生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(df, features):\n",
    "    # 欠損値処理\n",
    "    df['tags'].fillna(\"[none]\", inplace=True)\n",
    "    df['description'].fillna(df['tags'].replace(\"|\", \" \") + df['title'], inplace=True)\n",
    "\n",
    "    tokenizer = Tokenizer()\n",
    "    char_filters = [UnicodeNormalizeCharFilter(), RegexReplaceCharFilter(r\"[0123456789!#$%&()=~|\\-^\\\\@`{;:+*},./\\<>?_♪®」—]\", \"\")]\n",
    "    token_filters = [POSKeepFilter(['名詞'])]\n",
    "    #token_filters = [POSStopFilter(['接続詞', '接頭辞', '接尾辞', '記号', '助詞', '助動詞']), TokenCountFilter()]\n",
    "    a = Analyzer(char_filters, tokenizer, token_filters=token_filters)\n",
    "\n",
    "    df.loc[df['tags']==\"[none]\", 'tags'] = \\\n",
    "    df['title'][df['tags']==\"[none]\"].str.lower().apply(lambda x: \"|\".join([word.surface for word in a.analyze(x)]))\n",
    "    \n",
    "    # ------------------------------------------\n",
    "    # ビニング\n",
    "    # ------------------------------------------\n",
    "    df['likes_qcut'] = pd.qcut(df['likes'], 10000, False, duplicates='drop')\n",
    "    df['dislikes_qcut'] = pd.qcut(df['dislikes'], 10000, False, duplicates='drop')\n",
    "    df['comment_count_qcut'] = pd.qcut(df['comment_count'], 10000, False, duplicates='drop')\n",
    "    features[\"cat\"] += ['likes_qcut', 'dislikes_qcut', 'comment_count_qcut']\n",
    "\n",
    "    df['likes_cut'] = pd.cut(df['likes'], np.ceil(df['likes'].max()+1).astype('int'), False, labels=False, duplicates='drop', include_lowest=True)\n",
    "    df['dislikes_cut'] = pd.cut(df['dislikes'], np.ceil(df['dislikes'].max()+1).astype('int'), False, labels=False, duplicates='drop', include_lowest=True)\n",
    "    df['comment_count_cut'] = pd.cut(df['comment_count'], np.ceil(df['comment_count'].max()+1).astype('int'), False, labels=False, duplicates='drop', include_lowest=True)\n",
    "    features[\"cat\"] += ['likes_cut', 'dislikes_cut', 'comment_count_cut']\n",
    "\n",
    "    # ------------------------------------------\n",
    "    # 比率\n",
    "    # ------------------------------------------\n",
    "    df['likes_dislikes_ratio'] = df['likes']/(df['dislikes']+1)\n",
    "    df['comment_count_likes_ratio'] = df['comment_count']/(df['likes']+1)\n",
    "    df['comment_count_dislikes_ratio'] = df['comment_count']/(df['dislikes']+1)\n",
    "\n",
    "    features[\"num\"] += ['likes_dislikes_ratio', 'comment_count_likes_ratio', 'comment_count_dislikes_ratio']\n",
    "\n",
    "    # ------------------------------------------\n",
    "    # 積算\n",
    "    # ------------------------------------------\n",
    "    df['mul_likes_comments_disabled'] = df['likes'] * df['comments_disabled']\n",
    "    df['mul_dislikes_comments_disabled'] = df['dislikes'] * df['comments_disabled']\n",
    "    df['mul_comment_count_ratings_disabled'] = df['comment_count'] * df['ratings_disabled']\n",
    "\n",
    "    features[\"num\"] += ['mul_likes_comments_disabled', 'mul_dislikes_comments_disabled', 'mul_comment_count_ratings_disabled']\n",
    "    \n",
    "    # ------------------------------------------\n",
    "    # 出現頻度\n",
    "    # ------------------------------------------\n",
    "    for col in ['channelId', 'channelTitle', 'categoryId']:\n",
    "        df['_'.join(list(map(str, ['freq', col])))] = df[col].map(df[col].value_counts())\n",
    "        \n",
    "        features[\"num\"] += ['_'.join(list(map(str, ['freq', col])))]\n",
    "\n",
    "    # ------------------------------------------\n",
    "    # 'n_tags'数特徴量の生成\n",
    "    # ------------------------------------------    \n",
    "    df['n_tags'] = df['tags'].astype(str).apply(lambda x: len(x.split(\"|\")))\n",
    "    \n",
    "    features[\"num\"] += ['n_tags']\n",
    "    \n",
    "    # ------------------------------------------\n",
    "    # 'description'関連の特徴量生成\n",
    "    # ------------------------------------------    \n",
    "    df['http_count_in_desc'] = df['description'].apply(lambda x: x.lower().count(\"http\"))\n",
    "    df['len_description'] = df['description'].apply(lambda x: len(x))\n",
    "    df['len_title'] = df['title'].apply(lambda x: len(x))\n",
    "\n",
    "    features[\"num\"] += ['http_count_in_desc', 'len_description', 'len_title']\n",
    "    \n",
    "    # ------------------------------------------\n",
    "    # 'title'/'tag'/'description'内の記述言語関連特徴量の生成\n",
    "    # ------------------------------------------    \n",
    "    def checkJapanese(word):\n",
    "        for ch in word:\n",
    "            try:\n",
    "                name = unicodedata.name(ch) \n",
    "                if \"CJK UNIFIED\" in name \\\n",
    "                or \"HIRAGANA\" in name \\\n",
    "                or \"KATAKANA\" in name:\n",
    "                    return True\n",
    "            except:\n",
    "              continue\n",
    "        return False\n",
    "\n",
    "    def checkAlnum(word):\n",
    "        alnum = re.compile(r'^[a-zA-Z0-9]+$')\n",
    "        result = alnum.match(word) is not None\n",
    "        return result\n",
    "\n",
    "    # is japanese\n",
    "    df['isJa_title'] = df['title'].apply(lambda x: checkJapanese(x))\n",
    "    df['isJa_tags'] = df['tags'].apply(lambda x: checkJapanese(x))\n",
    "    df['isJa_description'] = df['description'].apply(lambda x: checkJapanese(x))\n",
    "\n",
    "    features[\"ohe\"] += ['isJa_title', 'isJa_tags', 'isJa_description']\n",
    "    \n",
    "    # isalnum\n",
    "    df['isalnum_title'] = df['title'].apply(lambda x: checkAlnum(x))\n",
    "    df['isalnum_tags'] = df['tags'].apply(lambda x: checkAlnum(x))\n",
    "    df['isalnum_description'] = df['description'].apply(lambda x: checkAlnum(x))\n",
    "\n",
    "    features[\"ohe\"] += ['isalnum_title', 'isalnum_tags', 'isalnum_description']\n",
    "    \n",
    "    # cotain english\n",
    "    df['inclEn_title'] = df['title'].apply(lambda x: len(re.findall(r'[a-zA-Z0-9]', x.lower())))\n",
    "    df['inclEn_tags'] = df['tags'].apply(lambda x: len(re.findall(r'[a-zA-Z0-9]', x.lower())))\n",
    "    df['inclEn_description'] = df['description'].apply(lambda x: len(re.findall(r'[a-zA-Z0-9]', x.lower())))\n",
    "    \n",
    "    features[\"num\"] += ['inclEn_title', 'inclEn_tags', 'inclEn_description']\n",
    "\n",
    "    # ------------------------------------------\n",
    "    # 投稿時期、採取時期からの期間、日時関連特徴量の生成\n",
    "    # ------------------------------------------    \n",
    "    # publishedAt\n",
    "    df['publishedAt'] = pd.to_datetime(df['publishedAt'], utc=True)\n",
    "    df['publishedAt_year'] = df['publishedAt'].apply(lambda x: x.year)\n",
    "    df['publishedAt_month'] = df['publishedAt'].apply(lambda x: x.month)\n",
    "    df['publishedAt_day'] = df['publishedAt'].apply(lambda x: x.day)\n",
    "    df['publishedAt_hour'] = df['publishedAt'].apply(lambda x: x.hour)\n",
    "    df['publishedAt_minute'] = df['publishedAt'].apply(lambda x: x.minute)\n",
    "    df['publishedAt_second'] = df['publishedAt'].apply(lambda x: x.second)\n",
    "    df['publishedAt_dayofweek'] = df['publishedAt'].apply(lambda x: x.dayofweek)\n",
    "\n",
    "    df['collection_date'] = \\\n",
    "    df['collection_date'].map(lambda x: x.split('.')).map(lambda x: '20'+x[0]+'-'+x[2]+'-'+x[1]+'T00:00:00.000Z')\n",
    "        \n",
    "    # collection_date\n",
    "    df['collection_date'] = pd.to_datetime(df['collection_date'], utc=True)\n",
    "    df['collection_date_year'] = df['collection_date'].apply(lambda x: x.year)\n",
    "    df['collection_date_month'] = df['collection_date'].apply(lambda x: x.month)\n",
    "    df['collection_date_day'] = df['collection_date'].apply(lambda x: x.day)\n",
    "\n",
    "    # delta\n",
    "    df['delta'] = (df['collection_date'] - df['publishedAt']).apply(lambda x: x.days)\n",
    "    df['log_delta'] = np.log(df['delta'])\n",
    "    df['sqrt_delta'] = np.sqrt(df['delta'])\n",
    "    df['pow_delta'] = pow(df['delta'], 2)\n",
    "    df['log_pow_delta'] = pow(np.log(df['delta']), 2)\n",
    "    df['publishedAt_delta'] = (df['publishedAt'] - df['publishedAt'].min()).apply(lambda x: x.days)\n",
    "    df['collection_delta'] = (df['collection_date'] - df['collection_date'].min()).apply(lambda x: x.days)\n",
    "    \n",
    "    features[\"cat\"] += ['publishedAt_year', 'publishedAt_month', 'publishedAt_day', \\\n",
    "                        'publishedAt_hour', 'publishedAt_minute', 'publishedAt_second', 'publishedAt_dayofweek', \\\n",
    "                        'collection_date_year', 'collection_date_month', 'collection_date_day']\n",
    "    \n",
    "    features[\"num\"] += ['delta', 'log_delta', 'sqrt_delta', 'pow_delta', 'log_pow_delta', \\\n",
    "                        'publishedAt_delta', 'collection_delta']\n",
    "    \n",
    "    return df, features\n",
    "\n",
    "def create_features2(df, features, cols_groupby, cols_transform, target_func, option):\n",
    "    # ------------------------------------------\n",
    "    # 'cols_groupby'ごとの特徴量生成\n",
    "    # ------------------------------------------   \n",
    "    \n",
    "    cols_to_transform = list(set(cols_transform) - set(cols_groupby))\n",
    "\n",
    "    #cols_to_transform = [c for c in df.columns if ('likes' in c) | ('dislikes' in c) | ('comment_count' in c)]\n",
    "    for col_base in cols_groupby:\n",
    "        for col in cols_to_transform:\n",
    "            for func in target_func:\n",
    "                df['_'.join(list(map(str, [col_base, col, func])))] = df.groupby(col_base)[col].transform(func)\n",
    "                features['num'] += ['_'.join(list(map(str, [col_base, col, func])))]\n",
    "                \n",
    "                if option[\"log\"]:\n",
    "                    df['_'.join(list(map(str, [col_base, col, 'log', func])))] = np.log1p(df.groupby(col_base)[col].transform(func))\n",
    "                    features['num'] += ['_'.join(list(map(str, [col_base, col, 'log', func])))]\n",
    "                if option[\"sqrt\"]:\n",
    "                    df['_'.join(list(map(str, [col_base, col, \"sqrt\", func])))] = np.sqrt(df.groupby(col_base)[col].transform(func))\n",
    "                    features[\"num\"] += ['_'.join(list(map(str, [col_base, col, \"sqrt\", func])))]\n",
    "                if option[\"sqrt_log\"]:\n",
    "                    df['_'.join(list(map(str, [col_base, col, \"sqrt_log\", func])))] = np.log1p(np.sqrt(df.groupby(col_base)[col].transform(func)))\n",
    "                    features[\"num\"] += ['_'.join(list(map(str, [col_base, col, \"sqrt_log\", func])))]\n",
    "                if option[\"log_sqrt\"]:\n",
    "                    df['_'.join(list(map(str, [col_base, col, \"log_sqrt\", func])))] = np.sqrt(np.log1p(df.groupby(col_base)[col].transform(func)))\n",
    "                    features[\"num\"] += ['_'.join(list(map(str, [col_base, col, \"log_sqrt\", func])))]\n",
    "                if option[\"pow\"]:\n",
    "                    df['_'.join(list(map(str, [col_base, col, \"pow\", func])))] = pow(df.groupby(col_base)[col].transform(func), 2)\n",
    "                    features[\"num\"] += ['_'.join(list(map(str, [col_base, col, \"pow\", func])))]\n",
    "                if option[\"pow_log\"]:\n",
    "                    df['_'.join(list(map(str, [col_base, col, 'pow_log', func])))] = 2*np.log1p(df.groupby(col_base)[col].transform(func))\n",
    "                    features['num'] += ['_'.join(list(map(str, [col_base, col, 'pow_log', func])))]\n",
    "                if option[\"log_pow\"]:\n",
    "                    df['_'.join(list(map(str, [col_base, col, 'log_pow', func])))] = pow(np.log1p(df.groupby(col_base)[col].transform(func)), 2)\n",
    "                    features['num'] += ['_'.join(list(map(str, [col_base, col, 'log_pow', func])))]\n",
    "\n",
    "    return df, features\n",
    "\n",
    "def create_features3(df, features, cols_transform, option):\n",
    "    for col in cols_transform:\n",
    "        if option[\"log\"]:\n",
    "            df['_'.join(list(map(str, ['log', col])))] = np.log1p(df[col])\n",
    "            features[\"num\"] += ['_'.join(list(map(str, ['log', col])))]\n",
    "\n",
    "        if option[\"sqrt\"]:\n",
    "            df['_'.join(list(map(str, ['sqrt', col])))] = np.sqrt(df[col])\n",
    "            features[\"num\"] += ['_'.join(list(map(str, ['sqrt', col])))]\n",
    "\n",
    "        if option[\"sqrt_log\"]:\n",
    "            df['_'.join(list(map(str, ['sqrt', 'log', col])))] = np.log1p(np.sqrt(df[col]))\n",
    "            features[\"num\"] += ['_'.join(list(map(str, ['sqrt', 'log', col])))]\n",
    "        \n",
    "        if option[\"log_sqrt\"]:\n",
    "            df['_'.join(list(map(str, ['log', 'sqrt', col])))] = np.sqrt(np.log1p(df[col]))\n",
    "            features[\"num\"] += ['_'.join(list(map(str, ['log', 'sqrt', col])))]\n",
    "        \n",
    "        if option[\"pow\"]:\n",
    "            df['_'.join(list(map(str, ['pow', col])))] = pow(df[col], 2)\n",
    "            features[\"num\"] += ['_'.join(list(map(str, ['pow', col])))]\n",
    "        \n",
    "        if option[\"pow_log\"]:\n",
    "            df['_'.join(list(map(str, ['pow', 'log', col])))] = np.log1p(pow(df[col], 2))\n",
    "            features[\"num\"] += ['_'.join(list(map(str, ['pow', 'log', col])))]\n",
    "        \n",
    "        if option[\"log_pow\"]:\n",
    "            df['_'.join(list(map(str, ['log', 'pow', col])))] = pow(np.log1p(df[col]), 2)\n",
    "            features[\"num\"] += ['_'.join(list(map(str, ['log', 'pow', col])))]\n",
    "        \n",
    "    return df, features\n",
    "\n",
    "def create_features4(df, features, option):\n",
    "    if (not option[\"title\"]) and (not option[\"tags\"]) and (not option[\"description\"]):\n",
    "                return df, features\n",
    "        \n",
    "    tokenizer = Tokenizer()\n",
    "    feats_increased = []\n",
    "   \n",
    "    #title_words = [\"video\", \"official\", \"music\", \"公式\"]\n",
    "    title_words = [\"video\", \"official\"]\n",
    "    #tags_words = [\"music\", \"video\", \"official\", \"song\", \"remastered\", \"vevo\", \"lyric\", \"rock\", \"you\", \"pop\", \"live\", \"queen\"]\n",
    "    tags_words = [\"music\", \"video\", \"official\"]\n",
    "    #desc_words= [\"http\", \"www\", \"smarturl\", \"super\", \"simple\", \"video\", \"music\", \"facebook\", \"youtube\", \"twitter\", \"official\", \"instagram\"]\n",
    "    desc_words= [\"com\", \"http\"]\n",
    "    \n",
    "    for j, (title_sentence, tags_sentence, desc_sentence) in enumerate(tqdm(zip(df['title'].str.lower(), df['tags'].str.lower(), df['description'].str.lower()))):\n",
    "        if option[\"title\"]:\n",
    "            title_text = \" \".join(tokenizer.tokenize(title_sentence, wakati=True))\n",
    "            for word in title_words:\n",
    "                if (word in title_text) | (f\"{word}s\" in title_text):\n",
    "                    #df['likes'][df.index==j] = df['likes'].iloc[j]*1.2\n",
    "                    #df['dislikes'][df.index==j] = df['dislikes'].iloc[j]*1.2\n",
    "                    #df['comment_count'][df.index==j] = df['comment_count'].iloc[j]*1.2\n",
    "                    if word == \"公式\":\n",
    "                        word = \"official\"\n",
    "                    df.loc[df.index==j, f'title_{word}'] = 1\n",
    "                    if not f'title_{word}' in feats_increased:\n",
    "                        feats_increased += [f'title_{word}']\n",
    "                \n",
    "        if option[\"tags\"]:\n",
    "            tags_text = \" \".join(tokenizer.tokenize(tags_sentence, wakati=True))\n",
    "            for word in tags_words:\n",
    "                if (word in tags_text) | (f\"{word}s\" in tags_text):\n",
    "                    #df['likes'][df.index==j] = df['likes'].iloc[j]*1.2\n",
    "                    #df['dislikes'][df.index==j] = df['dislikes'].iloc[j]*1.2\n",
    "                    #df['comment_count'][df.index==j] = df['comment_count'].iloc[j]*1.2\n",
    "                    df.loc[df.index==j, f'tags_{word}'] = 1\n",
    "                    if not f'tags_{word}' in feats_increased:\n",
    "                        feats_increased += [f'tags_{word}']\n",
    "\n",
    "        if option[\"description\"]:\n",
    "            desc_text = \" \".join(tokenizer.tokenize(desc_sentence, wakati=True))\n",
    "            for word in desc_words:\n",
    "                if (word in desc_text) | (f\"{word}s\" in desc_text):\n",
    "                    #df['likes'][df.index==j] = df['likes'].iloc[j]*1.2\n",
    "                    #df['dislikes'][df.index==j] = df['dislikes'].iloc[j]*1.2\n",
    "                    #df['comment_count'][df.index==j] = df['comment_count'].iloc[j]*1.2\n",
    "                    df.loc[df.index==j, f'desc_{word}'] = 1\n",
    "                    if not f'desc_{word}' in feats_increased:\n",
    "                        feats_increased += [f'desc_{word}']\n",
    "\n",
    "    features[\"ohe\"] += feats_increased\n",
    "    feats_increased_dict = {k: 0 for k in feats_increased}\n",
    "    df.fillna(feats_increased_dict, inplace=True)\n",
    "    df[feats_increased] = df[feats_increased].astype('int')\n",
    "        \n",
    "    return df, features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ラベルエンコーディング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encoder(df, cols_to_encode=[]):\n",
    "    lbl_enc_columns = cols_to_encode #cat_features + date_features + ohe_features\n",
    "\n",
    "    # Transforming all the labels of all variables\n",
    "    label_encoders = [LabelEncoder() for _ in range(len(lbl_enc_columns))]\n",
    "\n",
    "    for col, column in enumerate(lbl_enc_columns):\n",
    "        unique_values = pd.Series(df[column].unique())\n",
    "        unique_values = unique_values[unique_values.notnull()]\n",
    "        label_encoders[col].fit(unique_values)\n",
    "        df.loc[df[column].notnull(), column] = label_encoders[col].transform(df.loc[df[column].notnull(), column])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 標準化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardization(df, fnc_name=\"Standard\", cols_to_std=[]):\n",
    "    fnc_scaler = {\"Standard\": StandardScaler(copy=True, with_mean=True, with_std=True),\n",
    "                  \"MinMax\": MinMaxScaler(feature_range=(-1, 1), copy=True),\n",
    "                  \"MaxAbs\": MaxAbsScaler(copy=True),\n",
    "                  \"Normalize\": Normalizer(norm='max'),\n",
    "                  \"Robust\": RobustScaler(with_centering=True, with_scaling=True, quantile_range=(25.0, 75.0), copy=True),\n",
    "                  \"Quantile\": QuantileTransformer(n_quantiles=1000, output_distribution='normal', ignore_implicit_zeros=False, \\\n",
    "                                                  subsample=100000, random_state=None, copy=True),\n",
    "                  #\"box-cox\": PowerTransformer(method='box-cox'),\n",
    "                  \"yeo\": PowerTransformer(method='yeo-johnson', standardize=True, copy=True)\n",
    "                 }\n",
    "    scaler = fnc_scaler[fnc_name]\n",
    "    df[cols_to_std] = scaler.fit_transform(df[cols_to_std])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# featuresの削除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_features(df, features, features_to_drop):\n",
    "    cols_to_drop = features_to_drop[\"num\"]+features_to_drop[\"cat\"]+features_to_drop[\"date\"]+features_to_drop[\"ohe\"]\n",
    "    df.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "    for col in features_to_drop[\"num\"]:\n",
    "        try:\n",
    "            features[\"num\"].remove(col)\n",
    "        except ValueError as e:\n",
    "            print(e)\n",
    "            print(f\"error in num col: {col}\")\n",
    "\n",
    "    for col in features_to_drop[\"cat\"]:\n",
    "        try:\n",
    "            features[\"cat\"].remove(col)\n",
    "        except ValueError as e:\n",
    "            print(e)\n",
    "            print(f\"error in cat col: {col}\")\n",
    "\n",
    "    for col in features_to_drop[\"date\"]:\n",
    "        try:\n",
    "            features[\"date\"].remove(col)\n",
    "        except ValueError as e:\n",
    "            print(e)\n",
    "            print(f\"error in date col: {col}\")\n",
    "\n",
    "    for col in features_to_drop[\"ohe\"]:\n",
    "        try:\n",
    "            features[\"ohe\"].remove(col)\n",
    "        except ValueError as e:\n",
    "            print(e)\n",
    "            print(f\"error in ohe col: {col}\")\n",
    "\n",
    "    return df, features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学習、予測の共通処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIFF_THRESHOLD = 5\n",
    "\n",
    "################################################################################\n",
    "# METRICS\n",
    "################################################################################\n",
    "def rmsle(y, pred_y):\n",
    "    return mean_squared_error(y, pred_y, squared=False)\n",
    "\n",
    "def auc(y, pred_y):\n",
    "    return roc_auc_score(y, pred_y)\n",
    "\n",
    "################################################################################\n",
    "# CROSS-VALIDATION\n",
    "################################################################################\n",
    "def print_cv_scores(label, cv_scores):\n",
    "    print(\"*\"*40)\n",
    "    print(f\"type(cv_scores): {type(cv_scores)}\")\n",
    "    print(f\"{label} cv scores : {cv_scores}\")\n",
    "    print(f\"{label} cv mean score : {np.mean(cv_scores)}\")\n",
    "    print(f\"{label} cv std score : {np.std(cv_scores)}\")\n",
    "    \n",
    "def run_cv_model(train, test, target, target_skf, encoding, model_fn, params={}, \n",
    "                 eval_fn=None, label='model', cv=5,  repeats=5, seed=43):\n",
    "\n",
    "    if repeats==1:\n",
    "        if target_skf is None:\n",
    "            kf = KFold(n_splits=cv, shuffle=True, random_state=seed)\n",
    "            target_y = target\n",
    "        else:\n",
    "            kf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=seed)\n",
    "            target_y = target_skf\n",
    "        divide_counts = cv\n",
    "    else:\n",
    "        if target_skf is None:\n",
    "            kf = RepeatedKFold(n_splits=cv,n_repeats=repeats, random_state=seed)\n",
    "            target_y = target\n",
    "        else:\n",
    "            kf = RepeatedStratifiedKFold(n_splits=cv, n_repeats=repeats, random_state=seed)\n",
    "            target_y = target_skf\n",
    "        divide_counts = kf.get_n_splits()\n",
    "        \n",
    "    cv_scores = []\n",
    "    pred_full_test = 0\n",
    "    pred_train = np.zeros((train.shape[0]))\n",
    "\n",
    "    for fold_id, (train_idx, val_idx) in enumerate(kf.split(X=train, y=target_y)):\n",
    "        print(\"*\"*40)\n",
    "        print(f\"Started {label} fold:{fold_id+1} / {divide_counts}\")\n",
    "        tr_X, val_X = train.iloc[train_idx].copy(), train.iloc[val_idx].copy()\n",
    "        tr_y, val_y = target.iloc[train_idx], target.iloc[val_idx]\n",
    "        \n",
    "        # TARGET ENCODING\n",
    "        if encoding:\n",
    "            for c in encoding:\n",
    "                # 学習データ全体で各カテゴリにおけるtargetの平均を計算\n",
    "                data_tmp = pd.DataFrame({c: tr_X[c], 'target': tr_y})\n",
    "                target_mean = data_tmp.groupby(c)['target'].mean()\n",
    "                # バリデーションデータのカテゴリを置換\n",
    "                val_X.loc[:, c] = val_X[c].map(target_mean)\n",
    "\n",
    "                # 学習データの変換後の値を格納する配列を準備\n",
    "                tmp = np.repeat(np.nan, tr_X.shape[0])\n",
    "                kf_encoding = KFold(n_splits=4, shuffle=True, random_state=seed)\n",
    "                for idx_1, idx_2 in kf_encoding.split(tr_X):\n",
    "                    # out-of-foldで各カテゴリにおける目的変数の平均を計算\n",
    "                    target_mean = data_tmp.iloc[idx_1].groupby(c)['target'].mean()\n",
    "                    # 変換後の値を一次配列に格納\n",
    "                    tmp[idx_2] = tr_X[c].iloc[idx_2].map(target_mean)\n",
    "\n",
    "\n",
    "                tr_X.loc[:, c] = tmp\n",
    "        # TARGET ENCODING\n",
    "        \n",
    "        params2 = params.copy() \n",
    "        model, pred_val_y, pred_test_y = model_fn(\n",
    "            tr_X, tr_y, val_X, val_y, test, params2)\n",
    "        \n",
    "        pred_full_test = pred_full_test + pred_test_y\n",
    "        pred_train[val_idx] = pred_val_y\n",
    "        if eval_fn is not None:\n",
    "            cv_score = eval_fn(val_y, pred_val_y)\n",
    "            cv_scores.append(cv_score)\n",
    "            print(f\"{label} cv score {fold_id+1}: {cv_score}\")\n",
    "            \n",
    "    \n",
    "    print_cv_scores(label, cv_scores)    \n",
    "    pred_full_test = pred_full_test / divide_counts\n",
    "    results = {\"label\": label,\n",
    "               \"train\": pred_train, \n",
    "               \"test\": pred_full_test, \n",
    "               \"cv\": cv_scores}\n",
    "    \n",
    "    RMSLE = mean_squared_error(target.values, results[\"train\"], squared=False)\n",
    "    print(f\"Overall RMSLE={RMSLE}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "################################################################################\n",
    "# RESULTS\n",
    "################################################################################\n",
    "def submit_results(target, results, test_id, MODEL):\n",
    "    RMSLE = mean_squared_error(target.values, results[\"train\"], squared=False)\n",
    "    #print(f\"Overall RMSLE={RMSLE}\")\n",
    "\n",
    "    # Make submission\n",
    "    print(\"Saving submission file\")\n",
    "    submission = pd.DataFrame({'id': test_id, 'y': np.expm1(results[\"test\"])})\n",
    "    submission.to_csv(f\"./{out_dir}/submission_{MODEL}_CV{RMSLE:.6f}.csv\", index=False)\n",
    "        \n",
    "    return submission\n",
    "\n",
    "def check_results(y, results):\n",
    "    y_diff = np.abs(np.expm1(y) - np.expm1(results[\"train\"]))\n",
    "    y_log1p_diff = np.abs(y - results[\"train\"])\n",
    "    display(y_diff[y_log1p_diff>DIFF_THRESHOLD].index.values)\n",
    "    display(train_data[y_log1p_diff>DIFF_THRESHOLD])\n",
    "    display(pd.concat([pd.DataFrame(y[y_log1p_diff>DIFF_THRESHOLD], columns=['y']), \\\n",
    "                       pd.DataFrame(results[\"train\"][y_log1p_diff>DIFF_THRESHOLD], \\\n",
    "                                    index=y_diff[y_log1p_diff>DIFF_THRESHOLD].index.values, columns=[\"pred_train\"])], axis=1))    \n",
    "\n",
    "    get_hist4(results[\"train\"], \"pred_train\", \\\n",
    "              y, \"y\", \\\n",
    "              results[\"test\"], \"pred_test\", \\\n",
    "              y_log1p_diff, \"diff\")\n",
    "    \n",
    "    display(pd.concat([pd.DataFrame(results[\"train\"], columns=[\"pred_train\"]), \\\n",
    "                       pd.DataFrame(y, columns=[\"y\"]), \\\n",
    "                       y_log1p_diff.rename(\"y_log1p_diff\")], \\\n",
    "                       axis=1).describe())\n",
    "    \n",
    "    display(pd.DataFrame(results[\"test\"], columns=[\"pred_test\"]).describe())\n",
    "    \n",
    "    RMSLE = mean_squared_error(y, results[\"train\"], squared=False)\n",
    "    display(f\"Overall RMSLE={RMSLE:.6f}\")\n",
    "    \n",
    "################################################################################\n",
    "# MODEL\n",
    "################################################################################\n",
    "def runRDG(train_X, train_y, val_X, val_y, test_X, params):\n",
    "    model = Ridge(**params)\n",
    "    model.fit(train_X, train_y, sample_weight=None)\n",
    "    pred_val_y = model.predict(val_X)\n",
    "    pred_test_y = model.predict(test_X)\n",
    "    return model, pred_val_y, pred_test_y\n",
    "\n",
    "################################################################################\n",
    "# MODEL PARAMETERS\n",
    "################################################################################\n",
    "rdg_params = {'alpha': 1.0, 'copy_X': True, 'fit_intercept': True,\n",
    "              'max_iter': 100, 'normalize': False,\n",
    "              'random_state': 43, 'solver': 'auto', 'tol': 1e-3}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def ridge(train, test, target, target_skf, seed, n_folds, encoding):\n",
    "    rdg_params['alpha'] = 1.0\n",
    "    rdg_params['random_state'] = seed\n",
    "    rdg_params['max_iter'] = 1000\n",
    "\n",
    "    rdg_results = run_cv_model(train, test, target, target_skf, encoding, runRDG, rdg_params, rmsle, 'Ridge', cv=n_folds, repeats=1, seed=seed)\n",
    "    return rdg_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 学習/予測用データの準備 (#1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_features(df, features):\n",
    "    print(\"-\"*40)\n",
    "    print(f\"実特徴量数: {len(df.columns)} / 計算上の特徴量数: {len(features['num'])+len(features['cat'])+len(features['date'])+len(features['ohe'])}\")\n",
    "    print(\"-\"*40)\n",
    "    print(f\"(内訳) num_features: {len(features['num'])}, cat_features: {len(features['cat'])}, date_features: {len(features['date'])}, ohe_features: {len(features['ohe'])}\")\n",
    "    print(\"-\"*40)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 全データでの学習、予測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "seed_everything(seed=47)\n",
    "\n",
    "df = pd.concat([train, test]).reset_index(drop=True)\n",
    "\n",
    "df.loc[df['categoryId']==43, 'categoryId'] = 30\n",
    "\n",
    "# 特徴量の分類\n",
    "features = {\"cat\": ['video_id', 'title', 'channelId', 'channelTitle', 'categoryId', 'tags', 'thumbnail_link', 'description', 'comments_disabled', 'ratings_disabled'],\n",
    "            \"date\": ['publishedAt', 'collection_date'],\n",
    "            \"num\": ['likes', 'dislikes', 'comment_count'],\n",
    "            \"ohe\": []\n",
    "           }\n",
    "\n",
    "# ラベルエンコーディング\n",
    "print(\"ラベルエンコーディング\")\n",
    "df = label_encoder(df, cols_to_encode=['categoryId'])\n",
    "\n",
    "# 特徴量生成\n",
    "print(\"特徴量生成\")\n",
    "df, features = create_features(df, features)\n",
    "\n",
    "# 特徴量生成\n",
    "print(\"特徴量生成4\")\n",
    "df, features = create_features4(df, features, option={\"title\": False, \"tags\": False, \"description\": False})\n",
    "\n",
    "# ラベルエンコーディング\n",
    "print(\"ラベルエンコーディング\")\n",
    "df = label_encoder(df, cols_to_encode=features[\"cat\"]+features[\"date\"])\n",
    "\n",
    "# 特徴量生成2\n",
    "print(\"特徴量生成2 categoryId - likes/dislikes/comment_count/channelId\")\n",
    "df, features = create_features2(df, features,\n",
    "                                cols_groupby=['categoryId', 'comments_disabled', 'ratings_disabled', 'n_tags', 'len_description'],\n",
    "                                cols_transform=['likes', 'dislikes', 'comment_count', 'channelId',\n",
    "                                                'likes_cut', 'dislikes_cut', 'comment_count_cut',\n",
    "                                                'likes_qcut', 'dislikes_qcut', 'comment_count_qcut'],\n",
    "                                target_func=['max', 'min', 'mean'],\n",
    "                                option={\"log\": False,\\\n",
    "                                        \"sqrt\": False, \"sqrt_log\": False, \"log_sqrt\": False,\\\n",
    "                                        \"pow\": False, \"pow_log\": False, \"log_pow\": False})\n",
    "\n",
    "print(\"特徴量生成2 channelId - all features\")\n",
    "df, features = create_features2(df, features,\n",
    "                                cols_groupby=['channelId'],\n",
    "                                cols_transform=list(set(features['num'] + features['cat'] + features['ohe'])),\n",
    "                                target_func=['max', 'min', 'mean'],\n",
    "                                option={\"log\": False, \\\n",
    "                                        \"sqrt\": False, \"sqrt_log\": False, \"log_sqrt\": False,\\\n",
    "                                        \"pow\": False, \"pow_log\": False, \"log_pow\": False})\n",
    "# 特徴量生成3\n",
    "print(\"特徴量生成3\")\n",
    "df, features = create_features3(df, features,\n",
    "                                cols_transform=[c for c in df.columns if ('likes' in c)|('dislikes' in  c)|('comment_count' in c)],\n",
    "                                option={\"log\": False,\\\n",
    "                                        \"sqrt\": True, \"sqrt_log\": False, \"log_sqrt\": False,\\\n",
    "                                        \"pow\": True, \"pow_log\": False, \"log_pow\": True})\n",
    "# 特徴量数のチェック\n",
    "print(\"特徴量のチェック\")\n",
    "check_features(df, features)\n",
    "\n",
    "# 特徴量削除\n",
    "print(\"特徴量削除\")\n",
    "feats_to_drop = {\"cat\": ['video_id', 'channelId', 'title', 'channelTitle', 'tags', 'thumbnail_link', 'description'],\n",
    "                 \"date\": ['publishedAt', 'collection_date'],\n",
    "                 \"ohe\": [],\n",
    "                 \"num\": []}\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------\n",
    "# CV: 0.725691, LB: 0.723\n",
    "#-----------------------------------------------------------------------------------------------------------\n",
    "feats_to_drop[\"cat\"] += ['publishedAt_second', 'publishedAt_minute', 'publishedAt_hour', 'publishedAt_day', 'publishedAt_dayofweek']\n",
    "feats_to_drop[\"num\"] += ['len_description_dislikes_max', 'len_description_dislikes_mean',\n",
    "                         'sqrt_len_description_dislikes_max', 'sqrt_len_description_dislikes_mean',\n",
    "                         'pow_len_description_dislikes_max', 'pow_len_description_dislikes_mean',\n",
    "                         'log_pow_len_description_dislikes_max', 'log_pow_len_description_dislikes_mean']\n",
    "df, features = drop_features(df, features, feats_to_drop)\n",
    "\n",
    "# nunique()==1の特徴量を削除\n",
    "for col in df.loc[:, df.nunique()==1].columns:\n",
    "    features[\"num\"].remove(col)\n",
    "df.drop(df.loc[:, df.nunique()==1].columns, axis=1, inplace=True)\n",
    "\n",
    "# データ型変換\n",
    "print(\"データ型変換\")\n",
    "df = df.astype('float')\n",
    "df[features[\"cat\"]] = df[features[\"cat\"]].astype('int')\n",
    "df[features[\"ohe\"]] = df[features[\"ohe\"]].astype('int')\n",
    "\n",
    "print(\"標準化\")\n",
    "df = standardization(df, fnc_name=\"MinMax\", cols_to_std=features[\"num\"])\n",
    "    \n",
    "# 特徴量数のチェック\n",
    "print(\"特徴量のチェック\")\n",
    "check_features(df, features)\n",
    "\n",
    "# 学習、予測データ分割\n",
    "X_train = df.iloc[:y.shape[0], :].reset_index(drop=True)\n",
    "X_test  = df.iloc[y.shape[0]:, :].reset_index(drop=True)\n",
    "\n",
    "# 欠損値、無限大/無限小値有無のチェック\n",
    "print(f\"学習データ中の欠損値数: {X_train.isnull().sum().sum()}\")\n",
    "print(f\"学習データ中の無限値数: {np.count_nonzero(np.isinf(X_train))}\")\n",
    "print(f\"予測データ中の欠損値数: {X_test.isnull().sum().sum()}\")\n",
    "print(f\"予測データ中の無現値数: {np.count_nonzero(np.isinf(X_test))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "display(description(X_train))\n",
    "display(description(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Model\n",
    "#### データセット全体で学習・予測した結果に、\"comments_disabled == False\"条件で抽出したデータセットで学習・予測した結果をマージする\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "X_train_cmntf = X_train[(train_data['comments_disabled']==False)]\n",
    "y_cmntf       = y[(train_data['comments_disabled']==False)]\n",
    "y_bin_cmntf   = y_bin[(train_data['comments_disabled']==False)]\n",
    "X_test_cmntf  = X_test[(test_data['comments_disabled']==False)]\n",
    "\n",
    "X_train_rest = X_train[(train_data['comments_disabled']==True)]\n",
    "y_rest       = y[(train_data['comments_disabled']==True)]\n",
    "X_test_rest  = X_test[(test_data['comments_disabled']==True)]\n",
    "\n",
    "print(X_train_cmntf.shape, X_train_rest.shape, train_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnc_l = {\"Ridge\": ridge}\n",
    "\n",
    "def output_results(results, test_id, seed, N_FOLDS, out_dir):\n",
    "    submission = submit_results(y, results, test_id, f\"{results['label']}_SEED{seed}_FOLDS{N_FOLDS}_0627\")\n",
    "    with open(f\"./{out_dir}/{results['label']}_train_SEED{seed}_FOLDS{N_FOLDS}_0627.pickle\", 'wb') as f:\n",
    "        pickle.dump(results['train'], f)\n",
    "    with open(f\"./{out_dir}/{results['label']}_test_SEED{seed}_FOLDS{N_FOLDS}_0627.pickle\", 'wb') as f:\n",
    "        pickle.dump(results['test'], f)\n",
    "    return submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "target_fn = fnc_l['Ridge']\n",
    "\n",
    "seed_l = [51]\n",
    "N_FOLDS = 10\n",
    "encoding = []\n",
    "\n",
    "for seed in tqdm(seed_l, leave=False):\n",
    "    results_cmntf = target_fn(train=X_train_cmntf, test=X_test_cmntf, target=y_cmntf, target_skf=y_bin_cmntf,\n",
    "                              seed=seed, n_folds=N_FOLDS, encoding=encoding)\n",
    "\n",
    "    results = target_fn(train=X_train, test=X_test, target=y, target_skf=y_bin,\n",
    "                        seed=seed, n_folds=N_FOLDS, encoding=encoding)\n",
    "\n",
    "    keys = [k for k, v in fnc_l.items() if v == target_fn]\n",
    "    results_merged = {\n",
    "        \"label\": f\"{keys[0]}\",\n",
    "        \"train\": pd.concat([pd.Series(results_cmntf['train'], index=X_train_cmntf.index), \\\n",
    "                            pd.Series(results['train'][X_train_rest.index], index=X_train_rest.index)]).sort_index(),\n",
    "        \"test\":  pd.concat([pd.Series(results_cmntf['test'], index=X_test_cmntf.index), \\\n",
    "                            pd.Series(results['test'][X_test_rest.index], index=X_test_rest.index)]).sort_index(),\n",
    "        \"cv\": pd.DataFrame()}\n",
    "\n",
    "    print(\"-\"*80)\n",
    "    RMSLE_base    = mean_squared_error(y, results['train'], squared=False)\n",
    "    print(f\"Base    RMSLE={RMSLE_base}\")\n",
    "    RMSLE_base_false = mean_squared_error(y_cmntf, results['train'][X_train_cmntf.index], squared=False)\n",
    "    print(f\"f in B  RMSLE={RMSLE_base_false}\")\n",
    "    RMSLE_false   = mean_squared_error(y_cmntf, results_cmntf['train'], squared=False)\n",
    "    print(f\"False   RMSLE={RMSLE_false}\")\n",
    "    RMSLE_merged  = mean_squared_error(y, results_merged['train'], squared=False)\n",
    "    print(f\"merged  RMSLE={RMSLE_merged}\")\n",
    "    print(\"-\"*80)\n",
    "\n",
    "    if RMSLE_base_false > RMSLE_false:\n",
    "        results = results_merged\n",
    "    else:\n",
    "        results = results\n",
    "    sunmission = output_results(results, test_id, seed, N_FOLDS, out_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datetime.datetime.now()-start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
